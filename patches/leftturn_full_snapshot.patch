diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..9f579cd
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,37 @@
+# Azure Configuration
+AZURE_STORAGE_CONNECTION_STRING=your_storage_connection_string_here
+AZURE_COSMOSDB_CONNECTION_STRING=your_cosmosdb_connection_string_here
+AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING=your_communication_services_connection_string_here
+
+# Azure OpenAI Configuration
+AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
+AZURE_OPENAI_API_KEY=your_openai_api_key_here
+AZURE_OPENAI_MODEL=gpt-4.1
+
+# Email Configuration
+DEFAULT_SENDER_EMAIL=your_sender_email@domain.com
+
+# Validation Configuration
+MAX_FILE_SIZE_MB=50
+SUPPORTED_FILE_TYPES=xlsx
+
+# Agents & Data Tools
+FABRIC_ENDPOINT=https://your-fabric-endpoint
+FABRIC_TOKEN=your_fabric_bearer_token
+SEARCH_ENDPOINT=https://yoursearch.search.windows.net
+SEARCH_INDEX=contracts
+SEARCH_API_KEY=your_search_api_key
+GRAPH_ENDPOINT=https://graph.microsoft.com/v1.0
+GRAPH_TOKEN=your_graph_token
+
+# Reminder job
+REMINDER_DAYS_OLD=3
+REMINDER_MAX_ITEMS=100
+
+# Search data source seeding (infra/scripts/seed_search.sh)
+SEARCH_DS_CONNECTION_STRING=${AZURE_STORAGE_CONNECTION_STRING}
+SEARCH_DS_CONTAINER=contracts
+
+# Power BI deep link (optional)
+PBI_WORKSPACE_ID=
+PBI_REPORT_ID=
diff --git a/.gitattributes b/.gitattributes
new file mode 100644
index 0000000..dfe0770
--- /dev/null
+++ b/.gitattributes
@@ -0,0 +1,2 @@
+# Auto detect text files and perform LF normalization
+* text=auto
diff --git a/.github/copilot-instructions.md b/.github/copilot-instructions.md
new file mode 100644
index 0000000..22ed98e
--- /dev/null
+++ b/.github/copilot-instructions.md
@@ -0,0 +1,36 @@
+# Azure Excel Data Validation Agent
+
+This project implements an intelligent Azure-based agent that processes Excel files, validates data, sends email notifications for corrections, and tracks changes.
+
+## Architecture
+- **Azure Functions**: Serverless execution for processing Excel files and validation
+- **Azure AI Services**: Intelligent data validation and analysis using GPT models
+- **Azure Communication Services**: Email notifications and lookups
+- **Azure Storage**: State tracking and change validation
+- **Azure Cosmos DB**: Metadata and tracking storage
+
+## Completed Steps
+- [x] Project architecture designed
+- [x] Project structure created
+- [x] Excel processing implemented
+- [x] Data validation logic built
+- [x] Email functionality integrated
+- [x] Change validation implemented
+- [x] Configuration and deployment added
+
+## Project Features
+- **Excel File Processing**: Parse and validate .xlsx files
+- **AI-Powered Validation**: Uses Azure OpenAI for intelligent data validation and suggestions
+- **Email Notifications**: Automated email sending for validation failures and corrections
+- **Change Tracking**: Monitor and validate file corrections
+- **Serverless Architecture**: Scalable Azure Functions deployment
+
+## API Endpoints
+- `POST /api/process` - Process Excel file and validate data
+- `POST /api/validate` - Standalone data validation
+- `POST /api/notify` - Send email notifications
+- `POST /api/verify` - Verify file changes and corrections
+- `GET /api/health` - Health check endpoint
+
+## Development Complete
+All core functionality has been implemented and is ready for deployment to Azure.
diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
new file mode 100644
index 0000000..0f536db
--- /dev/null
+++ b/.github/workflows/ci.yml
@@ -0,0 +1,21 @@
+name: CI
+
+on:
+  pull_request:
+  push:
+    branches: [ main ]
+
+jobs:
+  test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - uses: actions/setup-python@v5
+        with:
+          python-version: '3.10'
+      - name: Install dependencies
+        run: |
+          bash tools/setup_cloud.sh
+      - name: Run tests
+        run: |
+          bash tools/run_tests.sh
diff --git a/.github/workflows/deploy-bicep.yml b/.github/workflows/deploy-bicep.yml
new file mode 100644
index 0000000..6fcdd7f
--- /dev/null
+++ b/.github/workflows/deploy-bicep.yml
@@ -0,0 +1,40 @@
+name: Deploy Bicep
+
+on:
+  workflow_dispatch:
+    inputs:
+      env:
+        description: 'Environment (dev/test/prod)'
+        required: true
+        default: 'dev'
+      location:
+        description: 'Azure location'
+        required: true
+        default: 'eastus'
+      baseName:
+        description: 'Base name prefix'
+        required: true
+        default: 'leftturn'
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  deploy:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Azure Login (OIDC)
+        uses: azure/login@v2
+        with:
+          client-id: ${{ secrets.AZURE_CLIENT_ID }}
+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
+      - name: Deploy Bicep
+        run: |
+          az deployment sub create \
+            --name leftturn-${{ github.run_id }} \
+            --location "${{ github.event.inputs.location }}" \
+            --template-file infra/bicep/main.bicep \
+            --parameters env=${{ github.event.inputs.env }} location=${{ github.event.inputs.location }} baseName=${{ github.event.inputs.baseName }}
diff --git a/.github/workflows/deploy-functions.yml b/.github/workflows/deploy-functions.yml
new file mode 100644
index 0000000..89f27a2
--- /dev/null
+++ b/.github/workflows/deploy-functions.yml
@@ -0,0 +1,43 @@
+name: Deploy Functions
+
+on:
+  workflow_dispatch:
+    inputs:
+      functionApp:
+        description: 'Azure Function App name'
+        required: true
+      python:
+        description: 'Python version'
+        required: true
+        default: '3.10'
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  deploy:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - uses: actions/setup-node@v4
+        with:
+          node-version: '20'
+      - uses: actions/setup-python@v5
+        with:
+          python-version: ${{ github.event.inputs.python }}
+      - name: Azure Login (OIDC)
+        uses: azure/login@v2
+        with:
+          client-id: ${{ secrets.AZURE_CLIENT_ID }}
+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
+      - name: Install Azure Functions Core Tools
+        run: |
+          npm i -g azure-functions-core-tools@4 --unsafe-perm true
+      - name: Install Python deps
+        run: |
+          pip install -r requirements.txt
+      - name: Publish to Azure Functions
+        run: |
+          func azure functionapp publish "${{ github.event.inputs.functionApp }}" --python
diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..722c225
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,103 @@
+# Python
+__pycache__/
+*.py[cod]
+*$py.class
+*.so
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# Virtual environments
+venv/
+env/
+ENV/
+env.bak/
+venv.bak/
+
+# Azure Functions
+.vscode/
+local.settings.json
+.azure/
+.funcignore
+
+# Environment variables
+.env
+.env.local
+.env.development
+.env.production
+
+# Logs
+*.log
+logs/
+
+# Runtime data
+pids
+*.pid
+*.seed
+*.pid.lock
+
+# Coverage directory used by tools like istanbul
+coverage/
+*.cover
+.nyc_output
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# pyenv
+.python-version
+
+# pipenv
+Pipfile.lock
+
+# pytest
+.pytest_cache/
+.coverage
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# IDE
+.vscode/
+.idea/
+*.swp
+*.swo
+*~
+
+# OS
+.DS_Store
+.DS_Store?
+._*
+.Spotlight-V100
+.Trashes
+ehthumbs.db
+Thumbs.db
+
+# Temporary files
+*.tmp
+*.temp
+
+# Azure specific
+.azure/
+*.publish
+
+# Excel test files
+*.xlsx
+*.xls
+test_files/
\ No newline at end of file
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
new file mode 100644
index 0000000..9920294
--- /dev/null
+++ b/.pre-commit-config.yaml
@@ -0,0 +1,22 @@
+repos:
+  - repo: https://github.com/astral-sh/ruff-pre-commit
+    rev: v0.6.9
+    hooks:
+      - id: ruff
+        args: [--fix]
+        files: ^(src/agents/router.py|src/services/sql_templates.py|src/services/http_client.py|tools/repo_sanity.py|tests/test_router.py)$
+      - id: ruff-format
+        files: ^(src/agents/router.py|src/services/sql_templates.py|src/services/http_client.py|tools/repo_sanity.py|tests/test_router.py)$
+  - repo: https://github.com/asottile/pyupgrade
+    rev: v3.17.0
+    hooks:
+      - id: pyupgrade
+        args: [--py311-plus]
+        files: ^(src/agents/router.py|src/services/sql_templates.py|src/services/http_client.py|tools/repo_sanity.py|tests/test_router.py)$
+  - repo: https://github.com/pre-commit/mirrors-mypy
+    rev: v1.11.2
+    hooks:
+      - id: mypy
+        additional_dependencies: [pydantic==2.7.3]
+        files: ^(src/agents/router.py|src/services/sql_templates.py|src/services/http_client.py|tools/repo_sanity.py|tests/test_router.py)$
+
diff --git a/AGENTS.md b/AGENTS.md
new file mode 100644
index 0000000..771d125
--- /dev/null
+++ b/AGENTS.md
@@ -0,0 +1,57 @@
+# AGENTS.md
+
+## Mission
+Implement a Microsoft Fabric‚Äìbased ‚ÄúLeftTurn-style‚Äù logistics intelligence stack that reconciles carrier contracts, invoices, tracking, and ERP data; exposes curated tables for analytics; and powers chat agents in M365 (Teams/Copilot) with retrieval over contracts and SQL over curated data.
+
+## Hard Constraints
+- Use Microsoft Fabric (Lakehouse + Warehouse/SQL endpoint) as the analytics backbone.
+- Use Azure AI Search for retrieval and Azure Document Intelligence (Form Recognizer) for parsing PDFs of carrier agreements/service guides.
+- Surface dashboards in Power BI; all numeric claims must be backed by curated tables.
+- Agents and APIs are read-only against production sources. Return evidence (rows/passages) for every numeric claim.
+
+## Repo Map
+- `src/agents/` ‚Äî orchestrator and chat-facing agents (Domain/Carrier/Customer Ops)
+- `src/services/` ‚Äî Fabric SQL client, AI Search client, Graph client, Excel/Email/Storage/Validation services
+- `src/functions/` ‚Äî Azure Functions blueprints (agent gateway, excel processing, validation, email, change-tracking)
+- `fabric/sql/` ‚Äî curated views to register in Fabric (e.g., `vw_Variance`)
+- `notebooks/` ‚Äî Fabric notebooks for ERP, Carrier (structured), and contract parsing (unstructured)
+- `infra/` ‚Äî IaC (Bicep/Terraform), APIM policies, Search assets, seed scripts
+- `docs/` ‚Äî architecture brief and diagram
+- `tests/` ‚Äî unit tests for agents/services/utils
+
+## Build & Run
+- Install deps: `pip install -r requirements.txt`
+- Lint: `flake8`
+- Tests: `pytest -q`
+- Local Functions host: `func start`
+- Cloud helpers: `tools/setup_cloud.sh`, `tools/run_tests.sh`
+
+## Environment
+Set env vars (see `.env.example`):
+- Storage/Cosmos/CommSvc: `AZURE_STORAGE_CONNECTION_STRING`, `AZURE_COSMOSDB_CONNECTION_STRING`, `AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING`
+- Agents & data tools: `FABRIC_ENDPOINT`, `FABRIC_TOKEN`, `SEARCH_ENDPOINT`, `SEARCH_INDEX`, `SEARCH_API_KEY`, `GRAPH_TOKEN`
+- Optional embeddings in Search: `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_EMBED_DEPLOYMENT`
+- Reminders: `REMINDER_DAYS_OLD`, `REMINDER_MAX_ITEMS`
+- Power BI: `PBI_WORKSPACE_ID`, `PBI_REPORT_ID` (adds deep links to answers)
+- APIM/EasyAuth (Bicep params): `enableEasyAuth`, `enableApim` (JWT + rate limiting)
+
+## How To Work (Playbook)
+1. Read `docs/leftturn-architecture.md` and this file.
+2. Structured questions ‚Üí `StructuredDataAgent` via Fabric views; use parameterized SQL.
+3. Unstructured questions ‚Üí `UnstructuredDataAgent` via AI Search; return citations (file, page, clause id if available).
+4. Keep PRs small and testable; update docs with any contract changes.
+5. Evidence-first: return tool results alongside any synthesized text.
+
+## Verification Steps
+- `flake8`
+- `pytest -q`
+- Optional: execute notebooks on sample data; confirm row counts and invariants.
+
+## Anti-goals
+- Non-Fabric warehouses; ad-hoc SQL bypassing curated views.
+- Printing secrets or sending sensitive content outside tenant.
+
+## Kickoff Tasks
+- Register `fabric/sql/create_views_carrier.sql` in Fabric.
+- `infra/scripts/seed_search.sh` with env set for Search and (optionally) Azure OpenAI embedding.
+- Publish Functions; protect with EasyAuth/APIM if required.
diff --git a/API_USAGE.md b/API_USAGE.md
new file mode 100644
index 0000000..46033e2
--- /dev/null
+++ b/API_USAGE.md
@@ -0,0 +1,328 @@
+# API Usage Examples
+
+This document provides examples of how to use the Azure Excel Data Validation Agent API endpoints.
+
+## Base URL
+
+```
+https://your-function-app.azurewebsites.net/api
+```
+
+## Authentication
+
+The Function App can be configured with authentication keys. Include the function key in requests:
+
+```
+?code=your_function_key
+```
+
+## 1. Process Excel File
+
+**Endpoint:** `POST /process`
+
+**Description:** Upload and validate an Excel file with automatic email notifications.
+
+**Request Body:**
+```json
+{
+    "filename": "employee_data.xlsx",
+    "file_data": "base64_encoded_file_content",
+    "validation_rules": [
+        {
+            "rule_id": "email_check",
+            "rule_name": "Email Validation",
+            "description": "Validate email format",
+            "rule_type": "format",
+            "parameters": {
+                "pattern": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$",
+                "columns": ["email"]
+            },
+            "severity": "error"
+        }
+    ],
+    "email_lookup_field": "email",
+    "requester_email": "admin@company.com"
+}
+```
+
+**Response:**
+```json
+{
+    "file_id": "excel_abc123_1638360000",
+    "validation_id": "val_excel_abc123_1638360000_1638360001",
+    "status": "failed",
+    "total_errors": 3,
+    "total_warnings": 1,
+    "processed_rows": 100,
+    "email_sent": true,
+    "notifications_sent": 2,
+    "timestamp": "2023-12-01T10:00:00.000Z",
+    "errors": [
+        {
+            "row": 5,
+            "column": "email",
+            "value": "invalid-email",
+            "message": "Value does not match expected format: Email Validation",
+            "suggested_correction": "Use format: user@domain.com"
+        }
+    ]
+}
+```
+
+## 2. Standalone Data Validation
+
+**Endpoint:** `POST /validate`
+
+**Description:** Validate JSON data without file upload.
+
+**Request Body:**
+```json
+{
+    "data": [
+        {"name": "John Doe", "email": "john@company.com", "age": 30},
+        {"name": "Jane Smith", "email": "invalid-email", "age": 25}
+    ],
+    "validation_rules": [
+        {
+            "rule_id": "required_fields",
+            "rule_name": "Required Fields",
+            "description": "Check required fields",
+            "rule_type": "custom",
+            "parameters": {
+                "required_columns": ["name", "email"]
+            },
+            "severity": "error"
+        }
+    ],
+    "data_id": "dataset_001"
+}
+```
+
+**Response:**
+```json
+{
+    "data_id": "dataset_001",
+    "validation_id": "val_dataset_001_1638360002",
+    "status": "failed",
+    "total_errors": 1,
+    "total_warnings": 0,
+    "processed_rows": 2,
+    "timestamp": "2023-12-01T10:05:00.000Z",
+    "errors": [
+        {
+            "row": 2,
+            "column": "email",
+            "value": "invalid-email",
+            "message": "Value does not match expected format: Email Format Validation",
+            "severity": "error",
+            "suggested_correction": "Use a valid email format like user@domain.com"
+        }
+    ]
+}
+```
+
+## 3. Send Email Notification
+
+**Endpoint:** `POST /notify`
+
+**Description:** Send email notifications for validation results.
+
+**Request Body:**
+```json
+{
+    "validation_id": "val_excel_abc123_1638360000_1638360001",
+    "recipient_emails": ["user@company.com", "admin@company.com"],
+    "notification_type": "failure"
+}
+```
+
+**Response:**
+```json
+{
+    "validation_id": "val_excel_abc123_1638360000_1638360001",
+    "notification_type": "failure",
+    "notifications_sent": 2,
+    "recipients": ["user@company.com", "admin@company.com"],
+    "timestamp": "2023-12-01T10:10:00.000Z",
+    "notification_ids": ["email_1638360600_user_company_com", "email_1638360600_admin_company_com"]
+}
+```
+
+## 4. Verify File Changes
+
+**Endpoint:** `POST /verify`
+
+**Description:** Check if corrections have been made to a previously failed file.
+
+**Request Body:**
+```json
+{
+    "original_file_id": "excel_abc123_1638360000",
+    "updated_file_data": "base64_encoded_updated_file_content",
+    "updated_filename": "employee_data_corrected.xlsx"
+}
+```
+
+**Response:**
+```json
+{
+    "original_file_id": "excel_abc123_1638360000",
+    "updated_file_id": "excel_def456_1638360600",
+    "updated_validation_id": "val_excel_def456_1638360600_1638360601",
+    "changes_successful": true,
+    "change_description": "File updated and re-validated",
+    "updated_file_hash": "def456...",
+    "validation_status": "passed",
+    "remaining_errors": 0,
+    "remaining_warnings": 0,
+    "timestamp": "2023-12-01T10:15:00.000Z",
+    "success_notifications_sent": 2
+}
+```
+
+## 5. Health Check
+
+**Endpoint:** `GET /health`
+
+**Description:** Check if the service is running.
+
+**Response:**
+```json
+{
+    "status": "healthy",
+    "service": "Azure Excel Data Validation Agent"
+}
+```
+
+## Error Responses
+
+All endpoints return error responses in this format:
+
+```json
+{
+    "error": "Error type",
+    "message": "Human readable error message",
+    "details": "Technical details (optional)"
+}
+```
+
+Common HTTP status codes:
+- `200`: Success
+- `400`: Bad Request (invalid input)
+- `404`: Not Found (resource not found)
+- `500`: Internal Server Error
+
+## Validation Rule Types
+
+### Format Validation
+```json
+{
+    "rule_type": "format",
+    "parameters": {
+        "pattern": "regex_pattern",
+        "columns": ["column1", "column2"]
+    }
+}
+```
+
+### Range Validation
+```json
+{
+    "rule_type": "range",
+    "parameters": {
+        "min": 0,
+        "max": 100,
+        "columns": ["score", "percentage"]
+    }
+}
+```
+
+### Data Type Validation
+```json
+{
+    "rule_type": "data_type",
+    "parameters": {
+        "expected_type": "int",
+        "columns": ["age", "count"]
+    }
+}
+```
+
+### Custom Validation
+```json
+{
+    "rule_type": "custom",
+    "parameters": {
+        "required_columns": ["name", "email", "id"]
+    }
+}
+```
+
+## File Encoding
+
+Excel files must be base64 encoded for API requests:
+
+```python
+import base64
+
+with open('file.xlsx', 'rb') as file:
+    file_data = base64.b64encode(file.read()).decode('utf-8')
+```
+
+```javascript
+// In browser
+const fileInput = document.getElementById('file-input');
+const file = fileInput.files[0];
+const reader = new FileReader();
+reader.readAsDataURL(file);
+reader.onload = function() {
+    const base64Data = reader.result.split(',')[1];
+    // Use base64Data in API request
+};
+```
+## 0. Ask an Agent
+
+Endpoint: `POST /agents/{agent}/ask`
+
+Description: Send a query to a chat agent. `{agent}` is one of `domain`, `carrier`, or `customer`.
+
+Request Body:
+```json
+{
+  "query": "Are we overbilled by Carrier X for SKU 812 this quarter?"
+}
+```
+
+Response:
+```json
+{
+  "agent": "CarrierAgent",
+  "tool": "fabric_sql",
+  "result": [
+    { "carrier": "X", "overbilled": true, "variance": 1243.55 }
+  ],
+  "citations": [
+    { "type": "table", "source": "fabric", "sql": "SELECT ..." }
+  ],
+  "powerBiLink": "https://app.powerbi.com/groups/<ws>/reports/<rep>/ReportSection?filter=vw_Variance/Carrier%20eq%20'X'"
+}
+```
+
+### Get Notification Status
+
+Endpoint: `GET /notify/status/{notification_id}`
+
+Description: Retrieve delivery/status of a previously sent notification. The status is read from Cosmos DB records.
+
+Response:
+```json
+{
+  "notification_id": "email_1699999999_user_domain_com",
+  "file_id": "excel_abc123_1638360000",
+  "validation_id": "val_excel_abc123_1638360000_1638360001",
+  "recipient_email": "user@domain.com",
+  "status": "sent",
+  "sent_timestamp": "2025-01-01T10:00:00Z",
+  "correction_deadline": "2025-01-04T10:00:00Z"
+}
+```
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..e6ea844
--- /dev/null
+++ b/README.md
@@ -0,0 +1,191 @@
+# LeftTurn Agents and Data Validation (Azure)
+
+This repository delivers a production‚Äëready, end‚Äëto‚Äëend implementation that maps to the architecture described in the prompt:
+
+- Chat‚Äëfacing LeftTurn agents (Domain/Carrier/Customer Ops) behind a single HTTP gateway
+- An Orchestrator that routes between Structured (Fabric) and Unstructured (Search) tools, with optional Microsoft Graph lookups
+- A robust Excel ingestion and validation flow with notifications, change tracking, and storage
+
+All components run inside your Microsoft 365/Azure tenant and integrate with Microsoft Fabric, Azure AI Search, Document Intelligence (optional), and Power BI.
+
+## Architecture
+
+- **Azure Functions**: Serverless app exposing all endpoints.
+- **AI Foundry Orchestrator**: `src/agents/orchestrator.py` selects tools per request.
+- **Structured Data Agent**: `src/agents/structured_data_agent.py` executes SQL via the Fabric connector client in `src/services/fabric_data_agent.py`.
+- **Unstructured Data Agent**: `src/agents/unstructured_data_agent.py` queries Azure AI Search via `src/services/search_service.py`.
+- **Microsoft Graph**: Optional enrichment via `src/services/graph_service.py`.
+- **Excel Validation Flow**: `src/functions/excel_processor`, `src/services/{excel,validation,email,storage}_service.py`.
+- **Storage**: Azure Blob Storage (files) + Cosmos DB (metadata, validations, notifications, tracking).
+- **Notifications**: Azure Communication Services (Email).
+- **API Management / EasyAuth**: Optional JWT enforcement and rate limiting in front of the Function App.
+
+## End‚Äëto‚ÄëEnd Flow
+
+1. User asks a question to an agent via the HTTP Agent Gateway (publishable via Copilot Studio/Teams).
+2. The Orchestrator decides whether to use Fabric SQL (numbers) or AI Search (clauses). Microsoft Graph can be used when queries mention calendar/email/files.
+3. The answer returns with grounded evidence (rows or passages).
+4. If the query touches curated tables and `PBI_*` variables are configured, the response also includes a Power BI deep link for deeper analysis.
+5. Separately, Excel files are ingested, validated, and persisted. Failed validations trigger email notifications and change tracking; corrections can be verified with a follow‚Äëup call.
+
+## Fabric Backbone and Data Model
+
+- Lakehouse medallion layout: landing ‚Üí standardized ‚Üí curated.
+- Canonical logistics model (suggested): `FactShipment`, `FactInvoice`, `DimCarrier`, `DimServiceLevel`, `DimSKU`, `DimZone`, `DimCustomer` and contract‚Äëderived tables `RateSheet`, `ZoneMatrix`, `AccessorialRule`, `FuelTable`, `Exceptions`.
+- Curated Delta tables power Power BI dashboards and agent queries; connect with the Fabric SQL endpoint set in `FABRIC_ENDPOINT`.
+
+## Development Setup
+
+### Prerequisites
+- Python 3.9+
+- Azure CLI
+- Azure Functions Core Tools v4
+- Azure subscription with: Storage, Cosmos DB, Communication Services, AI Search, Microsoft Fabric (for SQL endpoint), and optional Microsoft Graph app registration.
+
+### Installation
+
+```bash
+pip install -r requirements.txt
+```
+
+### Configuration
+
+Copy `.env.example` to `.env` and configure your Azure settings:
+
+```bash
+cp .env.example .env
+```
+
+### Running Locally
+
+```bash
+func start
+```
+
+## Deployment
+
+Deploy to Azure using the provided deployment script:
+
+```bash
+./deploy.sh
+```
+
+## Project Structure
+
+```
+‚îú‚îÄ‚îÄ .github/
+‚îÇ   ‚îî‚îÄ‚îÄ copilot-instructions.md
+‚îú‚îÄ‚îÄ src/
+‚îÇ   ‚îú‚îÄ‚îÄ functions/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ excel_processor/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_validator/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ email_sender/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ change_tracker/
+‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent_gateway/        # Chat gateway for Domain/Carrier/Customer agents
+‚îÇ   ‚îú‚îÄ‚îÄ services/
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ excel_service.py
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation_service.py
+‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ email_service.py
+‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage_service.py
+‚îÇ   ‚îú‚îÄ‚îÄ models/
+‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation_models.py
+‚îÇ   ‚îî‚îÄ‚îÄ utils/
+‚îÇ       ‚îî‚îÄ‚îÄ helpers.py
+‚îú‚îÄ‚îÄ tests/
+‚îú‚îÄ‚îÄ requirements.txt
+‚îú‚îÄ‚îÄ host.json
+‚îú‚îÄ‚îÄ local.settings.json
+‚îú‚îÄ‚îÄ function_app.py
+‚îú‚îÄ‚îÄ .env.example
+‚îî‚îÄ‚îÄ README.md
+
+See `AGENTS.md` for contributor/agent guidance and `docs/leftturn-architecture.md` for the full architecture write-up with a Mermaid diagram at `docs/diagrams/leftturn-arch.mmd`.
+```
+
+## Configuration
+
+The agent requires the following environment variables:
+
+- `AZURE_STORAGE_CONNECTION_STRING`: Connection string for Azure Storage
+- `AZURE_COSMOSDB_CONNECTION_STRING`: Connection string for Cosmos DB
+- `AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING`: Connection string for Communication Services
+- `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint
+- `AZURE_OPENAI_API_KEY`: Azure OpenAI API key
+- `AZURE_OPENAI_MODEL`: Model name (e.g., gpt-4.1)
+- `AZURE_OPENAI_EMBED_DEPLOYMENT`: Embedding deployment name used by the Search skillset
+
+Agents and data tools:
+
+- `FABRIC_ENDPOINT`: Base URL for Microsoft Fabric SQL endpoint (e.g., `https://<workspace>.<region>.fabric.microsoft.com`)
+- `FABRIC_TOKEN`: OAuth bearer token for Fabric endpoint (Entra ID).
+- `SEARCH_ENDPOINT`: Azure AI Search endpoint (e.g., `https://<name>.search.windows.net`)
+- `SEARCH_INDEX`: Search index name for contracts/guides.
+- `SEARCH_API_KEY`: Admin or query key for the Search service.
+- `GRAPH_TOKEN`: Optional bearer token for Microsoft Graph (or use API Management/proxy).
+- `GRAPH_ENDPOINT`: Optional Graph base URL (default `https://graph.microsoft.com/v1.0`).
+
+Optional configuration (with defaults):
+
+- `SUPPORTED_FILE_TYPES`: Comma-separated list of allowed file extensions (without the dot). Default: `xlsx`.
+- `MAX_FILE_SIZE_MB`: Maximum upload size in megabytes for `/api/process`. Default: `50`.
+- `DEFAULT_SENDER_EMAIL`: Sender address for email notifications. Default: `noreply@yourdomain.com`.
+- `REMINDER_DAYS_OLD`: Days after which failed validations receive a reminder. Default: `3`.
+- `REMINDER_MAX_ITEMS`: Safety cap for reminders processed per run. Default: `100`.
+- `PBI_WORKSPACE_ID` / `PBI_REPORT_ID`: If set, the agent gateway includes a `powerBiLink` deep-link in structured answers.
+- `SEARCH_DS_CONNECTION_STRING` / `SEARCH_DS_CONTAINER`: Storage settings used by `infra/scripts/seed_search.sh` to create the Search data source. Defaults: storage connection string and `contracts`.
+
+Notes:
+
+- Only `.xlsx` is supported by default. Legacy `.xls` is not enabled because the configured reader uses `openpyxl`.
+- Requests exceeding `MAX_FILE_SIZE_MB` return HTTP 413 (Payload Too Large).
+- Timestamps in responses, storage records, and emails are UTC.
+
+## Key Endpoints
+
+- `POST /api/agents/{agent}/ask` ‚Äî chat to `domain|carrier|customer|claims` agents with `{ "query": "..." }`. Returns `{ tool, result, citations }` and optional `powerBiLink`. Add `?format=card` or `{ "format": "card" }` to receive an Adaptive Card (for Teams).
+- `POST /api/teams/ask` ‚Äî Teams relay that always returns an Adaptive Card for `{ query, agent }`.
+- `POST /api/process` ‚Äî upload and validate an Excel file (base64 payload).
+- `GET /api/status/{file_id}` ‚Äî check processing and latest validation.
+- `POST /api/notify` ‚Äî send email notifications for a validation.
+- `GET /api/notify/status/{notification_id}` ‚Äî delivery/status of a notification backed by Cosmos DB.
+- `POST /api/verify` ‚Äî verify corrections with an updated file.
+- `GET /api/history/{file_id}` ‚Äî change tracking history.
+
+An OpenAPI definition for the agent gateway is available at `docs/openapi/agent-gateway.yaml`.
+
+## Operations and Governance
+
+- Lineage and audit: Validation results, notifications, and change tracking are recorded in Cosmos DB.
+- Least privilege: Scope tokens/keys to read‚Äëonly Fabric SQL and read‚Äëonly Search.
+- Guardrails: Orchestrator prefers structured data for numeric claims; each answer payload contains the raw results returned by tools for downstream rendering/citations.
+- EasyAuth/APIM: Enforce Azure AD JWTs and rate‚Äëlimit calls at the edge when enabled in Bicep.
+
+## Next Integrations
+
+- Document Intelligence: parse carrier PDFs into structured tables feeding Fabric.
+- Power BI: connect to curated Delta tables for dashboards; link BI bookmarks from agent responses.
+
+## Infrastructure-as-Code
+
+- Bicep templates: `infra/bicep/main.bicep` (RG scope). Creates Storage, Cosmos (serverless), Communication Services, Cognitive Services (Form Recognizer), Azure AI Search, Function App (Linux/Python). Outputs resource names.
+- EasyAuth/APIM: Bicep supports enabling App Service Authentication (`enableEasyAuth`) and optionally creates API Management (`enableApim`) for JWT enforcement and rate limiting. APIM policy templates live in `infra/apim/policies`.
+- Terraform: `infra/terraform/` equivalent provisioning using `azurerm` and `azapi` (preview Fabric workspace).
+- Search data-plane seeding: `infra/scripts/seed_search.sh` creates a Blob data source, PII‚Äëredacting skillset (with optional Azure OpenAI embeddings), the `contracts` index, and a scheduled indexer.
+- Fabric SQL views: `fabric/sql/create_views_carrier.sql` seeds curated views.
+- Sample notebooks: `notebooks/*.ipynb` ready to import into Fabric for ERP, carrier structured, and contracts processing.
+
+## Teams Integration
+- Use Copilot Studio (org agents) or a native Teams App (Bot + Message Extension).
+- Native manifest starter: `teams/manifest/manifest.dev.json`.
+- Adaptive Card builder: `src/utils/cards.py` (adds an ‚ÄúOpen in Power BI‚Äù button when configured).
+- See `docs/teams-integration.md` for step-by-step instructions.
+
+## Claims Agent (Logistics Value Add)
+- New persona `claims` focuses on disputes/claims workflows (e.g., SLA breaches, overbilling).
+- Typical actions: ‚ÄúCreate dispute packet for invoice 123‚Äù, ‚ÄúSummarize evidence for Carrier X Q2‚Äù, ‚ÄúStatus of claim 456‚Äù.
+- Pattern: Structured facts from Fabric (variance, shipments) + contract clauses from Search; package an evidence bundle linking both.
+- Integrations: TMS/WMS (status events), EDI 210/214, SharePoint/OneDrive for packet storage, ServiceNow/Jira for ticketing, Outlook for templated emails.
+
+## License
+
+MIT License
diff --git a/deploy.sh b/deploy.sh
new file mode 100755
index 0000000..c7ffaba
--- /dev/null
+++ b/deploy.sh
@@ -0,0 +1,141 @@
+#!/bin/bash
+
+# Azure Excel Data Validation Agent Deployment Script
+
+set -e  # Exit on any error
+
+echo "üöÄ Starting Azure Excel Data Validation Agent deployment..."
+
+# Configuration
+RESOURCE_GROUP=${RESOURCE_GROUP:-"excel-validation-rg"}
+LOCATION=${LOCATION:-"eastus"}
+FUNCTION_APP_NAME=${FUNCTION_APP_NAME:-"excel-validation-app"}
+STORAGE_ACCOUNT_NAME=${STORAGE_ACCOUNT_NAME:-"excelvalidationstorage"}
+COSMOSDB_ACCOUNT_NAME=${COSMOSDB_ACCOUNT_NAME:-"excel-validation-cosmos"}
+COMMUNICATION_SERVICE_NAME=${COMMUNICATION_SERVICE_NAME:-"excel-validation-communication"}
+
+echo "üìã Configuration:"
+echo "  Resource Group: $RESOURCE_GROUP"
+echo "  Location: $LOCATION"
+echo "  Function App: $FUNCTION_APP_NAME"
+echo "  Storage Account: $STORAGE_ACCOUNT_NAME"
+echo "  Cosmos DB Account: $COSMOSDB_ACCOUNT_NAME"
+echo "  Communication Service: $COMMUNICATION_SERVICE_NAME"
+
+# Check if Azure CLI is installed
+if ! command -v az &> /dev/null; then
+    echo "‚ùå Azure CLI is not installed. Please install it first."
+    exit 1
+fi
+
+# Check if logged in to Azure
+if ! az account show &> /dev/null; then
+    echo "‚ùå Not logged in to Azure. Please run 'az login' first."
+    exit 1
+fi
+
+echo "‚úÖ Azure CLI is configured"
+
+# Create resource group
+echo "üì¶ Creating resource group..."
+az group create --name $RESOURCE_GROUP --location $LOCATION
+
+# Create storage account
+echo "üíæ Creating storage account..."
+az storage account create \
+    --name $STORAGE_ACCOUNT_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --location $LOCATION \
+    --sku Standard_LRS
+
+# Get storage connection string
+STORAGE_CONNECTION_STRING=$(az storage account show-connection-string \
+    --name $STORAGE_ACCOUNT_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --query connectionString --output tsv)
+
+echo "‚úÖ Storage account created"
+
+# Create Cosmos DB account
+echo "üåç Creating Cosmos DB account..."
+az cosmosdb create \
+    --name $COSMOSDB_ACCOUNT_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --default-consistency-level Eventual \
+    --locations regionName=$LOCATION failoverPriority=0 isZoneRedundant=False
+
+# Get Cosmos DB connection string
+COSMOSDB_CONNECTION_STRING=$(az cosmosdb keys list \
+    --name $COSMOSDB_ACCOUNT_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --type connection-strings \
+    --query "connectionStrings[0].connectionString" --output tsv)
+
+echo "‚úÖ Cosmos DB account created"
+
+# Create Communication Services resource
+echo "üìß Creating Communication Services..."
+az communication create \
+    --name $COMMUNICATION_SERVICE_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --location "Global" \
+    --data-location "United States"
+
+# Get Communication Services connection string
+COMMUNICATION_CONNECTION_STRING=$(az communication list-key \
+    --name $COMMUNICATION_SERVICE_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --query primaryConnectionString --output tsv)
+
+echo "‚úÖ Communication Services created"
+
+# Create Function App
+echo "‚ö° Creating Function App..."
+az functionapp create \
+    --resource-group $RESOURCE_GROUP \
+    --consumption-plan-location $LOCATION \
+    --runtime python \
+    --runtime-version 3.9 \
+    --functions-version 4 \
+    --name $FUNCTION_APP_NAME \
+    --storage-account $STORAGE_ACCOUNT_NAME \
+    --os-type Linux
+
+echo "‚úÖ Function App created"
+
+# Configure application settings
+echo "‚öôÔ∏è  Configuring application settings..."
+az functionapp config appsettings set \
+    --name $FUNCTION_APP_NAME \
+    --resource-group $RESOURCE_GROUP \
+    --settings \
+        "AZURE_STORAGE_CONNECTION_STRING=$STORAGE_CONNECTION_STRING" \
+        "AZURE_COSMOSDB_CONNECTION_STRING=$COSMOSDB_CONNECTION_STRING" \
+        "AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING=$COMMUNICATION_CONNECTION_STRING" \
+        "AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}" \
+        "AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}" \
+        "AZURE_OPENAI_MODEL=${AZURE_OPENAI_MODEL:-gpt-4.1}" \
+        "DEFAULT_SENDER_EMAIL=${DEFAULT_SENDER_EMAIL:-noreply@yourdomain.com}"
+
+echo "‚úÖ Application settings configured"
+
+echo ""
+echo "üéâ Deployment completed successfully!"
+echo ""
+echo "üìã Resource Information:"
+echo "  Resource Group: $RESOURCE_GROUP"
+echo "  Function App URL: https://$FUNCTION_APP_NAME.azurewebsites.net"
+echo "  Storage Account: $STORAGE_ACCOUNT_NAME"
+echo "  Cosmos DB Account: $COSMOSDB_ACCOUNT_NAME"
+echo "  Communication Service: $COMMUNICATION_SERVICE_NAME"
+echo ""
+echo "üìù Next Steps:"
+echo "1. Configure your Azure OpenAI endpoint and API key in the Function App settings"
+echo "2. Set up email domain verification in Communication Services"
+echo "3. Deploy your function code using 'func azure functionapp publish $FUNCTION_APP_NAME'"
+echo "4. Test the health endpoint: https://$FUNCTION_APP_NAME.azurewebsites.net/api/health"
+echo ""
+echo "üîó Useful URLs:"
+echo "  Function App: https://portal.azure.com/#resource/subscriptions/$(az account show --query id -o tsv)/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Web/sites/$FUNCTION_APP_NAME"
+echo "  Storage Account: https://portal.azure.com/#resource/subscriptions/$(az account show --query id -o tsv)/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT_NAME"
+echo "  Cosmos DB: https://portal.azure.com/#resource/subscriptions/$(az account show --query id -o tsv)/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.DocumentDB/databaseAccounts/$COSMOSDB_ACCOUNT_NAME"
\ No newline at end of file
diff --git a/docs/diagrams/leftturn-arch.mmd b/docs/diagrams/leftturn-arch.mmd
new file mode 100644
index 0000000..800f828
--- /dev/null
+++ b/docs/diagrams/leftturn-arch.mmd
@@ -0,0 +1,22 @@
+flowchart LR
+  user([M365 Copilot / Teams]) --> agents[LeftTurn Agents\n(Domain, Carrier, Cust Ops)]
+  agents --> gw[Agent Gateway (Azure Functions)]
+  gw --> orch[Orchestrator]
+  orch -->|structured| sda[Structured Data Agent]
+  orch -->|unstructured| uda[Unstructured Data Agent]
+  orch -->|optional| graph[Microsoft Graph]
+
+  sda --> fabric[Fabric SQL Endpoint]
+  uda --> search[Azure AI Search]
+  uda --> docint[Document Intelligence]
+  fabric --> lakehouse[(Fabric Lakehouse\nCurated Delta Tables)]
+  lakehouse --> pbi[Power BI Reports]
+  subgraph Landing Zone
+    storage[(Azure Storage)]
+  end
+  storage --> lakehouse
+
+  classDef svc fill:#eef,stroke:#268bd2;
+  classDef data fill:#efe,stroke:#2aa198;
+  class agents,gw,orch,sda,uda,graph,search,docint svc;
+  class lakehouse,storage data;
diff --git a/docs/leftturn-architecture.md b/docs/leftturn-architecture.md
new file mode 100644
index 0000000..4ec3cc3
--- /dev/null
+++ b/docs/leftturn-architecture.md
@@ -0,0 +1,55 @@
+# LeftTurn-Style Architecture (Fabric + Agents)
+
+This document captures the end-to-end design and value proposition for a logistics intelligence stack that reconciles what the contract says with what the invoice shows and what the ERP ordered/shipped. It aligns with the repo‚Äôs implementation.
+
+## Setting
+Everything runs in the customer‚Äôs Microsoft 365/Azure tenant. Heavy data and AI plumbing runs in Microsoft Fabric; users interact in M365 Copilot Chat or Microsoft Teams via org-published agents.
+
+## Cast of Characters
+- LeftTurn Agents (org store): Domain, Carrier, Customer Operations
+- AI Foundry/Orchestrator: routes requests and invokes tools
+- Tooling agents: Structured Data (SQL/Fabric) and Unstructured Data (contracts/docs via Azure AI Search + Document Intelligence)
+- APIs: Fabric Data Agent (via Fabric connector) and Microsoft Graph (optional)
+- Data estate: Fabric Lakehouse Landing Zone with domains: Carrier and ERP feeding curated tables
+- Notebooks: three pipelines ‚Äî unstructured carrier agreements & service guides; structured carrier data (billing/tracking/rating); structured ERP data (LOB, SKU, orders)
+- AI Search & Doc Intelligence: indexing and document parsing/OCR with PII redaction
+- Visualization: Power BI reports inside Fabric
+- Storage: Azure Storage landing for structured streams
+
+## Plot (request ‚Üí answer)
+1. A user asks a question in Copilot/Teams (e.g., ‚ÄúAre we overbilled by Carrier X on 2‚Äëday air for SKU 812 this quarter?‚Äù).
+2. A LeftTurn Agent receives the prompt and routes to the Orchestrator, which decides which tools/agents to invoke.
+3. Structured needs ‚Üí Structured Data Agent runs parameterized SQL against curated tables via Fabric Data Agent.
+4. Unstructured needs ‚Üí Unstructured Data Agent uses Document Intelligence to parse PDFs and Azure AI Search to retrieve relevant passages.
+5. Optional M365 context via Microsoft Graph.
+6. Orchestrator blends table results with retrieved contract text and returns a grounded answer with citations.
+7. The same curated data powers Power BI dashboards for repeatable analysis.
+
+## Data Lifecycle
+- Ingestion: notebooks bring in ERP data, carrier invoices/tracking, and PDFs of agreements/service guides.
+- Parsing: Document Intelligence extracts entities (rates, zones, surcharges, minimums) into structured tables.
+- Indexing: Azure AI Search indexes cleaned text and vector embeddings for hybrid retrieval.
+- Curation: Fabric Delta tables become the ‚Äúsingle source of truth‚Äù used by BI, search, and agents.
+- Visualization: Power BI shows cost, exceptions, SLA adherence, and variance between ‚Äúshould pay‚Äù and ‚Äúdid pay.‚Äù
+
+## What LeftTurn Delivers
+- Prebuilt agents your users can chat with in M365
+- Data notebooks for contracts (unstructured), carrier (structured), and ERP (structured)
+- Canonical data model reconciling contracts ‚Üî invoices ‚Üî ERP
+- Search + document understanding tuned for logistics contracts
+- End-user value: cost control, faster answers, and an auditable chain from clause ‚Üí rate ‚Üí invoice variance ‚Üí dashboard
+
+## This Repo‚Äôs Mapping
+- Orchestrator & agents: `src/agents/`
+- Tool clients: `src/services/` (Fabric, Search, Graph)
+- HTTP endpoints: `src/functions/` (agent gateway, processing, validation, email, tracking)
+- Curated SQL: `fabric/sql/`
+- Notebooks: `notebooks/`
+- Infra: `infra/`
+
+## Guardrails
+- Read-only data access for agents; use parameterized SQL against approved views.
+- Evidence with every answer: return rows/passages and citations (file/page/clause or view/sql).
+- Row-level security and sensitivity labels in Fabric where needed; PII redaction in Search skillset.
+
+Refer to the Mermaid diagram for a compact view.
diff --git a/docs/openapi/agent-gateway.yaml b/docs/openapi/agent-gateway.yaml
new file mode 100644
index 0000000..e4b4f32
--- /dev/null
+++ b/docs/openapi/agent-gateway.yaml
@@ -0,0 +1,56 @@
+openapi: 3.0.3
+info:
+  title: LeftTurn Agent Gateway
+  version: 0.1.0
+servers:
+  - url: https://{host}/api
+    variables:
+      host:
+        default: your-function-app.azurewebsites.net
+paths:
+  /agents/{agent}/ask:
+    post:
+      summary: Ask an agent (Domain, Carrier, Customer, Claims)
+      parameters:
+        - in: path
+          name: agent
+          required: true
+          schema:
+            type: string
+            enum: [domain, carrier, customer, claims]
+        - in: query
+          name: format
+          required: false
+          schema:
+            type: string
+            enum: [json, card]
+      requestBody:
+        required: true
+        content:
+          application/json:
+            schema:
+              type: object
+              properties:
+                query: { type: string }
+                format: { type: string, enum: [json, card] }
+              required: [query]
+      responses:
+        '200':
+          description: Agent response
+          content:
+            application/json:
+              schema:
+                oneOf:
+                  - type: object
+                    properties:
+                      agent: { type: string }
+                      tool: { type: string }
+                      result: {}
+                      citations:
+                        type: array
+                        items: { type: object }
+                      powerBiLink: { type: string }
+                  - type: object
+                    description: Adaptive Card payload
+        '400': { description: Bad request }
+        '401': { description: Unauthorized }
diff --git a/docs/teams-integration.md b/docs/teams-integration.md
new file mode 100644
index 0000000..8729ea2
--- /dev/null
+++ b/docs/teams-integration.md
@@ -0,0 +1,47 @@
+# Teams Integration
+
+This solution exposes a single Agent Gateway endpoint that can be surfaced in Microsoft Teams in two primary ways:
+
+- Copilot Studio (recommended for fastest path): publish Domain/Carrier/Customer Ops agents with Actions that call our HTTP endpoints.
+- Native Teams app (Bot + Message Extension): use a Bot Framework bot with a compose extension that calls the gateway and renders Adaptive Cards.
+
+## 1) Copilot Studio (Org agents)
+- Import an OpenAPI definition that covers `POST /api/agents/{agent}/ask`.
+- Create three copilots (Domain, Carrier, Customer Ops) and add an Action per copilot that pins the agent in the path.
+- Authentication: Entra ID; configure the Function App with EasyAuth (AAD) or front with APIM.
+- Output: set response parsing to display the returned `citations` and provide the `powerBiLink`.
+
+## 2) Native Teams App (Bot + ME)
+- Register a Bot (AAD app) and create a Teams App manifest (see `teams/manifest/manifest.dev.json`).
+- Add a compose extension command `ask` that posts the user query to `/api/agents/{agent}/ask?format=card`.
+- The gateway returns an Adaptive Card; see `teams/cards/answerCard.sample.json` and the runtime card builder at `src/utils/cards.py`.
+
+### Endpoint contract for Teams
+```
+POST /api/agents/{agent}/ask
+Body: { "query": "...", "format": "card" }
+Return: Adaptive Card JSON (if format=card) or JSON payload with { tool, result, citations, powerBiLink }
+```
+
+### Auth & Rate Limiting
+- Enable EasyAuth via Bicep (`enableEasyAuth=true`) and require AAD tokens from the bot/calling app.
+- Optional: place API Management in front (`enableApim=true`) and apply JWT + rate-limit policy from `infra/apim/policies/global.xml`.
+
+### Deep links to Power BI
+- Set `PBI_WORKSPACE_ID`, `PBI_REPORT_ID` in Function App settings.
+- The Agent Gateway includes `powerBiLink` for structured answers; the Adaptive Card adds an OpenUrl button automatically.
+
+### SSO
+- Use Teams SSO to obtain an AAD token and exchange it for a backend token (on-behalf-of) when you move to Managed Identity for downstream services.
+
+### Developer Loop
+- Use Teams Toolkit or App Studio to load `teams/manifest/manifest.dev.json`.
+- Point `validDomains` to your Function App host.
+- Validate the compose extension by sending prompts like:
+  - ‚ÄúAre we overbilled by Carrier X for SKU 812 this quarter?‚Äù
+  - ‚ÄúFind clause 7.4 for Carrier X‚Äù
+
+## Troubleshooting
+- If cards don‚Äôt render: ensure `format=card` is sent and the card validates at https://adaptivecards.io/designer/.
+- If requests 401: verify EasyAuth audience and your bot AAD app‚Äôs token scopes.
+- If Search is empty: seed index via `infra/scripts/seed_search.sh`.
diff --git a/fabric/sql/create_views_carrier.sql b/fabric/sql/create_views_carrier.sql
new file mode 100644
index 0000000..f11025c
--- /dev/null
+++ b/fabric/sql/create_views_carrier.sql
@@ -0,0 +1,26 @@
+-- Curated views for logistics analytics in Microsoft Fabric
+
+CREATE OR ALTER VIEW vw_FactInvoice AS
+SELECT * FROM dbo.FactInvoice;
+
+CREATE OR ALTER VIEW vw_FactShipment AS
+SELECT * FROM dbo.FactShipment;
+
+CREATE OR ALTER VIEW vw_Variance AS
+SELECT 
+    i.InvoiceId,
+    i.Carrier,
+    i.ServiceLevel,
+    i.SKU,
+    i.ShipDate,
+    i.BilledAmount,
+    r.RatedAmount,
+    (i.BilledAmount - r.RatedAmount) AS Variance,
+    r.Reason
+FROM dbo.FactInvoice i
+LEFT JOIN dbo.RatingOutput r
+  ON r.InvoiceLineId = i.InvoiceLineId;
+
+CREATE OR ALTER VIEW vw_FuelSurcharge AS
+SELECT Carrier, EffectiveDate, Percent
+FROM dbo.FuelTable;
diff --git a/function_app.py b/function_app.py
new file mode 100644
index 0000000..85736c1
--- /dev/null
+++ b/function_app.py
@@ -0,0 +1,34 @@
+import azure.functions as func
+import logging
+from src.functions.excel_processor import excel_processor_bp
+from src.functions.data_validator import data_validator_bp
+from src.functions.email_sender import email_sender_bp
+from src.functions.change_tracker import change_tracker_bp
+from src.functions.agent_gateway import agent_gateway_bp
+from src.functions.teams_relay import teams_relay_bp
+
+# Initialize the Function App
+app = func.FunctionApp()
+
+# Register blueprints
+app.register_functions(excel_processor_bp)
+app.register_functions(data_validator_bp)
+app.register_functions(email_sender_bp)
+app.register_functions(change_tracker_bp)
+app.register_functions(agent_gateway_bp)
+app.register_functions(teams_relay_bp)
+
+# Configure logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+@app.function_name(name="health")
+@app.route(route="health", methods=["GET"])
+def health_check(req: func.HttpRequest) -> func.HttpResponse:
+    """Health check endpoint"""
+    logger.info("Health check requested")
+    return func.HttpResponse(
+        body='{"status": "healthy", "service": "Azure Excel Data Validation Agent"}',
+        status_code=200,
+        headers={"Content-Type": "application/json"}
+    )
diff --git a/host.json b/host.json
new file mode 100644
index 0000000..454e3cf
--- /dev/null
+++ b/host.json
@@ -0,0 +1,16 @@
+{
+  "version": "2.0",
+  "logging": {
+    "applicationInsights": {
+      "samplingSettings": {
+        "isEnabled": true,
+        "excludedTypes": "Request"
+      }
+    }
+  },
+  "extensionBundle": {
+    "id": "Microsoft.Azure.Functions.ExtensionBundle",
+    "version": "[4.*, 5.0.0)"
+  },
+  "functionTimeout": "00:10:00"
+}
\ No newline at end of file
diff --git a/infra/README.md b/infra/README.md
new file mode 100644
index 0000000..e0bdac7
--- /dev/null
+++ b/infra/README.md
@@ -0,0 +1,44 @@
+Infrastructure-as-Code (IaC)
+
+This folder contains Bicep and Terraform templates plus helper scripts to provision the core resources used by this solution inside your Azure tenant.
+
+What it creates
+- Storage account (Blob) for file landing and Function App storage
+- Cosmos DB account + database + containers
+- Azure Communication Services (Email)
+- Azure AI Search service
+- Azure Cognitive Services (Document Intelligence / Form Recognizer)
+- Azure Function App (Linux, Python)
+- Optional: Microsoft Fabric workspace via AzAPI (preview)
+- Optional: App Service Authentication (EasyAuth) and API Management (APIM) for JWT auth and rate limiting
+
+Quick start (Bicep)
+1) Ensure Azure CLI is logged in: `az login`
+2) Deploy resource group-level template:
+   az deployment sub create \
+     --name leftturn-bicep-$(date +%s) \
+     --location eastus \
+     --template-file infra/bicep/main.bicep \
+     --parameters env=dev location=eastus baseName=leftturn${RANDOM} enableEasyAuth=false enableApim=false
+
+Quick start (Terraform)
+1) cd infra/terraform
+2) terraform init
+3) terraform apply -var 'env=dev' -var 'location=eastus' -var 'base_name=leftturn'
+
+Seed assets
+- Search data-plane: run `infra/scripts/seed_search.sh` to create data source (Blob), skillset (PII redaction + optional embeddings), index, and indexer (scheduled).
+- Fabric SQL views: use files in `fabric/sql/` in your Fabric workspace SQL endpoint.
+- Notebooks: import `notebooks/*.ipynb` into Microsoft Fabric.
+
+APIM & EasyAuth
+- EasyAuth: set `enableEasyAuth=true` and pass `aadTenantId` and `aadAllowedAudience` parameters to enforce AAD on the Function App.
+- APIM: set `enableApim=true`. Use policy templates in `infra/apim/policies/` (global.xml) for JWT validation and rate limiting.
+
+Embeddings for Search
+- To populate vector embeddings automatically, set these before running `infra/scripts/seed_search.sh`:
+  - `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_EMBED_DEPLOYMENT`
+
+Notes
+- Some resources (Search index, skillset) use data-plane APIs; ARM cannot create them directly, so we provide scripts.
+- Microsoft Fabric ARM/TF support uses preview resource providers. AzAPI is used in Terraform to call RP if enabled.
diff --git a/infra/apim/policies/global.xml b/infra/apim/policies/global.xml
new file mode 100644
index 0000000..290965e
--- /dev/null
+++ b/infra/apim/policies/global.xml
@@ -0,0 +1,24 @@
+<policies>
+  <inbound>
+    <base />
+    <!-- Require JWT with specific audience -->
+    <validate-jwt header-name="Authorization" failed-validation-httpcode="401" failed-validation-error-message="Unauthorized">
+      <openid-config url="https://login.microsoftonline.com/{tenant}/v2.0/.well-known/openid-configuration" />
+      <audiences>
+        <audience>{audience}</audience>
+      </audiences>
+      <require-scheme>Bearer</require-scheme>
+    </validate-jwt>
+    <!-- Rate limit per subscription key -->
+    <rate-limit calls="60" renewal-period="60" />
+  </inbound>
+  <backend>
+    <base />
+  </backend>
+  <outbound>
+    <base />
+  </outbound>
+  <on-error>
+    <base />
+  </on-error>
+</policies>
diff --git a/infra/bicep/main.bicep b/infra/bicep/main.bicep
new file mode 100644
index 0000000..7429e41
--- /dev/null
+++ b/infra/bicep/main.bicep
@@ -0,0 +1,203 @@
+@description('Deployment environment tag, e.g., dev/test/prod')
+param env string
+
+@description('Azure location, e.g., eastus')
+param location string
+
+@description('Base name prefix for resources (letters and numbers only)')
+param baseName string
+@description('Enable EasyAuth (App Service Authentication) with AAD')
+param enableEasyAuth bool = false
+@description('AAD tenant Id for EasyAuth')
+@secure()
+param aadTenantId string = ''
+@description('Allowed audience (App registration client ID or Application ID URI)')
+param aadAllowedAudience string = ''
+
+@minLength(3)
+@maxLength(24)
+var storageName = toLower(replace('${baseName}stor', '-', ''))
+var funcName    = toLower('${baseName}-func-${env}')
+var cosmosName  = toLower(replace('${baseName}-cosmos', '_', ''))
+var searchName  = toLower(replace('${baseName}-search', '_', ''))
+var commName    = toLower(replace('${baseName}-comm', '_', ''))
+var cogName     = toLower(replace('${baseName}-cog', '_', ''))
+
+resource rg 'Microsoft.Resources/resourceGroups@2021-04-01' existing = {
+  name: resourceGroup().name
+}
+
+// Storage account
+resource storage 'Microsoft.Storage/storageAccounts@2023-01-01' = {
+  name: storageName
+  location: location
+  sku: { name: 'Standard_LRS' }
+  kind: 'StorageV2'
+  properties: {
+    allowBlobPublicAccess: false
+    minimumTlsVersion: 'TLS1_2'
+  }
+}
+
+// Cosmos DB account (Serverless)
+resource cosmos 'Microsoft.DocumentDB/databaseAccounts@2023-04-15' = {
+  name: cosmosName
+  location: location
+  kind: 'GlobalDocumentDB'
+  properties: {
+    locations: [
+      {
+        locationName: location
+        failoverPriority: 0
+        isZoneRedundant: false
+      }
+    ]
+    databaseAccountOfferType: 'Standard'
+    enableFreeTier: true
+    capabilities: [
+      { name: 'EnableServerless' }
+    ]
+  }
+}
+
+resource cosmosDb 'Microsoft.DocumentDB/databaseAccounts/sqlDatabases@2023-04-15' = {
+  name: '${cosmos.name}/validation-tracking'
+  properties: { resource: { id: 'validation-tracking' } }
+}
+
+resource containerMetadata 'Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers@2023-04-15' = {
+  name: '${cosmos.name}/${cosmosDb.name}/file-metadata'
+  properties: {
+    resource: {
+      id: 'file-metadata'
+      partitionKey: { paths: ['/file_id'], kind: 'Hash' }
+    }
+  }
+}
+
+resource containerValidations 'Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers@2023-04-15' = {
+  name: '${cosmos.name}/${cosmosDb.name}/validation-results'
+  properties: {
+    resource: {
+      id: 'validation-results'
+      partitionKey: { paths: ['/file_id'], kind: 'Hash' }
+    }
+  }
+}
+
+resource containerEmails 'Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers@2023-04-15' = {
+  name: '${cosmos.name}/${cosmosDb.name}/email-notifications'
+  properties: {
+    resource: {
+      id: 'email-notifications'
+      partitionKey: { paths: ['/file_id'], kind: 'Hash' }
+    }
+  }
+}
+
+resource containerTracking 'Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers@2023-04-15' = {
+  name: '${cosmos.name}/${cosmosDb.name}/change-tracking'
+  properties: {
+    resource: {
+      id: 'change-tracking'
+      partitionKey: { paths: ['/file_id'], kind: 'Hash' }
+    }
+  }
+}
+
+// Communication Services (Email)
+resource comm 'Microsoft.Communication/communicationServices@2023-03-31' = {
+  name: commName
+  location: 'Global'
+  properties: {
+    dataLocation: 'United States'
+  }
+}
+
+// Cognitive Services: Document Intelligence (Form Recognizer)
+resource cog 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
+  name: cogName
+  location: location
+  kind: 'FormRecognizer'
+  sku: { name: 'S0' }
+  properties: {
+    customSubDomainName: toLower('${baseName}${env}docint')
+  }
+}
+
+// Azure AI Search (control plane)
+resource search 'Microsoft.Search/searchServices@2023-11-01' = {
+  name: searchName
+  location: location
+  sku: { name: 'basic' }
+  properties: {
+    replicaCount: 1
+    partitionCount: 1
+    hostingMode: 'default'
+    publicNetworkAccess: 'enabled'
+  }
+}
+
+// Function App Plan (Consumption) and App
+resource plan 'Microsoft.Web/serverfarms@2022-03-01' = {
+  name: '${baseName}-plan-${env}'
+  location: location
+  sku: { name: 'Y1', tier: 'Dynamic' }
+  kind: 'linux'
+}
+
+resource func 'Microsoft.Web/sites@2022-03-01' = {
+  name: funcName
+  location: location
+  kind: 'functionapp,linux'
+  properties: {
+    serverFarmId: plan.id
+    siteConfig: {
+      linuxFxVersion: 'Python|3.10'
+      authSettingsV2: enableEasyAuth ? {
+        platform: { enabled: true }
+        globalValidation: { requireAuthentication: true }
+        identityProviders: {
+          azureActiveDirectory: {
+            enabled: true
+            registration: { openIdIssuer: 'https://login.microsoftonline.com/${aadTenantId}/v2.0' }
+            validation: { allowedAudiences: [ aadAllowedAudience ] }
+          }
+        }
+        login: { tokenStore: { enabled: true } }
+      } : null
+      appSettings: [
+        { name: 'AzureWebJobsStorage', value: storage.listKeys().keys[0].value }
+        { name: 'FUNCTIONS_WORKER_RUNTIME', value: 'python' }
+        { name: 'AZURE_STORAGE_CONNECTION_STRING', value: 'DefaultEndpointsProtocol=https;AccountName=${storage.name};AccountKey=${storage.listKeys().keys[0].value};EndpointSuffix=core.windows.net' }
+        { name: 'AZURE_COSMOSDB_CONNECTION_STRING', value: listKeys(cosmos.id, '2023-04-15').primaryMasterKey }
+        { name: 'AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING', value: listKeys(comm.id, '2023-03-31').primaryConnectionString }
+      ]
+    }
+    httpsOnly: true
+  }
+}
+
+// API Management (optional)
+@description('Deploy API Management (APIM) and import Function App endpoints')
+param enableApim bool = false
+@description('APIM SKU (Consumption or Developer recommended for dev)')
+param apimSkuName string = 'Consumption'
+
+resource apim 'Microsoft.ApiManagement/service@2023-05-01-preview' = if (enableApim) {
+  name: '${baseName}-apim-${env}'
+  location: location
+  sku: { name: apimSkuName, capacity: 0 }
+  properties: {
+    publisherName: 'leftturn'
+    publisherEmail: 'admin@example.com'
+  }
+}
+
+
+output functionAppName string = func.name
+output storageAccountName string = storage.name
+output cosmosAccountName string = cosmos.name
+output searchServiceName string = search.name
+output communicationServiceName string = comm.name
+output cognitiveServicesName string = cog.name
diff --git a/infra/scripts/seed_search.sh b/infra/scripts/seed_search.sh
new file mode 100755
index 0000000..4c19227
--- /dev/null
+++ b/infra/scripts/seed_search.sh
@@ -0,0 +1,88 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Seed Azure AI Search data-plane assets: data source, skillset, index, and indexer.
+# Requires: az CLI and env vars SEARCH_SERVICE, SEARCH_ADMIN_KEY.
+
+set -euo pipefail
+
+if [[ -z "${SEARCH_SERVICE:-}" || -z "${SEARCH_ADMIN_KEY:-}" ]]; then
+  echo "Please set SEARCH_SERVICE and SEARCH_ADMIN_KEY environment variables." >&2
+  exit 1
+fi
+
+API_VERSION="2023-11-01"
+BASE="https://${SEARCH_SERVICE}.search.windows.net"
+
+DS_NAME="${DS_NAME:-contracts-ds}"
+INDEX_NAME="${INDEX_NAME:-contracts}"
+SKILLSET_NAME="${SKILLSET_NAME:-contracts-skillset}"
+INDEXER_NAME="${INDEXER_NAME:-contracts-indexer}"
+
+CONTAINER="${SEARCH_DS_CONTAINER:-contracts}"
+CONN_STRING="${SEARCH_DS_CONNECTION_STRING:-${AZURE_STORAGE_CONNECTION_STRING:-}}"
+
+if [[ -z "$CONN_STRING" ]]; then
+  echo "Set SEARCH_DS_CONNECTION_STRING or AZURE_STORAGE_CONNECTION_STRING for the data source." >&2
+  exit 1
+fi
+
+tmpdir=$(mktemp -d)
+trap 'rm -rf "$tmpdir"' EXIT
+
+# --- Data Source ---
+cat >"$tmpdir/ds.json" <<EOF
+{
+  "name": "$DS_NAME",
+  "type": "azureblob",
+  "credentials": { "connectionString": "$CONN_STRING" },
+  "container": { "name": "$CONTAINER" }
+}
+EOF
+
+echo "Creating/Updating data source: $DS_NAME"
+az rest --method put \
+  --uri "$BASE/datasources/$DS_NAME?api-version=$API_VERSION" \
+  --headers "Content-Type=application/json" "api-key=$SEARCH_ADMIN_KEY" \
+  --body @"$tmpdir/ds.json" >/dev/null
+
+# --- Skillset ---
+echo "Creating/Updating skillset: $SKILLSET_NAME"
+# If AOAI env vars are present, substitute into the skillset for embedding
+if [[ -n "${AZURE_OPENAI_ENDPOINT:-}" && -n "${AZURE_OPENAI_API_KEY:-}" && -n "${AZURE_OPENAI_EMBED_DEPLOYMENT:-}" ]]; then
+  sed "s#\$\{AZURE_OPENAI_ENDPOINT\}#${AZURE_OPENAI_ENDPOINT}#g; s#\$\{AZURE_OPENAI_API_KEY\}#${AZURE_OPENAI_API_KEY}#g; s#\$\{AZURE_OPENAI_EMBED_DEPLOYMENT\}#${AZURE_OPENAI_EMBED_DEPLOYMENT}#g" \
+    infra/search/skillset.contracts.json > "$tmpdir/skillset.json"
+  SKILL_BODY="@$tmpdir/skillset.json"
+else
+  SKILL_BODY="@infra/search/skillset.contracts.json"
+fi
+az rest --method put \
+  --uri "$BASE/skillsets/$SKILLSET_NAME?api-version=$API_VERSION" \
+  --headers "Content-Type=application/json" "api-key=$SEARCH_ADMIN_KEY" \
+  --body $SKILL_BODY >/dev/null
+
+# --- Index ---
+echo "Creating/Updating index: $INDEX_NAME"
+az rest --method put \
+  --uri "$BASE/indexes/$INDEX_NAME?api-version=$API_VERSION" \
+  --headers "Content-Type=application/json" "api-key=$SEARCH_ADMIN_KEY" \
+  --body @infra/search/index.contracts.json >/dev/null
+
+# --- Indexer ---
+cat >"$tmpdir/indexer.json" <<EOF
+{
+  "name": "$INDEXER_NAME",
+  "dataSourceName": "$DS_NAME",
+  "targetIndexName": "$INDEX_NAME",
+  "skillsetName": "$SKILLSET_NAME",
+  "schedule": { "interval": "PT2H" }
+}
+EOF
+
+echo "Creating/Updating indexer: $INDEXER_NAME"
+az rest --method put \
+  --uri "$BASE/indexers/$INDEXER_NAME?api-version=$API_VERSION" \
+  --headers "Content-Type=application/json" "api-key=$SEARCH_ADMIN_KEY" \
+  --body @"$tmpdir/indexer.json" >/dev/null
+
+echo "‚úÖ Search seed completed: ds=$DS_NAME, index=$INDEX_NAME, skillset=$SKILLSET_NAME, indexer=$INDEXER_NAME"
diff --git a/infra/search/datasource.contracts.json b/infra/search/datasource.contracts.json
new file mode 100644
index 0000000..618895d
--- /dev/null
+++ b/infra/search/datasource.contracts.json
@@ -0,0 +1,7 @@
+{
+  "name": "contracts-ds",
+  "type": "azureblob",
+  "credentials": { "connectionString": "<CONNECTION_STRING>" },
+  "container": { "name": "<CONTAINER>" },
+  "dataDeletionDetectionPolicy": { "@odata.type": "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy", "softDeleteColumnName": "isDeleted", "softDeleteMarkerValue": "true" }
+}
diff --git a/infra/search/index.contracts.json b/infra/search/index.contracts.json
new file mode 100644
index 0000000..ff9e639
--- /dev/null
+++ b/infra/search/index.contracts.json
@@ -0,0 +1,16 @@
+{
+  "name": "contracts",
+  "fields": [
+    { "name": "id", "type": "Edm.String", "key": true, "searchable": false },
+    { "name": "carrier", "type": "Edm.String", "searchable": true, "filterable": true, "sortable": true, "facetable": true },
+    { "name": "serviceLevel", "type": "Edm.String", "searchable": true, "filterable": true, "sortable": true, "facetable": true },
+    { "name": "effectiveDate", "type": "Edm.DateTimeOffset", "filterable": true, "sortable": true, "facetable": true },
+    { "name": "file", "type": "Edm.String", "searchable": false, "filterable": true },
+    { "name": "page", "type": "Edm.Int32", "filterable": true, "sortable": true },
+    { "name": "clauseId", "type": "Edm.String", "searchable": false, "filterable": true, "sortable": true },
+    { "name": "text", "type": "Edm.String", "searchable": true, "analyzer": "en.microsoft" },
+    { "name": "embedding", "type": "Collection(Edm.Single)", "searchable": true, "vectorSearchDimensions": 1536, "vectorSearchProfile": "default" }
+  ],
+  "semantic": { "configurations": [ { "name": "default", "prioritizedFields": { "contentFields": [ { "fieldName": "text" } ] } } ] },
+  "vectorSearch": { "algorithms": [ { "name": "hnsw", "kind": "hnsw" } ], "profiles": [ { "name": "default", "algorithm": "hnsw" } ] }
+}
diff --git a/infra/search/indexer.contracts.json b/infra/search/indexer.contracts.json
new file mode 100644
index 0000000..4d93763
--- /dev/null
+++ b/infra/search/indexer.contracts.json
@@ -0,0 +1,12 @@
+{
+  "name": "contracts-indexer",
+  "dataSourceName": "contracts-ds",
+  "targetIndexName": "contracts",
+  "skillsetName": "contracts-skillset",
+  "schedule": { "interval": "PT2H" },
+  "parameters": {
+    "maxFailedItems": 10,
+    "maxFailedItemsPerBatch": 5,
+    "configuration": { "failOnUnsupportedContentType": false, "dataToExtract": "contentAndMetadata" }
+  }
+}
diff --git a/infra/search/skillset.contracts.json b/infra/search/skillset.contracts.json
new file mode 100644
index 0000000..8c8a6d6
--- /dev/null
+++ b/infra/search/skillset.contracts.json
@@ -0,0 +1,38 @@
+{
+  "name": "contracts-skillset",
+  "description": "Extract text, redact PII, and produce embeddings for contract documents.",
+  "skills": [
+    {
+      "@odata.type": "#Microsoft.Skills.Text.OcrSkill",
+      "context": "/document/pages/*",
+      "defaultLanguageCode": "en",
+      "inputs": [ { "name": "image", "source": "/document/pages/*/image" } ],
+      "outputs": [ { "name": "text", "targetName": "ocrText" } ]
+    },
+    {
+      "@odata.type": "#Microsoft.Skills.Text.EntityRecognitionSkill",
+      "context": "/document",
+      "categories": [ "Person", "Email", "PhoneNumber" ],
+      "inputs": [ { "name": "text", "source": "/document/pages/*/ocrText" } ],
+      "outputs": [ { "name": "entities", "targetName": "entities" } ]
+    },
+    {
+      "@odata.type": "#Microsoft.Skills.Text.PiiDetectionSkill",
+      "context": "/document",
+      "inputs": [ { "name": "text", "source": "/document/pages/*/ocrText" } ],
+      "outputs": [ { "name": "text", "targetName": "redactedText" } ]
+    },
+    {
+      "@odata.type": "#Microsoft.Skills.Custom.AzureOpenAIEmbeddingSkill",
+      "description": "Generate vector embeddings for contract text using Azure OpenAI",
+      "context": "/document/pages/*",
+      "inputs": [ { "name": "text", "source": "/document/pages/*/ocrText" } ],
+      "outputs": [ { "name": "embedding", "targetName": "pageEmbedding" } ],
+      "azureOpenAIParameters": {
+        "resourceUri": "${AZURE_OPENAI_ENDPOINT}",
+        "deploymentId": "${AZURE_OPENAI_EMBED_DEPLOYMENT}",
+        "apiKey": "${AZURE_OPENAI_API_KEY}"
+      }
+    }
+  ]
+}
diff --git a/infra/terraform/main.tf b/infra/terraform/main.tf
new file mode 100644
index 0000000..a7066d5
--- /dev/null
+++ b/infra/terraform/main.tf
@@ -0,0 +1,178 @@
+terraform {
+  required_version = ">= 1.5.0"
+  required_providers {
+    azurerm = {
+      source  = "hashicorp/azurerm"
+      version = ">= 3.111.0"
+    }
+    azapi = {
+      source  = "azure/azapi"
+      version = ">= 1.13.0"
+    }
+  }
+}
+
+provider "azurerm" {
+  features {}
+}
+
+variable "location" { type = string }
+variable "env" { type = string }
+variable "base_name" { type = string }
+
+locals {
+  rg_name      = "${var.base_name}-${var.env}-rg"
+  storage_name = lower(replace("${var.base_name}stor", "-", ""))
+  func_name    = lower("${var.base_name}-${var.env}-func")
+  cosmos_name  = lower(replace("${var.base_name}-cosmos", "_", ""))
+  search_name  = lower(replace("${var.base_name}-search", "_", ""))
+  comm_name    = lower(replace("${var.base_name}-comm", "_", ""))
+  cog_name     = lower(replace("${var.base_name}-cog", "_", ""))
+}
+
+resource "azurerm_resource_group" "rg" {
+  name     = local.rg_name
+  location = var.location
+}
+
+resource "azurerm_storage_account" "sa" {
+  name                     = local.storage_name
+  resource_group_name      = azurerm_resource_group.rg.name
+  location                 = azurerm_resource_group.rg.location
+  account_tier             = "Standard"
+  account_replication_type = "LRS"
+  allow_nested_items_to_be_public = false
+}
+
+resource "azurerm_cosmosdb_account" "cosmos" {
+  name                = local.cosmos_name
+  location            = azurerm_resource_group.rg.location
+  resource_group_name = azurerm_resource_group.rg.name
+  offer_type          = "Standard"
+  kind                = "GlobalDocumentDB"
+
+  consistency_policy {
+    consistency_level = "Eventual"
+  }
+
+  geo_location {
+    location          = azurerm_resource_group.rg.location
+    failover_priority = 0
+  }
+
+  capability { name = "EnableServerless" }
+}
+
+resource "azurerm_cosmosdb_sql_database" "db" {
+  name                = "validation-tracking"
+  resource_group_name = azurerm_resource_group.rg.name
+  account_name        = azurerm_cosmosdb_account.cosmos.name
+}
+
+resource "azurerm_cosmosdb_sql_container" "metadata" {
+  name                  = "file-metadata"
+  resource_group_name   = azurerm_resource_group.rg.name
+  account_name          = azurerm_cosmosdb_account.cosmos.name
+  database_name         = azurerm_cosmosdb_sql_database.db.name
+  partition_key_path    = "/file_id"
+}
+
+resource "azurerm_cosmosdb_sql_container" "validations" {
+  name                  = "validation-results"
+  resource_group_name   = azurerm_resource_group.rg.name
+  account_name          = azurerm_cosmosdb_account.cosmos.name
+  database_name         = azurerm_cosmosdb_sql_database.db.name
+  partition_key_path    = "/file_id"
+}
+
+resource "azurerm_cosmosdb_sql_container" "emails" {
+  name                  = "email-notifications"
+  resource_group_name   = azurerm_resource_group.rg.name
+  account_name          = azurerm_cosmosdb_account.cosmos.name
+  database_name         = azurerm_cosmosdb_sql_database.db.name
+  partition_key_path    = "/file_id"
+}
+
+resource "azurerm_cosmosdb_sql_container" "tracking" {
+  name                  = "change-tracking"
+  resource_group_name   = azurerm_resource_group.rg.name
+  account_name          = azurerm_cosmosdb_account.cosmos.name
+  database_name         = azurerm_cosmosdb_sql_database.db.name
+  partition_key_path    = "/file_id"
+}
+
+resource "azurerm_communication_service" "comm" {
+  name                = local.comm_name
+  resource_group_name = azurerm_resource_group.rg.name
+  data_location       = "United States"
+}
+
+resource "azurerm_cognitive_account" "docint" {
+  name                = local.cog_name
+  resource_group_name = azurerm_resource_group.rg.name
+  location            = azurerm_resource_group.rg.location
+  kind                = "FormRecognizer"
+  sku_name            = "S0"
+}
+
+resource "azurerm_search_service" "search" {
+  name                = local.search_name
+  resource_group_name = azurerm_resource_group.rg.name
+  location            = azurerm_resource_group.rg.location
+  sku                 = "basic"
+  partition_count     = 1
+  replica_count       = 1
+}
+
+resource "azurerm_app_service_plan" "plan" {
+  name                = "${var.base_name}-${var.env}-plan"
+  location            = azurerm_resource_group.rg.location
+  resource_group_name = azurerm_resource_group.rg.name
+  kind                = "Linux"
+  reserved            = true
+
+  sku {
+    tier = "Dynamic"
+    size = "Y1"
+  }
+}
+
+resource "azurerm_linux_function_app" "func" {
+  name                = local.func_name
+  location            = azurerm_resource_group.rg.location
+  resource_group_name = azurerm_resource_group.rg.name
+  service_plan_id     = azurerm_app_service_plan.plan.id
+  storage_account_name       = azurerm_storage_account.sa.name
+  storage_account_access_key = azurerm_storage_account.sa.primary_access_key
+
+  site_config {
+    application_stack {
+      python_version = "3.10"
+    }
+  }
+
+  app_settings = {
+    FUNCTIONS_WORKER_RUNTIME                      = "python"
+    AzureWebJobsStorage                            = azurerm_storage_account.sa.primary_connection_string
+    AZURE_STORAGE_CONNECTION_STRING                = azurerm_storage_account.sa.primary_connection_string
+    AZURE_COSMOSDB_CONNECTION_STRING               = azurerm_cosmosdb_account.cosmos.connection_strings[0]
+    AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING = azurerm_communication_service.comm.primary_connection_string
+  }
+}
+
+# Optional: Microsoft Fabric workspace via AzAPI (requires RP registered)
+resource "azapi_resource" "fabric_ws" {
+  type      = "Microsoft.Fabric/workspaces@2023-11-01-preview"
+  name      = "${var.base_name}-${var.env}-fabric"
+  location  = var.location
+  parent_id = azurerm_resource_group.rg.id
+  body = jsonencode({ properties = { } })
+  lifecycle { ignore_changes = [ body ] }
+}
+
+output function_app_name { value = azurerm_linux_function_app.func.name }
+output storage_account_name { value = azurerm_storage_account.sa.name }
+output cosmos_account_name { value = azurerm_cosmosdb_account.cosmos.name }
+output search_service_name { value = azurerm_search_service.search.name }
+output communication_service_name { value = azurerm_communication_service.comm.name }
+output cognitive_services_name { value = azurerm_cognitive_account.docint.name }
diff --git a/local.settings.json b/local.settings.json
new file mode 100644
index 0000000..69ae4d0
--- /dev/null
+++ b/local.settings.json
@@ -0,0 +1,13 @@
+{
+  "IsEncrypted": false,
+  "Values": {
+    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
+    "FUNCTIONS_WORKER_RUNTIME": "python",
+    "AZURE_STORAGE_CONNECTION_STRING": "",
+    "AZURE_COSMOSDB_CONNECTION_STRING": "",
+    "AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING": "",
+    "AZURE_OPENAI_ENDPOINT": "",
+    "AZURE_OPENAI_API_KEY": "",
+    "AZURE_OPENAI_MODEL": "gpt-4.1"
+  }
+}
\ No newline at end of file
diff --git a/notebooks/carrier_structured.ipynb b/notebooks/carrier_structured.ipynb
new file mode 100644
index 0000000..84bafa5
--- /dev/null
+++ b/notebooks/carrier_structured.ipynb
@@ -0,0 +1,26 @@
+{
+ "cells": [
+  {"cell_type": "markdown", "metadata": {}, "source": ["# Carrier Structured Ingestion\n","Process invoices, tracking, rating outputs and compute variance."]},
+  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
+    "import pandas as pd\n",
+    "from datetime import datetime\n",
+    "\n",
+    "invoices = pd.read_csv('landing/carrier/invoices.csv')\n",
+    "rating = pd.read_csv('landing/carrier/rating.csv')\n",
+    "\n",
+    "invoices['ShipDate'] = pd.to_datetime(invoices['ShipDate'])\n",
+    "rating['RatedAmount'] = rating['RatedAmount'].astype(float)\n",
+    "\n",
+    "variance = invoices.merge(rating, on='InvoiceLineId', how='left')\n",
+    "variance['Variance'] = variance['BilledAmount'] - variance['RatedAmount']\n",
+    "variance['Reason'] = variance['Reason'].fillna('')\n",
+    "\n",
+    "invoices.to_parquet('curated/FactInvoice.parquet', index=False)\n",
+    "variance[['InvoiceLineId','InvoiceId','Carrier','ServiceLevel','SKU','BilledAmount','RatedAmount','Variance','Reason']].to_parquet('curated/RatingOutput.parquet', index=False)\n",
+    "print('Wrote curated Carrier tables at', datetime.utcnow(), 'UTC')\n"
+  ]}
+ ],
+ "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}},
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/carrier_unstructured.ipynb b/notebooks/carrier_unstructured.ipynb
new file mode 100644
index 0000000..705f947
--- /dev/null
+++ b/notebooks/carrier_unstructured.ipynb
@@ -0,0 +1,34 @@
+{
+ "cells": [
+  {"cell_type": "markdown", "metadata": {}, "source": ["# Carrier Agreements & Guides (Unstructured)\n","Parse PDFs with Document Intelligence and produce structured tables."]},
+  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
+    "import os, json\n",
+    "from datetime import datetime\n",
+    "# Placeholder for using Azure Document Intelligence SDK.\n",
+    "# In Fabric, install package 'azure-ai-formrecognizer' or use built-in cognitive integration.\n",
+    "# This example emits normalized JSON ready for downstream tables.\n",
+    "agreements_dir = 'landing/carrier_docs'\n",
+    "rows = []\n",
+    "for f in os.listdir(agreements_dir):\n",
+    "    if not f.lower().endswith('.json'):\n",
+    "        continue\n",
+    "    doc = json.load(open(os.path.join(agreements_dir, f)))\n",
+    "    carrier = doc.get('carrier')\n",
+    "    eff = doc.get('effectiveDate')\n",
+    "    for clause in doc.get('clauses', []):\n",
+    "        rows.append({\n",
+    "            'carrier': carrier,\n",
+    "            'effectiveDate': eff,\n",
+    "            'clauseId': clause.get('id'),\n",
+    "            'text': clause.get('text'),\n",
+    "        })\n",
+    "import pandas as pd\n",
+    "df = pd.DataFrame(rows)\n",
+    "df.to_parquet('curated/ContractClauses.parquet', index=False)\n",
+    "print('Wrote clauses table at', datetime.utcnow(), 'UTC')\n"
+  ]}
+ ],
+ "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}},
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/notebooks/erp_structured.ipynb b/notebooks/erp_structured.ipynb
new file mode 100644
index 0000000..bcf33dd
--- /dev/null
+++ b/notebooks/erp_structured.ipynb
@@ -0,0 +1,48 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# ERP Structured Ingestion\n",
+    "Normalize Orders, SKUs, Customers and write curated tables."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "from datetime import datetime\n",
+    "\n",
+    "# Example: read landing CSVs (replace with OneLake paths in Fabric)\n",
+    "orders = pd.read_csv('landing/erp/orders.csv')\n",
+    "skus = pd.read_csv('landing/erp/skus.csv')\n",
+    "customers = pd.read_csv('landing/erp/customers.csv')\n",
+    "\n",
+    "# Clean and standardize\n",
+    "orders['OrderDate'] = pd.to_datetime(orders['OrderDate'])\n",
+    "skus['SKU'] = skus['SKU'].astype(str).str.strip().str.upper()\n",
+    "\n",
+    "# Star schema example\n",
+    "dim_sku = skus[['SKU','Description','Weight','Length','Width','Height']].drop_duplicates()\n",
+    "dim_customer = customers[['CustomerId','Name','Region']].drop_duplicates()\n",
+    "fact_shipment = orders.rename(columns={'CustomerId':'CustomerId'})\n",
+    "\n",
+    "# Persist (replace with lakehouse Delta writes in Fabric)\n",
+    "dim_sku.to_parquet('curated/DimSKU.parquet', index=False)\n",
+    "dim_customer.to_parquet('curated/DimCustomer.parquet', index=False)\n",
+    "fact_shipment.to_parquet('curated/FactShipment.parquet', index=False)\n",
+    "print('Wrote curated ERP tables at', datetime.utcnow(), 'UTC')\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
+  "language_info": {"name": "python", "version": "3.10"}
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..e997e6a
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,28 @@
+[project]
+name = "leftturn"
+version = "0.1.0"
+requires-python = ">=3.11"
+
+[tool.ruff]
+line-length = 100
+target-version = "py311"
+lint.select = ["E","F","I","UP","B","SIM","RUF","N","BLE"]
+lint.ignore = ["E501"]
+
+
+[tool.ruff.lint.per-file-ignores]
+"tests/*" = ["S101"]
+
+[tool.mypy]
+python_version = "3.11"
+disallow_untyped_defs = true
+no_implicit_optional = true
+warn_unused_ignores = true
+warn_redundant_casts = true
+strict_optional = true
+pretty = true
+
+[tool.pytest.ini_options]
+addopts = "-q"
+testpaths = ["tests"]
+
diff --git a/pytest.ini b/pytest.ini
new file mode 100644
index 0000000..5ee6477
--- /dev/null
+++ b/pytest.ini
@@ -0,0 +1,2 @@
+[pytest]
+testpaths = tests
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..fbcb9af
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,16 @@
+azure-functions==1.18.0
+azure-storage-blob==12.19.0
+azure-cosmos==4.6.0
+azure-communication-email==1.0.0
+azure-ai-inference==1.0.0b4
+pandas==2.2.0
+openpyxl==3.1.2
+python-dotenv==1.0.0
+pydantic==2.5.0
+requests==2.31.0
+httpx==0.27.0
+pytest==7.4.4
+pytest-asyncio==0.23.2
+flake8==6.1.0
+responses==0.25.0
+
diff --git a/src/__init__.py b/src/__init__.py
new file mode 100644
index 0000000..1d1d391
--- /dev/null
+++ b/src/__init__.py
@@ -0,0 +1,2 @@
+"""Top-level package for Azure Excel Data Validation Agent."""
+
diff --git a/src/agents/__init__.py b/src/agents/__init__.py
new file mode 100644
index 0000000..4831a2f
--- /dev/null
+++ b/src/agents/__init__.py
@@ -0,0 +1,16 @@
+"""Agent implementations for the LeftTurn system."""
+
+from .orchestrator import OrchestratorAgent
+from .structured_data_agent import StructuredDataAgent
+from .unstructured_data_agent import UnstructuredDataAgent
+from .domain_agents import DomainAgent, CarrierAgent, CustomerOpsAgent, ClaimsAgent
+
+__all__ = [
+    "OrchestratorAgent",
+    "StructuredDataAgent",
+    "UnstructuredDataAgent",
+    "DomainAgent",
+    "CarrierAgent",
+    "CustomerOpsAgent",
+    "ClaimsAgent",
+]
diff --git a/src/agents/domain_agents.py b/src/agents/domain_agents.py
new file mode 100644
index 0000000..748013f
--- /dev/null
+++ b/src/agents/domain_agents.py
@@ -0,0 +1,61 @@
+"""Chat-facing agents that delegate to the orchestrator."""
+from __future__ import annotations
+from typing import Any
+
+
+class _BaseAgent:
+    def __init__(self, orchestrator: Any) -> None:
+        self._orchestrator = orchestrator
+
+    def handle(self, query: Any) -> Any:
+        return self._orchestrator.handle(query)
+
+
+class DomainAgent(_BaseAgent):
+    """Generic agent for broad logistics questions."""
+
+    @property
+    def default_prompt(self) -> str:
+        """Return the system prompt used for domain-wide queries."""
+        return "General logistics assistant."
+
+
+class CarrierAgent(_BaseAgent):
+    """Agent dedicated to carrier-specific operations and contracts."""
+
+    @property
+    def default_prompt(self) -> str:
+        """Return the system prompt used for carrier questions."""
+        return "Carrier operations assistant."
+
+
+class CustomerOpsAgent(_BaseAgent):
+<<<<<<< Updated upstream
+    """Agent focused on customer operations and support topics."""
+
+    @property
+    def default_prompt(self) -> str:
+        """Return the system prompt used for customer ops questions."""
+        return "Customer operations assistant."
+=======
+    pass
+
+
+class ClaimsAgent(_BaseAgent):
+    """Agent focused on claims/disputes workflows.
+
+    Delegates retrieval and calculations to the orchestrator, while the
+    front-end (Teams or Copilot) can present specialized intents like
+    "open dispute packet" or "explain variance".
+    """
+    pass
+<<<<<<< Updated upstream
+<<<<<<< Updated upstream
+<<<<<<< Updated upstream
+>>>>>>> Stashed changes
+=======
+>>>>>>> Stashed changes
+=======
+>>>>>>> Stashed changes
+=======
+>>>>>>> Stashed changes
diff --git a/src/agents/orchestrator.py b/src/agents/orchestrator.py
new file mode 100644
index 0000000..6c96535
--- /dev/null
+++ b/src/agents/orchestrator.py
@@ -0,0 +1,85 @@
+"""Simple orchestrator that routes queries to specialized agents."""
+from __future__ import annotations
+from typing import Any
+
+
+class OrchestratorAgent:
+    """Routes user queries to structured or unstructured agents.
+
+    Uses deterministic, keyword‚Äëbased intent rules to keep behavior
+    predictable and safe. Swap this out for an LLM classifier if desired.
+    """
+
+    def __init__(
+        self,
+        structured_agent: Any,
+        unstructured_agent: Any,
+        graph_service: Any | None = None,
+    ) -> None:
+        self._structured_agent = structured_agent
+        self._unstructured_agent = unstructured_agent
+        self._graph_service = graph_service
+
+    def handle(self, query: Any) -> Any:
+        """Return a response by delegating to the appropriate agent."""
+        if isinstance(query, tuple):
+            template, params = query
+            return self._structured_agent.query(template, params)
+        if (
+            self._graph_service
+            and isinstance(query, str)
+            and self._needs_graph(query)
+        ):
+            return self._graph_service.get_resource(query)
+        return self._unstructured_agent.search(query)
+
+    def handle_with_citations(self, query: Any) -> dict:
+        """Return tool result with lightweight citations and routing metadata.
+
+        Structure:
+        {
+          "tool": "fabric_sql|ai_search|graph",
+          "result": <tool result>,
+          "citations": [ { ... } ]
+        }
+        """
+        if isinstance(query, tuple):
+            template, params = query
+            data = self._structured_agent.query(template, params)
+            return {
+                "tool": "fabric_sql",
+                "result": data,
+                "citations": [
+                    {
+                        "type": "table",
+                        "source": "fabric",
+                        "template": template,
+                        "parameters": params,
+                    }
+                ],
+            }
+        if (
+            self._graph_service
+            and isinstance(query, str)
+            and self._needs_graph(query)
+        ):
+            data = self._graph_service.get_resource(query)
+            return {
+                "tool": "graph",
+                "result": data,
+                "citations": [
+                    {"type": "graph", "query": query, "count": len(data)}
+                ],
+            }
+        data = self._unstructured_agent.search(query)
+        citations = [
+            {"type": "passage", "rank": i + 1, "excerpt": p[:200]}
+            for i, p in enumerate(data[:5])
+        ]
+        return {"tool": "ai_search", "result": data, "citations": citations}
+
+    @staticmethod
+    def _needs_graph(query: str) -> bool:
+        keywords = {"email", "calendar", "file", "meeting"}
+        text = query.lower()
+        return any(k in text for k in keywords)
diff --git a/src/agents/router.py b/src/agents/router.py
new file mode 100644
index 0000000..1dcb433
--- /dev/null
+++ b/src/agents/router.py
@@ -0,0 +1,22 @@
+from __future__ import annotations
+
+from typing import Literal, TypedDict
+
+
+class ToolCall(TypedDict):
+    tool: Literal["sql", "rag", "graph"]
+    name: str
+    params: dict
+
+
+def classify(query: str) -> ToolCall:
+    q = query.lower()
+    if any(
+        k in q for k in ("variance", "overbill", "sum ", "count ", "rate ", "how much", "total")
+    ):
+        return {"tool": "sql", "name": "variance_summary", "params": {}}
+    if any(
+        k in q for k in ("email from", "calendar on", "file named", "in sharepoint", "from user ")
+    ):
+        return {"tool": "graph", "name": "lookup", "params": {}}
+    return {"tool": "rag", "name": "contract_lookup", "params": {}}
diff --git a/src/agents/structured_data_agent.py b/src/agents/structured_data_agent.py
new file mode 100644
index 0000000..a2df183
--- /dev/null
+++ b/src/agents/structured_data_agent.py
@@ -0,0 +1,22 @@
+"""Agent that handles queries against structured data sources."""
+from __future__ import annotations
+from typing import Any
+
+from src.services.sql_templates import TEMPLATES
+
+
+class StructuredDataAgent:
+    def __init__(
+        self, fabric_agent: Any, templates: dict[str, str] | None = None
+    ) -> None:
+        self._fabric_agent = fabric_agent
+        self._templates = templates or TEMPLATES
+
+    def query(
+        self, template: str, parameters: dict[str, Any] | None = None
+    ) -> Any:
+        """Execute a parameterized SQL statement using an approved template."""
+        sql = self._templates.get(template)
+        if sql is None:
+            raise ValueError(f"Unknown SQL template: {template}")
+        return self._fabric_agent.run_sql_params(sql, parameters or {})
diff --git a/src/agents/unstructured_data_agent.py b/src/agents/unstructured_data_agent.py
new file mode 100644
index 0000000..69a90fe
--- /dev/null
+++ b/src/agents/unstructured_data_agent.py
@@ -0,0 +1,12 @@
+"""Agent for searching unstructured documents."""
+from __future__ import annotations
+from typing import Any
+
+
+class UnstructuredDataAgent:
+    def __init__(self, search_service: Any) -> None:
+        self._search_service = search_service
+
+    def search(self, query: str) -> Any:
+        """Search unstructured content via the configured service."""
+        return self._search_service.search(query)
diff --git a/src/functions/agent_gateway/__init__.py b/src/functions/agent_gateway/__init__.py
new file mode 100644
index 0000000..e6cbafa
--- /dev/null
+++ b/src/functions/agent_gateway/__init__.py
@@ -0,0 +1,176 @@
+import azure.functions as func
+import json
+import os
+import logging
+from datetime import datetime
+
+from src.agents import (
+    OrchestratorAgent,
+    StructuredDataAgent,
+    UnstructuredDataAgent,
+    DomainAgent,
+    CarrierAgent,
+    CustomerOpsAgent,
+    ClaimsAgent,
+)
+from src.services.fabric_data_agent import FabricDataAgent
+from src.services.search_service import SearchService
+from src.services.graph_service import GraphService
+from src.utils.helpers import get_correlation_id, log_function_execution
+from src.utils.pbi import build_pbi_deeplink
+from src.utils.cards import build_answer_card
+
+logger = logging.getLogger(__name__)
+
+agent_gateway_bp = func.Blueprint()
+
+
+def _build_orchestrator() -> OrchestratorAgent:
+    """Create an orchestrator wired to Fabric, Search, and Graph services.
+
+    Configuration is taken from environment variables so this function stays
+    testable and production-ready without code changes across environments.
+    """
+    # Structured: Fabric
+    fabric_endpoint = os.getenv("FABRIC_ENDPOINT", "").strip()
+    fabric_token = os.getenv("FABRIC_TOKEN", "").strip()
+    fabric = FabricDataAgent(fabric_endpoint, token=fabric_token) if fabric_endpoint else None
+
+    # Unstructured: Azure AI Search
+    search_endpoint = os.getenv("SEARCH_ENDPOINT", "").strip()
+    search_index = os.getenv("SEARCH_INDEX", "").strip()
+    search_key = os.getenv("SEARCH_API_KEY", "").strip()
+    search = (
+        SearchService(search_endpoint, search_index, api_key=search_key)
+        if search_endpoint and search_index
+        else None
+    )
+
+    # Microsoft Graph
+    graph_token = os.getenv("GRAPH_TOKEN", "").strip()
+    graph_endpoint = os.getenv("GRAPH_ENDPOINT", "https://graph.microsoft.com/v1.0")
+    graph = GraphService(token=graph_token, endpoint=graph_endpoint) if graph_token else None
+
+    structured = StructuredDataAgent(fabric) if fabric else None
+    unstructured = UnstructuredDataAgent(search) if search else None
+
+    if not structured and not unstructured:
+        raise RuntimeError("No data services configured: set FABRIC_*/SEARCH_* env vars")
+
+    # Fallbacks: If either side is missing, provide a shallow stub that raises clear error
+    class _Missing:
+        def __init__(self, name: str):
+            self._name = name
+
+        def query(self, *_args, **_kwargs):  # for StructuredDataAgent
+            raise RuntimeError(f"{self._name} is not configured")
+
+        def search(self, *_args, **_kwargs):  # for UnstructuredDataAgent
+            raise RuntimeError(f"{self._name} is not configured")
+
+    structured = structured or StructuredDataAgent(_Missing("Fabric Data Agent"))
+    unstructured = unstructured or UnstructuredDataAgent(_Missing("Search Service"))
+
+    return OrchestratorAgent(structured, unstructured, graph)
+
+
+def _resolve_chat_agent(name: str, orchestrator: OrchestratorAgent):
+    name = (name or "domain").lower()
+    if name in ("carrier", "carriers"):
+        return CarrierAgent(orchestrator)
+    if name in ("customer", "custops", "ops"):
+        return CustomerOpsAgent(orchestrator)
+    if name in ("claim", "claims", "dispute"):
+        return ClaimsAgent(orchestrator)
+    return DomainAgent(orchestrator)
+
+
+@agent_gateway_bp.function_name(name="agent_ask")
+@agent_gateway_bp.route(route="agents/{agent}/ask", methods=["POST"])
+async def agent_ask(req: func.HttpRequest) -> func.HttpResponse:
+    """HTTP gateway for LeftTurn chat agents.
+
+    Body: {"query": "text"}
+    Path: /api/agents/{agent}/ask where agent is one of: domain|carrier|customer
+    """
+    started = datetime.now()
+    cid = get_correlation_id(req)
+    try:
+        try:
+            body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"},
+            )
+        query = (body or {}).get("query")
+        fmt = (body or {}).get("format") or req.params.get("format")
+        if not query or not isinstance(query, str):
+            return func.HttpResponse(
+                json.dumps({"error": "'query' is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"},
+            )
+
+        orchestrator = _build_orchestrator()
+        agent = _resolve_chat_agent(req.route_params.get("agent", "domain"), orchestrator)
+
+        logger.info(f"[{cid}] Agent ask to {agent.__class__.__name__}")
+        # Prefer enriched result with evidence when available
+        result_payload = orchestrator.handle_with_citations(query)
+
+        finished = datetime.now()
+        log_function_execution(
+            "agent_ask",
+            started,
+            finished,
+            True,
+            {"agent": agent.__class__.__name__, "correlation_id": cid},
+        )
+
+        # If structured SQL was used, try to provide a Power BI link
+        if result_payload.get("tool") == "fabric_sql":
+            # naive filter inference from query keywords
+            ql = query.lower()
+            filters = {}
+            if "carrier" in ql:
+                filters["vw_Variance/Carrier"] = _extract_value(query, "carrier") or ""
+            if "sku" in ql:
+                filters["vw_Variance/SKU"] = _extract_value(query, "sku") or ""
+            pbi = build_pbi_deeplink({k: v for k, v in filters.items() if v})
+            if pbi:
+                result_payload["powerBiLink"] = pbi
+
+        # If Teams requests a card, return an Adaptive Card payload
+        if fmt and fmt.lower() == "card":
+            card = build_answer_card(result_payload)
+            return func.HttpResponse(
+                json.dumps(card),
+                status_code=200,
+                headers={"Content-Type": "application/json"},
+            )
+
+        return func.HttpResponse(
+            json.dumps({"agent": agent.__class__.__name__, **result_payload}),
+            status_code=200,
+            headers={"Content-Type": "application/json"},
+        )
+
+
+def _extract_value(text: str, key: str) -> str | None:
+    try:
+        import re
+        m = re.search(rf"{key}[:=\s]+([\w\-\.]+)", text, re.IGNORECASE)
+        return m.group(1) if m else None
+    except Exception:
+        return None
+    except Exception as e:
+        finished = datetime.now()
+        log_function_execution("agent_ask", started, finished, False, {"correlation_id": cid})
+        logger.error(f"[{cid}] agent_ask error: {str(e)}")
+        return func.HttpResponse(
+            json.dumps({"error": "Internal server error", "message": str(e)}),
+            status_code=500,
+            headers={"Content-Type": "application/json"},
+        )
diff --git a/src/functions/change_tracker/__init__.py b/src/functions/change_tracker/__init__.py
new file mode 100644
index 0000000..6ec8e49
--- /dev/null
+++ b/src/functions/change_tracker/__init__.py
@@ -0,0 +1,418 @@
+import azure.functions as func
+import logging
+import json
+import base64
+from datetime import datetime
+from src.services.excel_service import ExcelService
+from src.services.validation_service import ValidationService
+from src.services.storage_service import StorageService
+from src.services.email_service import EmailService
+from src.models.validation_models import ValidationStatus
+from src.utils.helpers import generate_file_hash, log_function_execution, get_correlation_id
+
+logger = logging.getLogger(__name__)
+
+# Create function blueprint
+change_tracker_bp = func.Blueprint()
+
+@change_tracker_bp.function_name(name="verify_changes")
+@change_tracker_bp.route(route="verify", methods=["POST"])
+async def verify_changes(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Verify that changes have been made to a previously failed validation
+    
+    Expected request body:
+    {
+        "original_file_id": "file_identifier",
+        "updated_file_data": "base64_encoded_updated_file",
+        "updated_filename": "updated_file.xlsx"
+    }
+    """
+    start_time = datetime.now()
+    correlation_id = get_correlation_id(req)
+    
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        required_fields = ['original_file_id', 'updated_file_data', 'updated_filename']
+        missing_fields = [field for field in required_fields if field not in req_body]
+        
+        if missing_fields:
+            return func.HttpResponse(
+                json.dumps({"error": f"Missing required fields: {', '.join(missing_fields)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        original_file_id = req_body['original_file_id']
+        updated_filename = req_body['updated_filename']
+        
+        # Decode updated file data
+        try:
+            updated_file_data = base64.b64decode(req_body['updated_file_data'])
+        except Exception as e:
+            return func.HttpResponse(
+                json.dumps({"error": f"Invalid file data encoding: {str(e)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Initialize services
+        excel_service = ExcelService()
+        validation_service = ValidationService()
+        storage_service = StorageService()
+        email_service = EmailService()
+        
+        # Get original file metadata
+        original_metadata = storage_service.get_file_metadata(original_file_id)
+        if not original_metadata:
+            return func.HttpResponse(
+                json.dumps({"error": "Original file not found"}),
+                status_code=404,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        logger.info(f"[{correlation_id}] Verifying changes for file: {original_file_id}")
+        
+        # Parse updated Excel file
+        updated_sheets_dict, updated_metadata = excel_service.parse_excel_file(
+            updated_file_data, updated_filename
+        )
+        
+        # Generate hash for comparison
+        updated_file_hash = generate_file_hash(updated_file_data)
+        
+        # Store updated file
+        storage_service.store_file(updated_file_data, updated_metadata.file_id, updated_filename)
+        storage_service.store_file_metadata(updated_metadata)
+        
+        # Extract data for validation
+        updated_validation_data = excel_service.extract_data_for_validation(updated_sheets_dict)
+        
+        # Re-validate the updated data using the same rules as the original
+        # (In a full implementation, you'd retrieve the original validation rules)
+        updated_validation_result = validation_service.validate_data(
+            updated_validation_data,
+            updated_metadata.file_id,
+            custom_rules=[]  # Use default rules for now
+        )
+        
+        # Store updated validation result
+        storage_service.store_validation_result(updated_validation_result)
+        
+        # Update existing change tracking record for the original file (if present)
+        latest_track = storage_service.get_latest_tracking_for_file(original_file_id)
+        if latest_track:
+            storage_service.update_change_tracking(
+                tracking_id=latest_track.tracking_id,
+                updated_file_hash=updated_file_hash,
+                change_description=change_description,
+                file_id=original_file_id,
+            )
+        
+        # Determine if changes were successful
+        changes_successful = updated_validation_result.total_errors == 0
+        
+        # Compare file contents to detect actual changes
+        change_description = "File updated and re-validated"
+        
+        # Prepare response
+        response_data = {
+            "original_file_id": original_file_id,
+            "updated_file_id": updated_metadata.file_id,
+            "updated_validation_id": updated_validation_result.validation_id,
+            "changes_successful": changes_successful,
+            "change_description": change_description,
+            "updated_file_hash": updated_file_hash,
+            "validation_status": updated_validation_result.status.value,
+            "remaining_errors": updated_validation_result.total_errors,
+            "remaining_warnings": updated_validation_result.total_warnings,
+            "timestamp": datetime.now().isoformat()
+        }
+        
+        # If validation passed, send success notification
+        if changes_successful:
+            # Mark the latest original validation as corrected
+            latest = storage_service.get_latest_validation_for_file(original_file_id)
+            if latest and latest.status.value == "failed":
+                storage_service.update_validation_status(latest.validation_id, ValidationStatus.CORRECTED)
+
+            # Extract email addresses from updated data
+            recipient_emails = excel_service.extract_email_column(updated_validation_data)
+            
+            if recipient_emails:
+                success_notifications = email_service.send_validation_success_notification(
+                    updated_metadata.file_id, recipient_emails
+                )
+                
+                # Store notification records
+                for notification in success_notifications:
+                    storage_service.store_email_notification(notification)
+                
+                response_data["success_notifications_sent"] = len(success_notifications)
+            
+            logger.info(f"Changes verified successfully for file: {original_file_id}")
+        
+        else:
+            # If still has errors, send updated failure notification
+            recipient_emails = excel_service.extract_email_column(updated_validation_data)
+            
+            if recipient_emails:
+                failure_notifications = email_service.send_validation_failure_notification(
+                    updated_validation_result, recipient_emails
+                )
+                
+                # Store notification records
+                for notification in failure_notifications:
+                    storage_service.store_email_notification(notification)
+                
+                response_data["failure_notifications_sent"] = len(failure_notifications)
+            
+            # Include error details
+            response_data["remaining_errors_details"] = [
+                {
+                    "row": error.row,
+                    "column": error.column,
+                    "value": str(error.value),
+                    "message": error.message,
+                    "suggested_correction": error.suggested_correction
+                }
+                for error in updated_validation_result.errors[:10]
+            ]
+            
+            logger.warning(f"Changes verification found remaining errors for file: {original_file_id}")
+        
+        end_time = datetime.now()
+        log_function_execution(
+            "verify_changes",
+            start_time,
+            end_time,
+            True,
+            {
+                "original_file_id": original_file_id,
+                "changes_successful": changes_successful,
+                "remaining_errors": updated_validation_result.total_errors,
+                "correlation_id": correlation_id
+            }
+        )
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        end_time = datetime.now()
+        log_function_execution("verify_changes", start_time, end_time, False, {"correlation_id": correlation_id})
+        
+        logger.error(f"Error verifying changes: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to verify changes",
+                "details": str(e)
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@change_tracker_bp.function_name(name="get_change_history")
+@change_tracker_bp.route(route="history/{file_id}", methods=["GET"])
+async def get_change_history(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Get change history for a file
+    """
+    try:
+        file_id = req.route_params.get('file_id')
+        
+        if not file_id:
+            return func.HttpResponse(
+                json.dumps({"error": "file_id is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        storage_service = StorageService()
+        records = storage_service.get_change_history(file_id)
+
+        response = {
+            "file_id": file_id,
+            "changes": [
+                {
+                    "tracking_id": r.tracking_id,
+                    "timestamp": (r.change_timestamp.isoformat() if r.change_timestamp else None),
+                    "description": r.change_description,
+                    "verified": r.verified,
+                    "original_file_hash": r.original_file_hash,
+                    "updated_file_hash": r.updated_file_hash,
+                    "validation_id": r.validation_id,
+                }
+                for r in records
+            ],
+            "total_changes": len(records),
+            "last_updated": (records[0].change_timestamp.isoformat() if records and records[0].change_timestamp else None),
+        }
+
+        return func.HttpResponse(
+            json.dumps(response),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error getting change history: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to get change history"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@change_tracker_bp.function_name(name="compare_files")
+@change_tracker_bp.route(route="compare", methods=["POST"])
+async def compare_files(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Compare two Excel files to identify changes
+    
+    Expected request body:
+    {
+        "original_file_data": "base64_encoded_original",
+        "updated_file_data": "base64_encoded_updated",
+        "comparison_type": "content|structure|both"  // Optional, defaults to "both"
+    }
+    """
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        if 'original_file_data' not in req_body or 'updated_file_data' not in req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "original_file_data and updated_file_data are required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        comparison_type = req_body.get('comparison_type', 'both')
+        
+        # Decode file data
+        try:
+            original_data = base64.b64decode(req_body['original_file_data'])
+            updated_data = base64.b64decode(req_body['updated_file_data'])
+        except Exception as e:
+            return func.HttpResponse(
+                json.dumps({"error": f"Invalid file data encoding: {str(e)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Initialize Excel service
+        excel_service = ExcelService()
+        
+        # Parse both files
+        original_sheets, _ = excel_service.parse_excel_file(original_data, "original.xlsx")
+        updated_sheets, _ = excel_service.parse_excel_file(updated_data, "updated.xlsx")
+        
+        # Generate hashes for comparison
+        original_hash = generate_file_hash(original_data)
+        updated_hash = generate_file_hash(updated_data)
+        
+        # Basic comparison results
+        comparison_result = {
+            "files_identical": original_hash == updated_hash,
+            "original_hash": original_hash,
+            "updated_hash": updated_hash,
+            "comparison_type": comparison_type,
+            "timestamp": datetime.now().isoformat()
+        }
+        
+        if not comparison_result["files_identical"]:
+            # Basic structural/content comparison; extend with cell‚Äëlevel diffs if needed
+            changes = []
+            
+            # Compare sheet structure
+            if comparison_type in ['structure', 'both']:
+                original_sheet_names = set(original_sheets.keys())
+                updated_sheet_names = set(updated_sheets.keys())
+                
+                if original_sheet_names != updated_sheet_names:
+                    changes.append({
+                        "type": "structure_change",
+                        "description": "Sheet names changed",
+                        "details": {
+                            "added_sheets": list(updated_sheet_names - original_sheet_names),
+                            "removed_sheets": list(original_sheet_names - updated_sheet_names)
+                        }
+                    })
+            
+            # Compare content (basic comparison)
+            if comparison_type in ['content', 'both']:
+                for sheet_name in original_sheets.keys():
+                    if sheet_name in updated_sheets:
+                        original_df = original_sheets[sheet_name]
+                        updated_df = updated_sheets[sheet_name]
+                        
+                        # Compare dimensions
+                        if original_df.shape != updated_df.shape:
+                            changes.append({
+                                "type": "content_change",
+                                "sheet": sheet_name,
+                                "description": "Sheet dimensions changed",
+                                "details": {
+                                    "original_shape": original_df.shape,
+                                    "updated_shape": updated_df.shape
+                                }
+                            })
+            
+            comparison_result["changes"] = changes
+            comparison_result["total_changes"] = len(changes)
+        
+        return func.HttpResponse(
+            json.dumps(comparison_result),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error comparing files: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to compare files",
+                "details": str(e)
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
diff --git a/src/functions/data_validator/__init__.py b/src/functions/data_validator/__init__.py
new file mode 100644
index 0000000..851f2df
--- /dev/null
+++ b/src/functions/data_validator/__init__.py
@@ -0,0 +1,339 @@
+import azure.functions as func
+import logging
+import json
+from datetime import datetime
+from src.services.validation_service import ValidationService
+from src.services.storage_service import StorageService
+from src.models.validation_models import ValidationRule
+from src.utils.helpers import log_function_execution
+
+logger = logging.getLogger(__name__)
+
+# Create function blueprint
+data_validator_bp = func.Blueprint()
+
+@data_validator_bp.function_name(name="validate_data")
+@data_validator_bp.route(route="validate", methods=["POST"])
+async def validate_data(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Standalone data validation function
+    
+    Expected request body:
+    {
+        "data": [
+            {"column1": "value1", "column2": "value2"},
+            ...
+        ],
+        "validation_rules": [...],  // Custom validation rules
+        "data_id": "optional_identifier"
+    }
+    """
+    start_time = datetime.now()
+    
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        if 'data' not in req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "data field is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Convert data to DataFrame
+        import pandas as pd
+        data = pd.DataFrame(req_body['data'])
+        
+        # Parse validation rules
+        custom_rules = []
+        if 'validation_rules' in req_body:
+            try:
+                custom_rules = [ValidationRule(**rule) for rule in req_body['validation_rules']]
+            except Exception as e:
+                logger.warning(f"Error parsing validation rules: {str(e)}")
+        
+        # Generate data ID if not provided
+        data_id = req_body.get('data_id', f"data_{int(datetime.now().timestamp())}")
+        
+        # Initialize validation service
+        validation_service = ValidationService()
+        
+        # Perform validation
+        validation_result = validation_service.validate_data(data, data_id, custom_rules)
+        
+        # Prepare response
+        response_data = {
+            "data_id": data_id,
+            "validation_id": validation_result.validation_id,
+            "status": validation_result.status.value,
+            "total_errors": validation_result.total_errors,
+            "total_warnings": validation_result.total_warnings,
+            "processed_rows": validation_result.processed_rows,
+            "timestamp": validation_result.timestamp.isoformat(),
+            "errors": [
+                {
+                    "row": error.row,
+                    "column": error.column,
+                    "value": str(error.value),
+                    "message": error.message,
+                    "severity": error.severity,
+                    "suggested_correction": error.suggested_correction
+                }
+                for error in validation_result.errors
+            ],
+            "warnings": [
+                {
+                    "row": warning.row,
+                    "column": warning.column,
+                    "value": str(warning.value),
+                    "message": warning.message,
+                    "severity": warning.severity,
+                    "suggested_correction": warning.suggested_correction
+                }
+                for warning in validation_result.warnings
+            ]
+        }
+        
+        end_time = datetime.now()
+        log_function_execution(
+            "validate_data",
+            start_time,
+            end_time,
+            True,
+            {
+                "data_id": data_id,
+                "validation_status": validation_result.status.value,
+                "errors": validation_result.total_errors
+            }
+        )
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        end_time = datetime.now()
+        log_function_execution("validate_data", start_time, end_time, False)
+        
+        logger.error(f"Error validating data: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to validate data",
+                "details": str(e)
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@data_validator_bp.function_name(name="create_validation_rules")
+@data_validator_bp.route(route="rules", methods=["POST"])
+async def create_validation_rules(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Create and test validation rules
+    
+    Expected request body:
+    {
+        "rule_name": "Custom Rule",
+        "rule_type": "format|range|data_type|custom",
+        "description": "Rule description",
+        "parameters": {...},
+        "severity": "error|warning|info",
+        "test_data": [{"col": "value"}, ...]  // Optional test data
+    }
+    """
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        required_fields = ['rule_name', 'rule_type', 'description', 'parameters']
+        missing_fields = [field for field in required_fields if field not in req_body]
+        
+        if missing_fields:
+            return func.HttpResponse(
+                json.dumps({"error": f"Missing required fields: {', '.join(missing_fields)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Create validation rule
+        rule_id = f"custom_{int(datetime.now().timestamp())}"
+        
+        try:
+            validation_rule = ValidationRule(
+                rule_id=rule_id,
+                rule_name=req_body['rule_name'],
+                description=req_body['description'],
+                rule_type=req_body['rule_type'],
+                parameters=req_body['parameters'],
+                severity=req_body.get('severity', 'error')
+            )
+        except Exception as e:
+            return func.HttpResponse(
+                json.dumps({"error": f"Invalid validation rule: {str(e)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        response_data = {
+            "rule_id": rule_id,
+            "rule": validation_rule.dict(),
+            "status": "created"
+        }
+        
+        # Test the rule if test data is provided
+        if 'test_data' in req_body and req_body['test_data']:
+            try:
+                import pandas as pd
+                test_df = pd.DataFrame(req_body['test_data'])
+                
+                validation_service = ValidationService()
+                test_result = validation_service.validate_data(test_df, "test", [validation_rule])
+                
+                response_data["test_result"] = {
+                    "status": test_result.status.value,
+                    "errors": len(test_result.errors),
+                    "warnings": len(test_result.warnings),
+                    "sample_errors": [
+                        {
+                            "row": error.row,
+                            "column": error.column,
+                            "message": error.message
+                        }
+                        for error in test_result.errors[:3]  # First 3 errors
+                    ]
+                }
+                
+            except Exception as e:
+                response_data["test_result"] = {
+                    "status": "test_failed",
+                    "error": str(e)
+                }
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=201,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error creating validation rule: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to create validation rule"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@data_validator_bp.function_name(name="get_validation_templates")
+@data_validator_bp.route(route="templates", methods=["GET"])
+async def get_validation_templates(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Get predefined validation rule templates
+    """
+    try:
+        templates = {
+            "email_validation": {
+                "rule_name": "Email Format Validation",
+                "rule_type": "format",
+                "description": "Validate email addresses",
+                "parameters": {
+                    "pattern": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
+                    "columns": ["email"]
+                },
+                "severity": "error"
+            },
+            "phone_validation": {
+                "rule_name": "Phone Number Validation",
+                "rule_type": "format",
+                "description": "Validate phone number format",
+                "parameters": {
+                    "pattern": r"^\+?1?-?(\d{3})-?(\d{3})-?(\d{4})$",
+                    "columns": ["phone"]
+                },
+                "severity": "error"
+            },
+            "numeric_range": {
+                "rule_name": "Numeric Range Validation",
+                "rule_type": "range",
+                "description": "Validate numeric values within range",
+                "parameters": {
+                    "min": 0,
+                    "max": 100,
+                    "columns": ["score", "percentage"]
+                },
+                "severity": "error"
+            },
+            "required_fields": {
+                "rule_name": "Required Fields",
+                "rule_type": "custom",
+                "description": "Ensure required fields are not empty",
+                "parameters": {
+                    "required_columns": ["name", "email", "id"]
+                },
+                "severity": "error"
+            },
+            "date_format": {
+                "rule_name": "Date Format Validation",
+                "rule_type": "format",
+                "description": "Validate date format (YYYY-MM-DD)",
+                "parameters": {
+                    "pattern": r"^\d{4}-\d{2}-\d{2}$",
+                    "columns": ["date", "created_date"]
+                },
+                "severity": "warning"
+            }
+        }
+        
+        return func.HttpResponse(
+            json.dumps({
+                "templates": templates,
+                "total_templates": len(templates)
+            }),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error getting validation templates: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to get validation templates"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
diff --git a/src/functions/email_sender/__init__.py b/src/functions/email_sender/__init__.py
new file mode 100644
index 0000000..b165052
--- /dev/null
+++ b/src/functions/email_sender/__init__.py
@@ -0,0 +1,372 @@
+import azure.functions as func
+import logging
+import json
+from datetime import datetime
+from src.services.email_service import EmailService
+from src.services.storage_service import StorageService
+from src.models.validation_models import ValidationResult, ValidationStatus
+from src.utils.helpers import log_function_execution, validate_email_format, get_correlation_id
+
+logger = logging.getLogger(__name__)
+
+# Create function blueprint
+email_sender_bp = func.Blueprint()
+
+@email_sender_bp.function_name(name="send_notification")
+@email_sender_bp.route(route="notify", methods=["POST"])
+async def send_notification(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Send email notification for validation results
+    
+    Expected request body:
+    {
+        "validation_id": "validation_identifier",
+        "recipient_emails": ["email1@domain.com", "email2@domain.com"],
+        "notification_type": "failure|success|reminder"
+    }
+    """
+    start_time = datetime.now()
+    correlation_id = get_correlation_id(req)
+    
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        if 'validation_id' not in req_body or 'recipient_emails' not in req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "validation_id and recipient_emails are required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        validation_id = req_body['validation_id']
+        recipient_emails = req_body['recipient_emails']
+        notification_type = req_body.get('notification_type', 'failure')
+        
+        # Validate email addresses
+        valid_emails = []
+        for email in recipient_emails:
+            if validate_email_format(email):
+                valid_emails.append(email)
+            else:
+                logger.warning(f"[{correlation_id}] Invalid email format: {email}")
+        
+        if not valid_emails:
+            return func.HttpResponse(
+                json.dumps({"error": "No valid email addresses provided"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Initialize services
+        email_service = EmailService()
+        storage_service = StorageService()
+        
+        notifications_sent = []
+        
+        if notification_type in ['failure', 'reminder']:
+            # Get validation result
+            validation_result = storage_service.get_validation_result(validation_id)
+            if not validation_result:
+                return func.HttpResponse(
+                    json.dumps({"error": "Validation result not found"}),
+                    status_code=404,
+                    headers={"Content-Type": "application/json"}
+                )
+            
+            # Send failure/reminder notifications
+            notifications = email_service.send_validation_failure_notification(
+                validation_result, valid_emails
+            )
+            notifications_sent.extend(notifications)
+            
+        elif notification_type == 'success':
+            # Prefer retrieving the validation to get the true file_id
+            file_id = None
+            validation_result = storage_service.get_validation_result(validation_id)
+            if validation_result:
+                file_id = validation_result.file_id
+            else:
+                # Fallback: use provided validation_id as-is
+                file_id = validation_id
+            # Send success notifications
+            notifications = email_service.send_validation_success_notification(
+                file_id, valid_emails
+            )
+            notifications_sent.extend(notifications)
+        
+        # Store notification records
+        for notification in notifications_sent:
+            storage_service.store_email_notification(notification)
+        
+        response_data = {
+            "validation_id": validation_id,
+            "notification_type": notification_type,
+            "notifications_sent": len(notifications_sent),
+            "recipients": valid_emails,
+            "timestamp": datetime.now().isoformat(),
+            "notification_ids": [n.notification_id for n in notifications_sent]
+        }
+        
+        end_time = datetime.now()
+        log_function_execution(
+            "send_notification",
+            start_time,
+            end_time,
+            True,
+            {
+                "validation_id": validation_id,
+                "notification_type": notification_type,
+                "recipients": len(valid_emails),
+                "correlation_id": correlation_id
+            }
+        )
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        end_time = datetime.now()
+        log_function_execution("send_notification", start_time, end_time, False, {"correlation_id": correlation_id})
+        
+        logger.error(f"Error sending notification: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to send notification",
+                "details": str(e)
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@email_sender_bp.function_name(name="send_custom_email")
+@email_sender_bp.route(route="custom", methods=["POST"])
+async def send_custom_email(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Send custom email notification
+    
+    Expected request body:
+    {
+        "recipient_emails": ["email@domain.com"],
+        "subject": "Custom Subject",
+        "message": "Custom message content",
+        "message_type": "html|text"  // Optional, defaults to text
+    }
+    """
+    try:
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        required_fields = ['recipient_emails', 'subject', 'message']
+        missing_fields = [field for field in required_fields if field not in req_body]
+        
+        if missing_fields:
+            return func.HttpResponse(
+                json.dumps({"error": f"Missing required fields: {', '.join(missing_fields)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        recipient_emails = req_body['recipient_emails']
+        subject = req_body['subject']
+        message = req_body['message']
+        message_type = req_body.get('message_type', 'text')
+        
+        # Validate email addresses
+        valid_emails = [email for email in recipient_emails if validate_email_format(email)]
+        
+        if not valid_emails:
+            return func.HttpResponse(
+                json.dumps({"error": "No valid email addresses provided"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Initialize email service
+        email_service = EmailService()
+        
+        if not email_service.email_client:
+            return func.HttpResponse(
+                json.dumps({"error": "Email service not configured"}),
+                status_code=503,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Send emails
+        sent_count = 0
+        failed_emails = []
+        
+        for email in valid_emails:
+            try:
+                # Prepare email content
+                email_message = {
+                    "senderAddress": email_service.sender_email,
+                    "recipients": {
+                        "to": [{"address": email}]
+                    },
+                    "content": {
+                        "subject": subject
+                    }
+                }
+                
+                # Set content based on message type
+                if message_type.lower() == 'html':
+                    email_message["content"]["html"] = message
+                else:
+                    email_message["content"]["plainText"] = message
+                
+                # Send email
+                poller = email_service.email_client.begin_send(email_message)
+                result = poller.result()
+                
+                sent_count += 1
+                logger.info(f"Custom email sent to {email}: {result.message_id}")
+                
+            except Exception as e:
+                failed_emails.append({"email": email, "error": str(e)})
+                logger.error(f"Failed to send custom email to {email}: {str(e)}")
+        
+        response_data = {
+            "emails_sent": sent_count,
+            "total_recipients": len(valid_emails),
+            "failed_emails": failed_emails,
+            "subject": subject,
+            "timestamp": datetime.now().isoformat()
+        }
+        
+        status_code = 200 if sent_count > 0 else 500
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=status_code,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error sending custom email: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to send custom email"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@email_sender_bp.function_name(name="get_notification_status")
+@email_sender_bp.route(route="status/{notification_id}", methods=["GET"])
+async def get_notification_status(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Get the status of a sent notification
+    """
+    try:
+        notification_id = req.route_params.get('notification_id')
+        
+        if not notification_id:
+            return func.HttpResponse(
+                json.dumps({"error": "notification_id is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Query storage for the email notification record
+        storage_service = StorageService()
+        record = storage_service.get_email_notification(notification_id)
+        if not record:
+            return func.HttpResponse(
+                json.dumps({"error": "Notification not found"}),
+                status_code=404,
+                headers={"Content-Type": "application/json"}
+            )
+
+        response_data = {
+            "notification_id": record.notification_id,
+            "file_id": record.file_id,
+            "validation_id": record.validation_id,
+            "recipient_email": record.recipient_email,
+            "status": record.delivery_status,
+            "sent_timestamp": record.sent_timestamp.isoformat(),
+            "correction_deadline": record.correction_deadline.isoformat() if record.correction_deadline else None,
+        }
+
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error getting notification status: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to get notification status"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@email_sender_bp.function_name(name="send_reminders")
+@email_sender_bp.timer_trigger(schedule="0 0 9 * * *")  # Daily at 9 AM UTC
+async def send_reminder_emails(timer: func.TimerRequest) -> None:
+    """Send reminder emails for failed validations older than N days.
+
+    Controls:
+    - REMINDER_DAYS_OLD: integer days threshold (default 3)
+    - REMINDER_MAX_ITEMS: safety cap (default 100)
+    """
+    try:
+        import os
+        days = int(os.getenv("REMINDER_DAYS_OLD", "3"))
+        cap = int(os.getenv("REMINDER_MAX_ITEMS", "100"))
+
+        storage_service = StorageService()
+        email_service = EmailService()
+
+        candidates = storage_service.list_failed_validations(days_older_than=days, limit=cap)
+        total_notifications = 0
+        for vr in candidates:
+            recipients = storage_service.list_email_recipients_for_validation(vr.validation_id)
+            if not recipients:
+                continue
+            notes = email_service.send_validation_failure_notification(vr, recipients)
+            for n in notes:
+                storage_service.store_email_notification(n)
+            total_notifications += len(notes)
+
+        logger.info(f"Reminder job processed {len(candidates)} validations; sent {total_notifications} notifications")
+    except Exception as e:
+        logger.error(f"Error in reminder email job: {str(e)}")
diff --git a/src/functions/excel_processor/__init__.py b/src/functions/excel_processor/__init__.py
new file mode 100644
index 0000000..fd4b80d
--- /dev/null
+++ b/src/functions/excel_processor/__init__.py
@@ -0,0 +1,303 @@
+import azure.functions as func
+import logging
+import json
+from datetime import datetime
+from src.services.excel_service import ExcelService
+from src.services.validation_service import ValidationService
+from src.services.email_service import EmailService
+from src.services.storage_service import StorageService
+from src.models.validation_models import ProcessingRequest, ValidationRule
+from src.utils.helpers import generate_file_hash, log_function_execution, get_correlation_id
+
+logger = logging.getLogger(__name__)
+
+# Create function blueprint
+excel_processor_bp = func.Blueprint()
+
+@excel_processor_bp.function_name(name="process_excel_file")
+@excel_processor_bp.route(route="process", methods=["POST"])
+async def process_excel_file(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Main function to process uploaded Excel files
+    
+    Expected request body:
+    {
+        "filename": "data.xlsx",
+        "file_data": "base64_encoded_file_data",
+        "validation_rules": [...],  // Optional custom rules
+        "email_lookup_field": "email",  // Optional, defaults to "email"
+        "requester_email": "user@domain.com"  // Optional
+    }
+    """
+    start_time = datetime.now()
+    correlation_id = get_correlation_id(req)
+    
+    try:
+        # Parse request
+        try:
+            req_body = req.get_json()
+        except Exception:
+            return func.HttpResponse(
+                json.dumps({"error": "Invalid JSON in request body"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        if not req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "Request body is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Validate required fields
+        if 'filename' not in req_body or 'file_data' not in req_body:
+            return func.HttpResponse(
+                json.dumps({"error": "filename and file_data are required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        filename = req_body['filename']
+        
+        # Decode base64 file data
+        import base64
+        try:
+            file_data = base64.b64decode(req_body['file_data'])
+        except Exception as e:
+            return func.HttpResponse(
+                json.dumps({"error": f"Invalid file data encoding: {str(e)}"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Enforce file size limit if configured
+        import os
+        try:
+            max_mb = float(os.getenv('MAX_FILE_SIZE_MB', '50'))
+        except ValueError:
+            max_mb = 50.0
+        max_bytes = int(max_mb * 1024 * 1024)
+        if len(file_data) > max_bytes:
+            return func.HttpResponse(
+                json.dumps({
+                    "error": "File too large",
+                    "message": f"Max allowed size is {int(max_mb)} MB"
+                }),
+                status_code=413,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Initialize services
+        excel_service = ExcelService()
+        validation_service = ValidationService()
+        email_service = EmailService()
+        storage_service = StorageService()
+        
+        # Validate file format against allowed types
+        if not excel_service.validate_file_format(filename):
+            return func.HttpResponse(
+                json.dumps({"error": "Unsupported file format"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        logger.info(f"[{correlation_id}] Processing Excel file: {filename}")
+        
+        # Parse Excel file
+        sheets_dict, metadata = excel_service.parse_excel_file(file_data, filename)
+        
+        # Store file and metadata
+        storage_service.store_file(file_data, metadata.file_id, filename)
+        storage_service.store_file_metadata(metadata)
+        
+        # Extract data for validation
+        validation_data = excel_service.extract_data_for_validation(sheets_dict)
+        
+        # Parse custom validation rules if provided
+        custom_rules = []
+        if 'validation_rules' in req_body:
+            try:
+                custom_rules = [ValidationRule(**rule) for rule in req_body['validation_rules']]
+            except Exception as e:
+                logger.warning(f"Error parsing custom validation rules: {str(e)}")
+        
+        # Perform validation
+        validation_result = validation_service.validate_data(
+            validation_data,
+            metadata.file_id,
+            custom_rules
+        )
+        
+        # Store validation result
+        storage_service.store_validation_result(validation_result)
+        
+        # Handle email notifications based on validation result
+        email_lookup_field = req_body.get('email_lookup_field', 'email')
+        requester_email = req_body.get('requester_email')
+        
+        # Extract email addresses for notifications
+        recipient_emails = excel_service.extract_email_column(validation_data, email_lookup_field)
+        if requester_email:
+            recipient_emails.append(requester_email)
+        
+        # Remove duplicates
+        recipient_emails = list(set(recipient_emails))
+        
+        # Send appropriate notifications
+        email_notifications = []
+        if validation_result.status.value == "failed":
+            # Send failure notification
+            email_notifications = email_service.send_validation_failure_notification(
+                validation_result, recipient_emails
+            )
+            
+            # Create change tracking record
+            file_hash = generate_file_hash(file_data)
+            storage_service.create_change_tracking_record(
+                metadata.file_id,
+                validation_result.validation_id,
+                file_hash
+            )
+            
+        elif validation_result.status.value == "passed":
+            # Send success notification
+            email_notifications = email_service.send_validation_success_notification(
+                metadata.file_id, recipient_emails
+            )
+        
+        # Store email notification records
+        for notification in email_notifications:
+            storage_service.store_email_notification(notification)
+        
+        # Update validation result with email info
+        validation_result.email_sent = len(email_notifications) > 0
+        if recipient_emails:
+            validation_result.email_recipient = recipient_emails[0]  # Primary recipient
+        
+        # Prepare response
+        response_data = {
+            "file_id": metadata.file_id,
+            "validation_id": validation_result.validation_id,
+            "status": validation_result.status.value,
+            "total_errors": validation_result.total_errors,
+            "total_warnings": validation_result.total_warnings,
+            "processed_rows": validation_result.processed_rows,
+            "email_sent": validation_result.email_sent,
+            "notifications_sent": len(email_notifications),
+            "timestamp": validation_result.timestamp.isoformat()
+        }
+        
+        # Include error details if validation failed
+        if validation_result.status.value == "failed":
+            response_data["errors"] = [
+                {
+                    "row": error.row,
+                    "column": error.column,
+                    "value": str(error.value),
+                    "message": error.message,
+                    "suggested_correction": error.suggested_correction
+                }
+                for error in validation_result.errors[:10]  # Limit to first 10 errors
+            ]
+        
+        end_time = datetime.now()
+        log_function_execution(
+            "process_excel_file",
+            start_time,
+            end_time,
+            True,
+            {
+                "file_id": metadata.file_id,
+                "validation_status": validation_result.status.value,
+                "errors": validation_result.total_errors,
+                "correlation_id": correlation_id
+            }
+        )
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        end_time = datetime.now()
+        log_function_execution("process_excel_file", start_time, end_time, False, {"correlation_id": correlation_id})
+        
+        logger.error(f"Error processing Excel file: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to process Excel file",
+                "details": str(e)
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
+
+@excel_processor_bp.function_name(name="get_processing_status")
+@excel_processor_bp.route(route="status/{file_id}", methods=["GET"])
+async def get_processing_status(req: func.HttpRequest) -> func.HttpResponse:
+    """
+    Get processing status for a file
+    """
+    try:
+        file_id = req.route_params.get('file_id')
+        
+        if not file_id:
+            return func.HttpResponse(
+                json.dumps({"error": "file_id is required"}),
+                status_code=400,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        storage_service = StorageService()
+        
+        # Get file metadata
+        metadata = storage_service.get_file_metadata(file_id)
+        if not metadata:
+            return func.HttpResponse(
+                json.dumps({"error": "File not found"}),
+                status_code=404,
+                headers={"Content-Type": "application/json"}
+            )
+        
+        # Get latest validation result
+        latest = storage_service.get_latest_validation_for_file(file_id)
+
+        response_data = {
+            "file_id": file_id,
+            "filename": metadata.filename,
+            "upload_timestamp": metadata.upload_timestamp.isoformat(),
+            "file_size": metadata.file_size,
+            "total_rows": metadata.total_rows,
+            "total_columns": metadata.total_columns,
+            "sheet_names": metadata.sheet_names
+        }
+        if latest:
+            response_data.update({
+                "latest_validation_id": latest.validation_id,
+                "latest_status": latest.status.value,
+                "latest_total_errors": latest.total_errors,
+                "latest_total_warnings": latest.total_warnings,
+                "latest_timestamp": latest.timestamp.isoformat(),
+            })
+        
+        return func.HttpResponse(
+            json.dumps(response_data),
+            status_code=200,
+            headers={"Content-Type": "application/json"}
+        )
+        
+    except Exception as e:
+        logger.error(f"Error getting processing status: {str(e)}")
+        
+        return func.HttpResponse(
+            json.dumps({
+                "error": "Internal server error",
+                "message": "Failed to get processing status"
+            }),
+            status_code=500,
+            headers={"Content-Type": "application/json"}
+        )
diff --git a/src/functions/teams_relay/__init__.py b/src/functions/teams_relay/__init__.py
new file mode 100644
index 0000000..a712b1f
--- /dev/null
+++ b/src/functions/teams_relay/__init__.py
@@ -0,0 +1,61 @@
+import azure.functions as func
+import json
+import logging
+from datetime import datetime
+
+from src.functions.agent_gateway import _build_orchestrator, _resolve_chat_agent
+from src.utils.helpers import get_correlation_id, log_function_execution
+from src.utils.cards import build_answer_card
+
+logger = logging.getLogger(__name__)
+
+teams_relay_bp = func.Blueprint()
+
+
+@teams_relay_bp.function_name(name="teams_ask")
+@teams_relay_bp.route(route="teams/ask", methods=["POST"])
+async def teams_ask(req: func.HttpRequest) -> func.HttpResponse:
+    """Teams relay endpoint that returns an Adaptive Card.
+
+    Body: { "query": "...", "agent": "carrier|claims|customer|domain" }
+    """
+    started = datetime.now()
+    cid = get_correlation_id(req)
+    try:
+        body = req.get_json()
+    except Exception:
+        return func.HttpResponse(
+            json.dumps({"error": "Invalid JSON"}),
+            status_code=400,
+            headers={"Content-Type": "application/json"},
+        )
+
+    query = (body or {}).get("query")
+    agent_name = (body or {}).get("agent", "domain")
+    if not query:
+        return func.HttpResponse(
+            json.dumps({"error": "'query' is required"}),
+            status_code=400,
+            headers={"Content-Type": "application/json"},
+        )
+
+    orch = _build_orchestrator()
+    agent = _resolve_chat_agent(agent_name, orch)
+    result_payload = orch.handle_with_citations(query)
+
+    card = build_answer_card(result_payload)
+
+    finished = datetime.now()
+    log_function_execution(
+        "teams_ask",
+        started,
+        finished,
+        True,
+        {"agent": agent.__class__.__name__, "correlation_id": cid},
+    )
+    return func.HttpResponse(
+        json.dumps(card),
+        status_code=200,
+        headers={"Content-Type": "application/json"},
+    )
+
diff --git a/src/models/validation_models.py b/src/models/validation_models.py
new file mode 100644
index 0000000..5907556
--- /dev/null
+++ b/src/models/validation_models.py
@@ -0,0 +1,85 @@
+from pydantic import BaseModel, Field
+from typing import List, Dict, Any, Optional
+from datetime import datetime
+from enum import Enum
+
+class ValidationStatus(str, Enum):
+    PENDING = "pending"
+    PASSED = "passed"
+    FAILED = "failed"
+    CORRECTED = "corrected"
+
+class ValidationRule(BaseModel):
+    """Model for validation rules"""
+    rule_id: str
+    rule_name: str
+    description: str
+    rule_type: str  # "data_type", "range", "format", "custom"
+    parameters: Dict[str, Any]
+    severity: str = "error"  # "error", "warning", "info"
+
+class ValidationError(BaseModel):
+    """Model for validation errors"""
+    row: int
+    column: str
+    value: Any
+    rule_id: str
+    message: str
+    severity: str
+    suggested_correction: Optional[str] = None
+
+class ExcelFileMetadata(BaseModel):
+    """Model for Excel file metadata"""
+    file_id: str
+    filename: str
+    upload_timestamp: datetime
+    file_size: int
+    sheet_names: List[str]
+    total_rows: int
+    total_columns: int
+    uploaded_by: Optional[str] = None
+
+class ValidationResult(BaseModel):
+    """Model for validation results"""
+    file_id: str
+    validation_id: str
+    status: ValidationStatus
+    timestamp: datetime
+    errors: List[ValidationError]
+    warnings: List[ValidationError]
+    total_errors: int
+    total_warnings: int
+    processed_rows: int
+    email_sent: bool = False
+    email_recipient: Optional[str] = None
+
+class EmailNotification(BaseModel):
+    """Model for email notifications"""
+    notification_id: str
+    file_id: str
+    validation_id: str
+    recipient_email: str
+    subject: str
+    sent_timestamp: datetime
+    delivery_status: str = "pending"
+    correction_deadline: Optional[datetime] = None
+
+class ChangeTrackingRecord(BaseModel):
+    """Model for tracking changes"""
+    tracking_id: str
+    file_id: str
+    validation_id: str
+    original_file_hash: str
+    updated_file_hash: Optional[str] = None
+    change_timestamp: Optional[datetime] = None
+    change_description: Optional[str] = None
+    verified: bool = False
+
+class ProcessingRequest(BaseModel):
+    """Model for processing requests"""
+    request_id: str
+    file_data: bytes
+    filename: str
+    validation_rules: List[ValidationRule]
+    email_lookup_field: str = "email"  # Column name for email lookup
+    requester_email: Optional[str] = None
\ No newline at end of file
diff --git a/src/services/email_service.py b/src/services/email_service.py
new file mode 100644
index 0000000..eb864cb
--- /dev/null
+++ b/src/services/email_service.py
@@ -0,0 +1,327 @@
+import logging
+import os
+from typing import List, Optional
+from datetime import datetime, timedelta, timezone
+try:
+    from azure.communication.email import EmailClient
+except Exception:  # pragma: no cover - allow running without Azure SDKs installed
+    EmailClient = None  # type: ignore
+from src.models.validation_models import ValidationResult, EmailNotification
+
+logger = logging.getLogger(__name__)
+
+class EmailService:
+    """Service for sending email notifications using Azure Communication Services"""
+    
+    def __init__(self):
+        self.email_client = self._initialize_email_client()
+        self.sender_email = os.getenv("DEFAULT_SENDER_EMAIL", "noreply@yourdomain.com")
+    
+    def _initialize_email_client(self) -> Optional[EmailClient]:
+        """Initialize Azure Communication Services Email client"""
+        try:
+            connection_string = os.getenv("AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING")
+            if not connection_string:
+                logger.warning("Azure Communication Services connection string not configured")
+                return None
+            
+            return EmailClient.from_connection_string(connection_string)
+        except Exception as e:
+            logger.error(f"Failed to initialize email client: {str(e)}")
+            return None
+    
+    def send_validation_failure_notification(self, validation_result: ValidationResult, 
+                                           recipient_emails: List[str]) -> List[EmailNotification]:
+        """
+        Send email notification for validation failures
+        
+        Args:
+            validation_result: ValidationResult object with errors
+            recipient_emails: List of email addresses to notify
+            
+        Returns:
+            List of EmailNotification objects
+        """
+        notifications = []
+        
+        if not self.email_client:
+            logger.error("Email client not initialized")
+            return notifications
+        
+        # Generate email content
+        subject = f"Data Validation Failed - {validation_result.file_id}"
+        html_content = self._generate_validation_email_html(validation_result)
+        text_content = self._generate_validation_email_text(validation_result)
+        
+        # Send to each recipient
+        for recipient in recipient_emails:
+            try:
+                notification = self._send_email(
+                    recipient,
+                    subject,
+                    html_content,
+                    text_content,
+                    validation_result
+                )
+                notifications.append(notification)
+                
+            except Exception as e:
+                logger.error(f"Failed to send email to {recipient}: {str(e)}")
+                # Create failed notification record
+                notification = EmailNotification(
+                    notification_id=f"email_{int(datetime.now(timezone.utc).timestamp())}",
+                    file_id=validation_result.file_id,
+                    validation_id=validation_result.validation_id,
+                    recipient_email=recipient,
+                    subject=subject,
+                    sent_timestamp=datetime.now(timezone.utc),
+                    delivery_status="failed"
+                )
+                notifications.append(notification)
+        
+        return notifications
+    
+    def _send_email(self, recipient: str, subject: str, html_content: str, 
+                   text_content: str, validation_result: ValidationResult) -> EmailNotification:
+        """Send individual email"""
+        
+        # Create email message
+        message = {
+            "senderAddress": self.sender_email,
+            "recipients": {
+                "to": [{"address": recipient}]
+            },
+            "content": {
+                "subject": subject,
+                "html": html_content,
+                "plainText": text_content
+            }
+        }
+        
+        # Send email
+        poller = self.email_client.begin_send(message)
+        result = poller.result()
+        
+        # Create notification record
+        notification = EmailNotification(
+            notification_id=f"email_{int(datetime.now(timezone.utc).timestamp())}_{recipient.replace('@', '_')}",
+            file_id=validation_result.file_id,
+            validation_id=validation_result.validation_id,
+            recipient_email=recipient,
+            subject=subject,
+            sent_timestamp=datetime.now(timezone.utc),
+            delivery_status="sent",
+            correction_deadline=datetime.now(timezone.utc) + timedelta(days=3)  # 3 days to correct
+        )
+        
+        logger.info(f"Email sent successfully to {recipient}: {result.message_id}")
+        return notification
+    
+    def _generate_validation_email_html(self, validation_result: ValidationResult) -> str:
+        """Generate HTML email content for validation failures"""
+        
+        errors_html = ""
+        for error in validation_result.errors[:10]:  # Limit to first 10 errors
+            suggestion = f"<br><strong>Suggestion:</strong> {error.suggested_correction}" if error.suggested_correction else ""
+            errors_html += f"""
+            <tr>
+                <td>{error.row}</td>
+                <td>{error.column}</td>
+                <td>{error.value}</td>
+                <td>{error.message}{suggestion}</td>
+            </tr>
+            """
+        
+        html_content = f"""
+        <!DOCTYPE html>
+        <html>
+        <head>
+            <style>
+                body {{ font-family: Arial, sans-serif; }}
+                .container {{ max-width: 800px; margin: 0 auto; padding: 20px; }}
+                .header {{ color: #d73027; }}
+                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
+                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
+                th {{ background-color: #f2f2f2; }}
+                .footer {{ margin-top: 30px; font-size: 12px; color: #666; }}
+            </style>
+        </head>
+        <body>
+            <div class="container">
+                <h1 class="header">Data Validation Failed</h1>
+                
+                <p>Your submitted Excel file has failed data validation. Please review and correct the following issues:</p>
+                
+                <h3>Validation Summary</h3>
+                <ul>
+                    <li><strong>File ID:</strong> {validation_result.file_id}</li>
+                    <li><strong>Total Errors:</strong> {validation_result.total_errors}</li>
+                    <li><strong>Total Warnings:</strong> {validation_result.total_warnings}</li>
+                    <li><strong>Rows Processed:</strong> {validation_result.processed_rows}</li>
+                    <li><strong>Validation Date (UTC):</strong> {validation_result.timestamp.strftime('%Y-%m-%d %H:%M:%S %Z')}</li>
+                </ul>
+                
+                <h3>Errors Found</h3>
+                <table>
+                    <tr>
+                        <th>Row</th>
+                        <th>Column</th>
+                        <th>Current Value</th>
+                        <th>Issue & Suggested Correction</th>
+                    </tr>
+                    {errors_html}
+                </table>
+                
+                {"<p><em>Note: Only the first 10 errors are shown. Please correct all issues and resubmit.</em></p>" if len(validation_result.errors) > 10 else ""}
+                
+                <p><strong>Next Steps:</strong></p>
+                <ol>
+                    <li>Download and correct your Excel file</li>
+                    <li>Address all validation errors listed above</li>
+                    <li>Resubmit the corrected file</li>
+                </ol>
+                
+                <p>Please correct these issues and resubmit your file within 3 business days.</p>
+                
+                <div class="footer">
+                    <p>This is an automated message from the Azure Excel Data Validation Agent.</p>
+                </div>
+            </div>
+        </body>
+        </html>
+        """
+        
+        return html_content
+    
+    def _generate_validation_email_text(self, validation_result: ValidationResult) -> str:
+        """Generate plain text email content for validation failures"""
+        
+        errors_text = ""
+        for error in validation_result.errors[:10]:
+            suggestion = f" | Suggestion: {error.suggested_correction}" if error.suggested_correction else ""
+            errors_text += f"Row {error.row}, Column {error.column}: {error.message} (Value: {error.value}){suggestion}\n"
+        
+        text_content = f"""
+Data Validation Failed
+
+Your submitted Excel file has failed data validation. Please review and correct the following issues:
+
+Validation Summary (UTC):
+- File ID: {validation_result.file_id}
+- Total Errors: {validation_result.total_errors}
+- Total Warnings: {validation_result.total_warnings}
+- Rows Processed: {validation_result.processed_rows}
+- Validation Date (UTC): {validation_result.timestamp.strftime('%Y-%m-%d %H:%M:%S %Z')}
+
+Errors Found:
+{errors_text}
+
+{"Note: Only the first 10 errors are shown. Please correct all issues and resubmit." if len(validation_result.errors) > 10 else ""}
+
+Next Steps:
+1. Download and correct your Excel file
+2. Address all validation errors listed above
+3. Resubmit the corrected file
+
+Please correct these issues and resubmit your file within 3 business days.
+
+This is an automated message from the Azure Excel Data Validation Agent.
+        """
+        
+        return text_content
+    
+    def send_validation_success_notification(self, file_id: str, recipient_emails: List[str]) -> List[EmailNotification]:
+        """
+        Send email notification for successful validation
+        
+        Args:
+            file_id: File identifier
+            recipient_emails: List of email addresses to notify
+            
+        Returns:
+            List of EmailNotification objects
+        """
+        notifications = []
+        
+        if not self.email_client:
+            logger.error("Email client not initialized")
+            return notifications
+        
+        subject = f"Data Validation Successful - {file_id}"
+        html_content = self._generate_success_email_html(file_id)
+        text_content = self._generate_success_email_text(file_id)
+        
+        for recipient in recipient_emails:
+            try:
+                message = {
+                    "senderAddress": self.sender_email,
+                    "recipients": {
+                        "to": [{"address": recipient}]
+                    },
+                    "content": {
+                        "subject": subject,
+                        "html": html_content,
+                        "plainText": text_content
+                    }
+                }
+                
+                poller = self.email_client.begin_send(message)
+                result = poller.result()
+                
+                notification = EmailNotification(
+                    notification_id=f"success_{int(datetime.now(timezone.utc).timestamp())}_{recipient.replace('@', '_')}",
+                    file_id=file_id,
+                    validation_id="success",
+                    recipient_email=recipient,
+                    subject=subject,
+                    sent_timestamp=datetime.now(timezone.utc),
+                    delivery_status="sent"
+                )
+                notifications.append(notification)
+                
+                logger.info(f"Success notification sent to {recipient}: {result.message_id}")
+                
+            except Exception as e:
+                logger.error(f"Failed to send success notification to {recipient}: {str(e)}")
+        
+        return notifications
+    
+    def _generate_success_email_html(self, file_id: str) -> str:
+        """Generate HTML content for success notification"""
+        return f"""
+        <!DOCTYPE html>
+        <html>
+        <head>
+            <style>
+                body {{ font-family: Arial, sans-serif; }}
+                .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
+                .header {{ color: #2e7d32; }}
+                .success {{ background-color: #e8f5e8; padding: 15px; border-radius: 5px; }}
+            </style>
+        </head>
+        <body>
+            <div class="container">
+                <h1 class="header">Data Validation Successful</h1>
+                <div class="success">
+                    <p>‚úÖ Your Excel file has passed all validation checks!</p>
+                    <p><strong>File ID:</strong> {file_id}</p>
+                    <p><strong>Validation Date (UTC):</strong> {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>
+                </div>
+                <p>Your data has been successfully processed and is ready for use.</p>
+            </div>
+        </body>
+        </html>
+        """
+    
+    def _generate_success_email_text(self, file_id: str) -> str:
+        """Generate text content for success notification"""
+        return f"""
+Data Validation Successful
+
+‚úÖ Your Excel file has passed all validation checks!
+
+File ID: {file_id}
+Validation Date (UTC): {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}
+
+Your data has been successfully processed and is ready for use.
+        """
diff --git a/src/services/excel_service.py b/src/services/excel_service.py
new file mode 100644
index 0000000..f723fe8
--- /dev/null
+++ b/src/services/excel_service.py
@@ -0,0 +1,173 @@
+import os
+import pandas as pd
+import hashlib
+import io
+import logging
+from typing import Dict, List, Any, Tuple
+from datetime import datetime, timezone
+from src.models.validation_models import ExcelFileMetadata
+from src.utils.helpers import validate_email_format
+
+logger = logging.getLogger(__name__)
+
+class ExcelService:
+    """Service for handling Excel file operations"""
+    
+    def __init__(self):
+        # Load supported types from env, default to xlsx only (openpyxl)
+        env_types = os.getenv('SUPPORTED_FILE_TYPES', 'xlsx')
+        self.supported_formats = [f".{ext.strip().lower()}" for ext in env_types.split(',') if ext.strip()]
+    
+    def parse_excel_file(self, file_data: bytes, filename: str) -> Tuple[Dict[str, pd.DataFrame], ExcelFileMetadata]:
+        """
+        Parse Excel file and return dataframes with metadata
+        
+        Args:
+            file_data: Raw file bytes
+            filename: Original filename
+            
+        Returns:
+            Tuple of (sheet_data_dict, metadata)
+        """
+        try:
+            # Create file-like object from bytes
+            file_buffer = io.BytesIO(file_data)
+            
+            # Read all sheets
+            sheets_dict = pd.read_excel(file_buffer, sheet_name=None, engine='openpyxl')
+            
+            # Calculate metadata
+            total_rows = sum(len(df) for df in sheets_dict.values())
+            total_columns = sum(len(df.columns) for df in sheets_dict.values())
+            
+            # Generate file ID from hash
+            file_hash = hashlib.md5(file_data).hexdigest()
+            file_id = f"excel_{file_hash}_{int(datetime.now(timezone.utc).timestamp())}"
+            
+            metadata = ExcelFileMetadata(
+                file_id=file_id,
+                filename=filename,
+                upload_timestamp=datetime.now(timezone.utc),
+                file_size=len(file_data),
+                sheet_names=list(sheets_dict.keys()),
+                total_rows=total_rows,
+                total_columns=total_columns
+            )
+            
+            logger.info(f"Successfully parsed Excel file: {filename} with {len(sheets_dict)} sheets")
+            return sheets_dict, metadata
+            
+        except Exception as e:
+            logger.error(f"Error parsing Excel file {filename}: {str(e)}")
+            raise ValueError(f"Failed to parse Excel file: {str(e)}")
+    
+    def extract_data_for_validation(self, sheets_dict: Dict[str, pd.DataFrame], 
+                                  target_sheet: str = None) -> pd.DataFrame:
+        """
+        Extract data from specific sheet or first sheet for validation
+        
+        Args:
+            sheets_dict: Dictionary of sheet names to DataFrames
+            target_sheet: Specific sheet name to extract (optional)
+            
+        Returns:
+            DataFrame for validation
+        """
+        try:
+            if target_sheet and target_sheet in sheets_dict:
+                df = sheets_dict[target_sheet]
+            else:
+                # Use first sheet if no target specified
+                df = list(sheets_dict.values())[0]
+            
+            # Clean the dataframe
+            df = self._clean_dataframe(df)
+            
+            logger.info(f"Extracted data: {len(df)} rows, {len(df.columns)} columns")
+            return df
+            
+        except Exception as e:
+            logger.error(f"Error extracting data for validation: {str(e)}")
+            raise ValueError(f"Failed to extract validation data: {str(e)}")
+    
+    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
+        """
+        Clean dataframe by removing empty rows and standardizing column names
+        
+        Args:
+            df: Input DataFrame
+            
+        Returns:
+            Cleaned DataFrame
+        """
+        # Strip whitespace from string columns
+        string_columns = df.select_dtypes(include=['object']).columns
+        df[string_columns] = df[string_columns].astype(str).apply(lambda x: x.str.strip())
+        
+        # Replace common null-like strings with actual NaN
+        df = df.replace({'nan': pd.NA, 'None': pd.NA, '': pd.NA})
+        
+        # Clean column names
+        df.columns = [str(col).strip() for col in df.columns]
+        
+        # Remove completely empty rows after cleaning
+        df = df.dropna(how='all')
+
+        return df
+    
+    def get_file_hash(self, file_data: bytes) -> str:
+        """
+        Generate MD5 hash of file content
+        
+        Args:
+            file_data: Raw file bytes
+            
+        Returns:
+            MD5 hash string
+        """
+        return hashlib.md5(file_data).hexdigest()
+    
+    def validate_file_format(self, filename: str) -> bool:
+        """
+        Validate if file format is supported
+        
+        Args:
+            filename: Name of the file
+            
+        Returns:
+            True if supported format
+        """
+        return any(filename.lower().endswith(fmt) for fmt in self.supported_formats)
+    
+    def extract_email_column(self, df: pd.DataFrame, email_field: str = "email") -> List[str]:
+        """
+        Extract email addresses from specified column
+        
+        Args:
+            df: DataFrame to extract from
+            email_field: Column name containing emails
+            
+        Returns:
+            List of unique email addresses
+        """
+        try:
+            if email_field not in df.columns:
+                logger.warning(f"Email field '{email_field}' not found in data")
+                return []
+            
+            # Extract unique, non-null email addresses
+            emails = df[email_field].dropna().unique().tolist()
+            
+            # Validate with shared helper
+            valid_emails = []
+            for email in emails:
+                email_str = str(email).strip().lower()
+                if validate_email_format(email_str):
+                    valid_emails.append(email_str)
+            
+            logger.info(f"Extracted {len(valid_emails)} valid email addresses")
+            return valid_emails
+            
+        except Exception as e:
+            logger.error(f"Error extracting email column: {str(e)}")
+            return []
diff --git a/src/services/fabric_data_agent.py b/src/services/fabric_data_agent.py
new file mode 100644
index 0000000..6a930ee
--- /dev/null
+++ b/src/services/fabric_data_agent.py
@@ -0,0 +1,58 @@
+"""HTTP client for executing SQL against a Microsoft Fabric endpoint."""
+from __future__ import annotations
+
+import os
+from typing import List
+
+import requests
+
+
+class FabricDataAgent:
+    """Execute SQL queries through the Fabric Data endpoint.
+
+    Parameters
+    ----------
+    endpoint: str
+        Base URL of the Fabric SQL endpoint.
+    token: str | None
+        Bearer token for authentication. If ``None`` the ``FABRIC_TOKEN``
+        environment variable is used.
+    """
+
+    def __init__(self, endpoint: str, token: str | None = None) -> None:
+        self._endpoint = endpoint.rstrip("/")
+        self._token = token or os.getenv("FABRIC_TOKEN", "")
+
+    def run_sql(self, sql: str) -> List[dict]:
+        """Run raw SQL against the Fabric endpoint and return rows."""
+        url = f"{self._endpoint}/sql"
+        headers = {
+            "Authorization": f"Bearer {self._token}",
+            "Content-Type": "application/json",
+        }
+        response = requests.post(
+            url,
+            json={"query": sql},
+            headers=headers,
+            timeout=10,
+        )
+        response.raise_for_status()
+        return response.json().get("rows", [])
+
+    def run_sql_params(self, sql: str, parameters: dict) -> List[dict]:
+        """Execute a parameterized SQL query.
+
+        Parameters should be provided as a dict; they are sent to the Fabric
+        service using a standard `parameters` payload to avoid string
+        interpolation. Example: `{"@carrier": "X"}` used with
+        `WHERE carrier = @carrier`.
+        """
+        url = f"{self._endpoint}/sql"
+        headers = {
+            "Authorization": f"Bearer {self._token}",
+            "Content-Type": "application/json",
+        }
+        payload = {"query": sql, "parameters": [{"name": k, "value": v} for k, v in parameters.items()]}
+        response = requests.post(url, json=payload, headers=headers, timeout=10)
+        response.raise_for_status()
+        return response.json().get("rows", [])
diff --git a/src/services/graph_service.py b/src/services/graph_service.py
new file mode 100644
index 0000000..ba37917
--- /dev/null
+++ b/src/services/graph_service.py
@@ -0,0 +1,57 @@
+"""Microsoft Graph client for accessing M365 resources."""
+from __future__ import annotations
+
+import os
+from typing import List
+
+import requests
+
+
+class GraphService:
+    """Minimal Microsoft Graph search client."""
+
+    def __init__(
+        self,
+        token: str | None = None,
+        endpoint: str = "https://graph.microsoft.com/v1.0",
+    ) -> None:
+        self._token = token or os.getenv("GRAPH_TOKEN", "")
+        self._endpoint = endpoint.rstrip("/")
+
+    def get_resource(self, query: str) -> List[str]:
+        """Search messages, events, and files matching *query*."""
+        url = f"{self._endpoint}/search/query"
+        payload = {
+            "requests": [
+                {
+                    "entityTypes": ["message", "event", "driveItem"],
+                    "query": {"queryString": query},
+                    "from": 0,
+                    "size": 5,
+                }
+            ]
+        }
+        headers = {
+            "Authorization": f"Bearer {self._token}",
+            "Content-Type": "application/json",
+        }
+        response = requests.post(
+            url,
+            json=payload,
+            headers=headers,
+            timeout=10,
+        )
+        response.raise_for_status()
+        results: List[str] = []
+        for req in response.json().get("value", []):
+            for container in req.get("hitsContainers", []):
+                for hit in container.get("hits", []):
+                    source = hit.get("_source", {})
+                    name = (
+                        source.get("subject")
+                        or source.get("name")
+                        or source.get("displayName")
+                    )
+                    if name:
+                        results.append(name)
+        return results
diff --git a/src/services/http_client.py b/src/services/http_client.py
new file mode 100644
index 0000000..348f0b5
--- /dev/null
+++ b/src/services/http_client.py
@@ -0,0 +1,25 @@
+from __future__ import annotations
+
+import asyncio
+from collections.abc import Mapping
+from typing import Any
+
+import httpx
+
+_client = httpx.AsyncClient(
+    timeout=httpx.Timeout(10.0, read=20.0),
+    limits=httpx.Limits(max_keepalive_connections=20, max_connections=100),
+)
+
+
+async def post_json(url: str, headers: Mapping[str, str], payload: Mapping[str, Any]) -> dict:
+    for attempt in range(3):
+        try:
+            resp = await _client.post(url, headers=headers, json=payload)
+            resp.raise_for_status()
+            return resp.json()
+        except httpx.HTTPError:
+            if attempt == 2:
+                raise
+            await asyncio.sleep(0.2 * (2**attempt))
+    return {}
diff --git a/src/services/search_service.py b/src/services/search_service.py
new file mode 100644
index 0000000..bf55b15
--- /dev/null
+++ b/src/services/search_service.py
@@ -0,0 +1,46 @@
+"""Client for Azure Cognitive Search used by the unstructured data agent."""
+from __future__ import annotations
+
+import os
+from typing import List
+
+import requests
+
+
+class SearchService:
+    """Query an Azure Cognitive Search index."""
+
+    def __init__(
+        self, endpoint: str, index: str, api_key: str | None = None
+    ) -> None:
+        self._endpoint = endpoint.rstrip("/")
+        self._index = index
+        self._api_key = api_key or os.getenv("SEARCH_API_KEY", "")
+
+    def search(self, query: str, top: int = 5, semantic: bool = False) -> List[str]:
+        url = (
+            f"{self._endpoint}/indexes/{self._index}/docs/search"
+            "?api-version=2021-04-30-Preview"
+        )
+        headers = {
+            "api-key": self._api_key,
+            "Content-Type": "application/json",
+        }
+        body = {"search": query, "top": top}
+        if semantic:
+            # Basic semantic settings; requires a semantic configuration on the index
+            body.update({
+                "queryType": "semantic",
+                "queryLanguage": "en-us",
+                "semanticConfiguration": "default",
+            })
+
+        response = requests.post(
+            url,
+            headers=headers,
+            json=body,
+            timeout=10,
+        )
+        response.raise_for_status()
+        docs = response.json().get("value", [])
+        return [d.get("content") or d.get("text", "") for d in docs]
diff --git a/src/services/sql_templates.py b/src/services/sql_templates.py
new file mode 100644
index 0000000..994c48e
--- /dev/null
+++ b/src/services/sql_templates.py
@@ -0,0 +1,18 @@
+"""
+Approved, parameterized SQL templates for Fabric queries.
+Add new keys and keep them read-only.
+"""
+
+VARIANCE_SUMMARY = """
+SELECT
+  carrier_id,
+  SUM(billed_total - rated_total) AS variance
+FROM curated_fact_invoice
+WHERE invoice_date BETWEEN @from AND @to
+GROUP BY carrier_id
+ORDER BY variance DESC
+"""
+
+TEMPLATES = {
+    "variance_summary": VARIANCE_SUMMARY,
+}
diff --git a/src/services/storage_service.py b/src/services/storage_service.py
new file mode 100644
index 0000000..4ea93e5
--- /dev/null
+++ b/src/services/storage_service.py
@@ -0,0 +1,637 @@
+import logging
+import os
+import json
+from typing import Optional, List, Dict, Any
+from datetime import datetime, timezone
+try:
+    from azure.storage.blob import BlobServiceClient
+    from azure.cosmos import CosmosClient, PartitionKey
+except Exception:  # pragma: no cover - allow running without Azure SDKs installed
+    BlobServiceClient = None  # type: ignore
+    CosmosClient = None  # type: ignore
+    PartitionKey = None  # type: ignore
+from src.models.validation_models import (
+    ExcelFileMetadata, ValidationResult, EmailNotification, 
+    ChangeTrackingRecord, ValidationStatus
+)
+
+logger = logging.getLogger(__name__)
+
+class StorageService:
+    """Service for managing storage and tracking using Azure Storage and Cosmos DB"""
+    
+    def __init__(self):
+        self.blob_client = self._initialize_blob_client()
+        self.cosmos_client = self._initialize_cosmos_client()
+        self.container_name = "excel-files"
+        self.database_name = "validation-tracking"
+        self.containers = {
+            "metadata": "file-metadata",
+            "validations": "validation-results", 
+            "emails": "email-notifications",
+            "tracking": "change-tracking"
+        }
+        self._ensure_storage_exists()
+    
+    def _initialize_blob_client(self) -> Optional[BlobServiceClient]:
+        """Initialize Azure Blob Storage client"""
+        try:
+            connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
+            if not connection_string:
+                logger.warning("Azure Storage connection string not configured")
+                return None
+            
+            return BlobServiceClient.from_connection_string(connection_string)
+        except Exception as e:
+            logger.error(f"Failed to initialize blob client: {str(e)}")
+            return None
+    
+    def _initialize_cosmos_client(self) -> Optional[CosmosClient]:
+        """Initialize Azure Cosmos DB client"""
+        try:
+            connection_string = os.getenv("AZURE_COSMOSDB_CONNECTION_STRING")
+            if not connection_string:
+                logger.warning("Azure Cosmos DB connection string not configured")
+                return None
+            
+            # Extract endpoint and key from connection string
+            parts = connection_string.split(';')
+            endpoint = next(part.split('=', 1)[1] for part in parts if part.startswith('AccountEndpoint='))
+            key = next(part.split('=', 1)[1] for part in parts if part.startswith('AccountKey='))
+            
+            return CosmosClient(endpoint, key)
+        except Exception as e:
+            logger.error(f"Failed to initialize Cosmos client: {str(e)}")
+            return None
+    
+    def _ensure_storage_exists(self):
+        """Ensure required storage containers and databases exist"""
+        try:
+            # Create blob container if it doesn't exist
+            if self.blob_client:
+                try:
+                    self.blob_client.create_container(self.container_name)
+                except Exception:
+                    pass  # Container might already exist
+            
+            # Create Cosmos database and containers
+            if self.cosmos_client:
+                try:
+                    database = self.cosmos_client.create_database_if_not_exists(self.database_name)
+                    
+                    # Create containers with appropriate partition keys
+                    database.create_container_if_not_exists(
+                        id=self.containers["metadata"],
+                        partition_key=PartitionKey(path="/file_id")
+                    )
+                    
+                    database.create_container_if_not_exists(
+                        id=self.containers["validations"],
+                        partition_key=PartitionKey(path="/file_id")
+                    )
+                    
+                    database.create_container_if_not_exists(
+                        id=self.containers["emails"],
+                        partition_key=PartitionKey(path="/file_id")
+                    )
+                    
+                    database.create_container_if_not_exists(
+                        id=self.containers["tracking"],
+                        partition_key=PartitionKey(path="/file_id")
+                    )
+                    
+                except Exception as e:
+                    logger.error(f"Error creating Cosmos containers: {str(e)}")
+        
+        except Exception as e:
+            logger.error(f"Error ensuring storage exists: {str(e)}")
+    
+    def store_file(self, file_data: bytes, file_id: str, filename: str) -> bool:
+        """
+        Store Excel file in blob storage
+        
+        Args:
+            file_data: Raw file bytes
+            file_id: Unique file identifier
+            filename: Original filename
+            
+        Returns:
+            True if successful
+        """
+        if not self.blob_client:
+            logger.error("Blob client not initialized")
+            return False
+        
+        try:
+            blob_name = f"{file_id}/{filename}"
+            blob_client = self.blob_client.get_blob_client(
+                container=self.container_name,
+                blob=blob_name
+            )
+            
+            blob_client.upload_blob(file_data, overwrite=True)
+            logger.info(f"File stored successfully: {blob_name}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error storing file {file_id}: {str(e)}")
+            return False
+    
+    def store_file_metadata(self, metadata: ExcelFileMetadata) -> bool:
+        """
+        Store file metadata in Cosmos DB
+        
+        Args:
+            metadata: ExcelFileMetadata object
+            
+        Returns:
+            True if successful
+        """
+        if not self.cosmos_client:
+            logger.error("Cosmos client not initialized")
+            return False
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["metadata"])
+            
+            # Convert to dict and add required fields
+            item = metadata.model_dump()
+            item['id'] = metadata.file_id
+            item['timestamp'] = (
+                metadata.upload_timestamp if isinstance(metadata.upload_timestamp, datetime) else datetime.now(timezone.utc)
+            ).isoformat()
+            
+            container.create_item(item)
+            logger.info(f"Metadata stored for file: {metadata.file_id}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error storing metadata for {metadata.file_id}: {str(e)}")
+            return False
+    
+    def store_validation_result(self, result: ValidationResult) -> bool:
+        """
+        Store validation result in Cosmos DB
+        
+        Args:
+            result: ValidationResult object
+            
+        Returns:
+            True if successful
+        """
+        if not self.cosmos_client:
+            logger.error("Cosmos client not initialized")
+            return False
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["validations"])
+            
+            # Convert to dict and prepare for storage
+            item = result.model_dump()
+            item['id'] = result.validation_id
+            item['timestamp'] = (
+                result.timestamp if isinstance(result.timestamp, datetime) else datetime.now(timezone.utc)
+            ).isoformat()
+            
+            # Convert errors and warnings to serializable format
+            item['errors'] = [error.dict() for error in result.errors]
+            item['warnings'] = [warning.dict() for warning in result.warnings]
+            
+            container.create_item(item)
+            logger.info(f"Validation result stored: {result.validation_id}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error storing validation result {result.validation_id}: {str(e)}")
+            return False
+    
+    def store_email_notification(self, notification: EmailNotification) -> bool:
+        """
+        Store email notification record in Cosmos DB
+        
+        Args:
+            notification: EmailNotification object
+            
+        Returns:
+            True if successful
+        """
+        if not self.cosmos_client:
+            logger.error("Cosmos client not initialized")
+            return False
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["emails"])
+            
+            # Convert to dict and prepare for storage
+            item = notification.model_dump()
+            item['id'] = notification.notification_id
+            item['sent_timestamp'] = (
+                notification.sent_timestamp if isinstance(notification.sent_timestamp, datetime) else datetime.now(timezone.utc)
+            ).isoformat()
+            
+            if notification.correction_deadline:
+                item['correction_deadline'] = notification.correction_deadline.isoformat()
+            
+            container.create_item(item)
+            logger.info(f"Email notification stored: {notification.notification_id}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error storing email notification {notification.notification_id}: {str(e)}")
+            return False
+
+    def get_email_notification(self, notification_id: str) -> Optional[EmailNotification]:
+        """Retrieve an email notification record by id from Cosmos DB."""
+        if not self.cosmos_client:
+            return None
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["emails"])
+            query = "SELECT * FROM c WHERE c.id = @id"
+            items = list(
+                container.query_items(
+                    query=query,
+                    parameters=[{"name": "@id", "value": notification_id}],
+                    enable_cross_partition_query=True,
+                )
+            )
+            if not items:
+                return None
+            item = items[0]
+            # Normalize timestamps
+            if isinstance(item.get("sent_timestamp"), str):
+                try:
+                    item["sent_timestamp"] = datetime.fromisoformat(item["sent_timestamp"])  # type: ignore
+                except Exception:
+                    pass
+            if isinstance(item.get("correction_deadline"), str):
+                try:
+                    item["correction_deadline"] = datetime.fromisoformat(item["correction_deadline"])  # type: ignore
+                except Exception:
+                    pass
+            return EmailNotification(**item)
+        except Exception as e:
+            logger.error(f"Error retrieving email notification {notification_id}: {str(e)}")
+            return None
+    
+    def create_change_tracking_record(self, file_id: str, validation_id: str, 
+                                    original_file_hash: str) -> Optional[ChangeTrackingRecord]:
+        """
+        Create a change tracking record
+        
+        Args:
+            file_id: File identifier
+            validation_id: Validation identifier
+            original_file_hash: Hash of original file
+            
+        Returns:
+            ChangeTrackingRecord if successful
+        """
+        try:
+            tracking_record = ChangeTrackingRecord(
+                tracking_id=f"track_{file_id}_{int(datetime.now().timestamp())}",
+                file_id=file_id,
+                validation_id=validation_id,
+                original_file_hash=original_file_hash
+            )
+            
+            if self._store_tracking_record(tracking_record):
+                return tracking_record
+            
+        except Exception as e:
+            logger.error(f"Error creating tracking record for {file_id}: {str(e)}")
+        
+        return None
+    
+    def _store_tracking_record(self, record: ChangeTrackingRecord) -> bool:
+        """Store change tracking record in Cosmos DB"""
+        if not self.cosmos_client:
+            return False
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["tracking"])
+            
+            item = record.dict()
+            item['id'] = record.tracking_id
+            
+            if record.change_timestamp:
+                item['change_timestamp'] = record.change_timestamp.isoformat()
+            
+            container.create_item(item)
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error storing tracking record {record.tracking_id}: {str(e)}")
+            return False
+    
+    def update_change_tracking(self, tracking_id: str, updated_file_hash: str, 
+                             change_description: str = None, file_id: Optional[str] = None) -> bool:
+        """
+        Update change tracking record with new file information
+        
+        Args:
+            tracking_id: Tracking record identifier
+            updated_file_hash: Hash of updated file
+            change_description: Description of changes made
+            
+        Returns:
+            True if successful
+        """
+        if not self.cosmos_client:
+            return False
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["tracking"])
+            
+            # Get existing record (prefer direct read with known partition key)
+            if file_id:
+                existing_item = container.read_item(tracking_id, partition_key=file_id)
+            else:
+                query = "SELECT * FROM c WHERE c.id = @id"
+                items = list(container.query_items(
+                    query=query,
+                    parameters=[{"name": "@id", "value": tracking_id}],
+                    enable_cross_partition_query=True
+                ))
+                if not items:
+                    logger.error(f"Tracking record not found: {tracking_id}")
+                    return False
+                existing_item = items[0]
+            
+            # Update with new information
+            existing_item['updated_file_hash'] = updated_file_hash
+            existing_item['change_timestamp'] = datetime.now().isoformat()
+            existing_item['verified'] = True
+            
+            if change_description:
+                existing_item['change_description'] = change_description
+            
+            container.replace_item(existing_item['id'], existing_item)
+            logger.info(f"Change tracking updated: {tracking_id}")
+            return True
+            
+        except Exception as e:
+            logger.error(f"Error updating change tracking {tracking_id}: {str(e)}")
+            return False
+    
+    def get_validation_result(self, validation_id: str) -> Optional[ValidationResult]:
+        """
+        Retrieve validation result by ID
+        
+        Args:
+            validation_id: Validation identifier
+            
+        Returns:
+            ValidationResult if found
+        """
+        if not self.cosmos_client:
+            return None
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["validations"])
+            
+            # Query by id across partitions to avoid brittle partition key extraction
+            query = "SELECT * FROM c WHERE c.id = @id"
+            items = list(container.query_items(
+                query=query,
+                parameters=[{"name": "@id", "value": validation_id}],
+                enable_cross_partition_query=True
+            ))
+            if not items:
+                return None
+            item = items[0]
+            
+            # Deserialize to ValidationResult
+            return self._deserialize_validation_result(item)
+            
+        except Exception as e:
+            logger.error(f"Error retrieving validation result {validation_id}: {str(e)}")
+            return None
+    
+    def get_file_metadata(self, file_id: str) -> Optional[ExcelFileMetadata]:
+        """
+        Retrieve file metadata by ID
+        
+        Args:
+            file_id: File identifier
+            
+        Returns:
+            ExcelFileMetadata if found
+        """
+        if not self.cosmos_client:
+            return None
+        
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["metadata"])
+            
+            item = container.read_item(file_id, partition_key=file_id)
+            
+            # Convert timestamp back to datetime
+            item['upload_timestamp'] = datetime.fromisoformat(item['timestamp'])
+            
+            return ExcelFileMetadata(**item)
+            
+        except Exception as e:
+            logger.error(f"Error retrieving file metadata {file_id}: {str(e)}")
+            return None
+
+    def get_latest_validation_for_file(self, file_id: str) -> Optional[ValidationResult]:
+        """Get the most recent validation result for a file."""
+        if not self.cosmos_client:
+            return None
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["validations"])
+            # Query within the partition for latest by timestamp
+            query = (
+                "SELECT TOP 1 * FROM c WHERE c.file_id = @file_id ORDER BY c.timestamp DESC"
+            )
+            items = list(
+                container.query_items(
+                    query=query,
+                    parameters=[{"name": "@file_id", "value": file_id}],
+                    enable_cross_partition_query=False,
+                )
+            )
+            if not items:
+                return None
+            return self._deserialize_validation_result(items[0])
+        except Exception as e:
+            logger.error(f"Error retrieving latest validation for {file_id}: {str(e)}")
+            return None
+
+    def get_change_history(self, file_id: str, limit: int = 50) -> List[ChangeTrackingRecord]:
+        """Retrieve change tracking history for a file."""
+        results: List[ChangeTrackingRecord] = []
+        if not self.cosmos_client:
+            return results
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["tracking"])
+            query = (
+                "SELECT TOP @limit * FROM c WHERE c.file_id = @file_id ORDER BY c.change_timestamp DESC"
+            )
+            items = list(
+                container.query_items(
+                    query=query,
+                    parameters=[
+                        {"name": "@file_id", "value": file_id},
+                        {"name": "@limit", "value": limit},
+                    ],
+                    enable_cross_partition_query=False,
+                )
+            )
+            for it in items:
+                # Some records may not have change_timestamp yet
+                if it.get("change_timestamp"):
+                    try:
+                        it["change_timestamp"] = datetime.fromisoformat(it["change_timestamp"])  # type: ignore
+                    except Exception:
+                        pass
+                results.append(ChangeTrackingRecord(**it))
+        except Exception as e:
+            logger.error(f"Error retrieving change history for {file_id}: {str(e)}")
+        return results
+
+    def get_latest_tracking_for_file(self, file_id: str) -> Optional[ChangeTrackingRecord]:
+        """Return the most recent change tracking record for a file, if any."""
+        records = self.get_change_history(file_id, limit=1)
+        return records[0] if records else None
+
+    def _deserialize_validation_result(self, item: Dict[str, Any]) -> ValidationResult:
+        """Convert a stored dict into a ValidationResult model."""
+        try:
+            # Normalize timestamp
+            if isinstance(item.get("timestamp"), str):
+                try:
+                    item["timestamp"] = datetime.fromisoformat(item["timestamp"])  # type: ignore
+                except Exception:
+                    pass
+            # Convert status string to enum value if needed
+            if isinstance(item.get("status"), str):
+                status_str = item["status"]
+                if status_str.lower() == "passed":
+                    item["status"] = ValidationStatus.PASSED
+                elif status_str.lower() == "failed":
+                    item["status"] = ValidationStatus.FAILED
+                elif status_str.lower() == "pending":
+                    item["status"] = ValidationStatus.PENDING
+                else:
+                    item["status"] = ValidationStatus.PENDING
+            # Convert errors and warnings dicts to models if necessary
+            def map_err(e: Any) -> Any:
+                try:
+                    return e if isinstance(e, dict) is False else e
+                except Exception:
+                    return e
+            if isinstance(item.get("errors"), list):
+                item["errors"] = [map_err(e) for e in item["errors"]]
+            if isinstance(item.get("warnings"), list):
+                item["warnings"] = [map_err(w) for w in item["warnings"]]
+            # Pydantic will coerce dicts into the embedded models
+            return ValidationResult(**item)
+        except Exception as e:
+            logger.error(f"Failed to deserialize ValidationResult {item.get('id')}: {str(e)}")
+            # Fallback minimal object to avoid crashing callers
+            return ValidationResult(
+                file_id=item.get("file_id", "unknown"),
+                validation_id=item.get("id", item.get("validation_id", "unknown")),
+                status=ValidationStatus.PENDING,
+                timestamp=datetime.now(timezone.utc),
+                errors=[],
+                warnings=[],
+                total_errors=item.get("total_errors", 0),
+                total_warnings=item.get("total_warnings", 0),
+                processed_rows=item.get("processed_rows", 0),
+            )
+
+    # ----------------------------
+    # Additional helpers for ops
+    # ----------------------------
+
+    def update_validation_status(self, validation_id: str, new_status: ValidationStatus) -> bool:
+        """Update the status field of a validation record."""
+        if not self.cosmos_client:
+            return False
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["validations"])
+            # Query by id; cross partition
+            items = list(
+                container.query_items(
+                    query="SELECT * FROM c WHERE c.id = @id",
+                    parameters=[{"name": "@id", "value": validation_id}],
+                    enable_cross_partition_query=True,
+                )
+            )
+            if not items:
+                return False
+            item = items[0]
+            item["status"] = new_status.value if isinstance(new_status, ValidationStatus) else str(new_status)
+            container.replace_item(item["id"], item)
+            return True
+        except Exception as e:
+            logger.error(f"Error updating validation status {validation_id}: {str(e)}")
+            return False
+
+    def list_failed_validations(self, days_older_than: int = 3, limit: int = 100) -> List[ValidationResult]:
+        """Return failed validations older than N days (for reminders)."""
+        results: List[ValidationResult] = []
+        if not self.cosmos_client:
+            return results
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["validations"])
+            # ISO instant cutoff
+            from datetime import timedelta
+            cutoff = (datetime.now(timezone.utc) - timedelta(days=days_older_than)).isoformat()
+            query = (
+                "SELECT TOP @limit * FROM c WHERE c.status = 'failed' AND c.timestamp < @cutoff"
+            )
+            items = list(
+                container.query_items(
+                    query=query,
+                    parameters=[
+                        {"name": "@limit", "value": limit},
+                        {"name": "@cutoff", "value": cutoff},
+                    ],
+                    enable_cross_partition_query=True,
+                )
+            )
+            for it in items:
+                try:
+                    results.append(self._deserialize_validation_result(it))
+                except Exception:
+                    continue
+        except Exception as e:
+            logger.error(f"Error listing failed validations: {str(e)}")
+        return results
+
+    def list_email_recipients_for_validation(self, validation_id: str) -> List[str]:
+        """Return distinct recipient emails recorded for a validation."""
+        recipients: List[str] = []
+        if not self.cosmos_client:
+            return recipients
+        try:
+            database = self.cosmos_client.get_database_client(self.database_name)
+            container = database.get_container_client(self.containers["emails"])
+            items = list(
+                container.query_items(
+                    query=(
+                        "SELECT c.recipient_email FROM c WHERE c.validation_id = @vid"
+                    ),
+                    parameters=[{"name": "@vid", "value": validation_id}],
+                    enable_cross_partition_query=True,
+                )
+            )
+            recipients = sorted({it.get("recipient_email") for it in items if it.get("recipient_email")})  # type: ignore
+        except Exception as e:
+            logger.error(f"Error listing recipients for validation {validation_id}: {str(e)}")
+        return recipients
diff --git a/src/services/validation_service.py b/src/services/validation_service.py
new file mode 100644
index 0000000..632431d
--- /dev/null
+++ b/src/services/validation_service.py
@@ -0,0 +1,312 @@
+import pandas as pd
+import logging
+import os
+from typing import List, Dict, Any, Optional
+from datetime import datetime, timezone
+try:
+    from azure.ai.inference import ChatCompletionsClient
+    from azure.ai.inference.models import SystemMessage, UserMessage
+    from azure.core.credentials import AzureKeyCredential
+except Exception:  # pragma: no cover - allow running without Azure SDKs installed
+    ChatCompletionsClient = None  # type: ignore
+    SystemMessage = None  # type: ignore
+    UserMessage = None  # type: ignore
+    AzureKeyCredential = None  # type: ignore
+from src.models.validation_models import ValidationRule, ValidationError, ValidationResult, ValidationStatus
+
+logger = logging.getLogger(__name__)
+
+class ValidationService:
+    """Service for data validation using Azure AI"""
+    
+    def __init__(self):
+        self.ai_client = self._initialize_ai_client()
+        self.default_rules = self._get_default_validation_rules()
+    
+    def _initialize_ai_client(self) -> Optional[ChatCompletionsClient]:
+        """Initialize Azure AI client"""
+        try:
+            if ChatCompletionsClient is None or AzureKeyCredential is None:
+                logger.warning("Azure OpenAI SDK not available; AI suggestions disabled")
+                return None
+            endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
+            api_key = os.getenv("AZURE_OPENAI_API_KEY")
+            
+            if not endpoint or not api_key:
+                logger.warning("Azure OpenAI credentials not configured")
+                return None
+            
+            return ChatCompletionsClient(
+                endpoint=endpoint,
+                credential=AzureKeyCredential(api_key)
+            )
+        except Exception as e:
+            logger.error(f"Failed to initialize AI client: {str(e)}")
+            return None
+    
+    def _get_default_validation_rules(self) -> List[ValidationRule]:
+        """Get default validation rules"""
+        return [
+            ValidationRule(
+                rule_id="email_format",
+                rule_name="Email Format Validation",
+                description="Validate email format",
+                rule_type="format",
+                parameters={
+                    "pattern": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
+                    "columns": ["email"]
+                },
+                severity="error"
+            ),
+            ValidationRule(
+                rule_id="required_fields",
+                rule_name="Required Fields",
+                description="Check for required field completion",
+                rule_type="custom",
+                parameters={"required_columns": []},
+                severity="error"
+            ),
+            ValidationRule(
+                rule_id="data_consistency",
+                rule_name="Data Consistency",
+                description="Check for data consistency across rows",
+                rule_type="custom",
+                parameters={},
+                severity="warning"
+            )
+        ]
+    
+    def validate_data(self, df: pd.DataFrame, file_id: str, 
+                     custom_rules: List[ValidationRule] = None) -> ValidationResult:
+        """
+        Validate DataFrame using specified rules
+        
+        Args:
+            df: DataFrame to validate
+            file_id: Unique file identifier
+            custom_rules: Custom validation rules
+            
+        Returns:
+            ValidationResult object
+        """
+        validation_id = f"val_{file_id}_{int(datetime.now(timezone.utc).timestamp())}"
+        
+        # Combine default and custom rules
+        rules = self.default_rules.copy()
+        if custom_rules:
+            rules.extend(custom_rules)
+        
+        errors = []
+        warnings = []
+        
+        # Apply each validation rule
+        for rule in rules:
+            rule_errors = self._apply_validation_rule(df, rule)
+            
+            for error in rule_errors:
+                if error.severity == "error":
+                    errors.append(error)
+                else:
+                    warnings.append(error)
+        
+        # Use AI for intelligent validation if available
+        if self.ai_client and len(errors) > 0:
+            ai_suggestions = self._get_ai_suggestions(df, errors)
+            for i, error in enumerate(errors):
+                if i < len(ai_suggestions):
+                    error.suggested_correction = ai_suggestions[i]
+        
+        # Determine overall status
+        status = ValidationStatus.FAILED if errors else ValidationStatus.PASSED
+        
+        result = ValidationResult(
+            file_id=file_id,
+            validation_id=validation_id,
+            status=status,
+            timestamp=datetime.now(timezone.utc),
+            errors=errors,
+            warnings=warnings,
+            total_errors=len(errors),
+            total_warnings=len(warnings),
+            processed_rows=len(df)
+        )
+        
+        logger.info(f"Validation completed: {len(errors)} errors, {len(warnings)} warnings")
+        return result
+    
+    def _apply_validation_rule(self, df: pd.DataFrame, rule: ValidationRule) -> List[ValidationError]:
+        """Apply a single validation rule to the DataFrame"""
+        errors = []
+        
+        try:
+            if rule.rule_type == "format":
+                errors.extend(self._validate_format(df, rule))
+            elif rule.rule_type == "range":
+                errors.extend(self._validate_range(df, rule))
+            elif rule.rule_type == "data_type":
+                errors.extend(self._validate_data_type(df, rule))
+            elif rule.rule_type == "custom":
+                errors.extend(self._validate_custom(df, rule))
+            
+        except Exception as e:
+            logger.error(f"Error applying validation rule {rule.rule_id}: {str(e)}")
+        
+        return errors
+    
+    def _validate_format(self, df: pd.DataFrame, rule: ValidationRule) -> List[ValidationError]:
+        """Validate format using regex patterns"""
+        errors = []
+        pattern = rule.parameters.get("pattern", "")
+        columns = rule.parameters.get("columns", df.columns)
+        
+        for col in columns:
+            if col in df.columns:
+                for idx, value in df[col].items():
+                    if pd.notna(value):
+                        import re
+                        if not re.match(pattern, str(value)):
+                            errors.append(ValidationError(
+                                row=idx + 1,
+                                column=col,
+                                value=value,
+                                rule_id=rule.rule_id,
+                                message=f"Value does not match expected format: {rule.description}",
+                                severity=rule.severity
+                            ))
+        
+        return errors
+    
+    def _validate_range(self, df: pd.DataFrame, rule: ValidationRule) -> List[ValidationError]:
+        """Validate numeric ranges"""
+        errors = []
+        min_val = rule.parameters.get("min")
+        max_val = rule.parameters.get("max")
+        columns = rule.parameters.get("columns", [])
+        
+        for col in columns:
+            if col in df.columns:
+                for idx, value in df[col].items():
+                    if pd.notna(value):
+                        try:
+                            num_val = float(value)
+                            if min_val is not None and num_val < min_val:
+                                errors.append(ValidationError(
+                                    row=idx + 1,
+                                    column=col,
+                                    value=value,
+                                    rule_id=rule.rule_id,
+                                    message=f"Value {num_val} is below minimum {min_val}",
+                                    severity=rule.severity
+                                ))
+                            elif max_val is not None and num_val > max_val:
+                                errors.append(ValidationError(
+                                    row=idx + 1,
+                                    column=col,
+                                    value=value,
+                                    rule_id=rule.rule_id,
+                                    message=f"Value {num_val} is above maximum {max_val}",
+                                    severity=rule.severity
+                                ))
+                        except ValueError:
+                            errors.append(ValidationError(
+                                row=idx + 1,
+                                column=col,
+                                value=value,
+                                rule_id=rule.rule_id,
+                                message=f"Value is not numeric: {value}",
+                                severity=rule.severity
+                            ))
+        
+        return errors
+    
+    def _validate_data_type(self, df: pd.DataFrame, rule: ValidationRule) -> List[ValidationError]:
+        """Validate data types"""
+        errors = []
+        expected_type = rule.parameters.get("expected_type")
+        columns = rule.parameters.get("columns", [])
+        
+        for col in columns:
+            if col in df.columns:
+                for idx, value in df[col].items():
+                    if pd.notna(value):
+                        if expected_type == "int" and not isinstance(value, int):
+                            try:
+                                int(value)
+                            except (ValueError, TypeError):
+                                errors.append(ValidationError(
+                                    row=idx + 1,
+                                    column=col,
+                                    value=value,
+                                    rule_id=rule.rule_id,
+                                    message=f"Expected integer, got {type(value).__name__}",
+                                    severity=rule.severity
+                                ))
+        
+        return errors
+    
+    def _validate_custom(self, df: pd.DataFrame, rule: ValidationRule) -> List[ValidationError]:
+        """Apply custom validation logic"""
+        errors = []
+        
+        if rule.rule_id == "required_fields":
+            required_columns = rule.parameters.get("required_columns", [])
+            for col in required_columns:
+                if col in df.columns:
+                    null_rows = df[col].isna()
+                    for idx in df[null_rows].index:
+                        errors.append(ValidationError(
+                            row=idx + 1,
+                            column=col,
+                            value=None,
+                            rule_id=rule.rule_id,
+                            message=f"Required field is empty",
+                            severity=rule.severity
+                        ))
+        
+        return errors
+    
+    def _get_ai_suggestions(self, df: pd.DataFrame, errors: List[ValidationError]) -> List[str]:
+        """Get AI-powered suggestions for corrections"""
+        if not self.ai_client:
+            return []
+        
+        suggestions = []
+        
+        try:
+            # Prepare context for AI
+            context = f"Data validation errors found in Excel file with {len(df)} rows and {len(df.columns)} columns."
+            error_summary = "\n".join([
+                f"Row {error.row}, Column {error.column}: {error.message} (Value: {error.value})"
+                for error in errors[:5]  # Limit to first 5 errors
+            ])
+            
+            prompt = f"""
+            {context}
+            
+            Errors found:
+            {error_summary}
+            
+            Please provide specific correction suggestions for each error. 
+            Keep suggestions concise and actionable.
+            """
+            
+            model = os.getenv("AZURE_OPENAI_MODEL", "gpt-4.1")
+            
+            response = self.ai_client.complete(
+                messages=[
+                    SystemMessage("You are a data validation expert. Provide specific, actionable suggestions for correcting data validation errors."),
+                    UserMessage(prompt)
+                ],
+                temperature=0.3,
+                model=model
+            )
+            
+            if response.choices:
+                suggestion_text = response.choices[0].message.content
+                # Split suggestions by lines and clean up
+                suggestions = [s.strip() for s in suggestion_text.split('\n') if s.strip()]
+            
+        except Exception as e:
+            logger.error(f"Error getting AI suggestions: {str(e)}")
+        
+        return suggestions
diff --git a/src/utils/cards.py b/src/utils/cards.py
new file mode 100644
index 0000000..a017408
--- /dev/null
+++ b/src/utils/cards.py
@@ -0,0 +1,80 @@
+from __future__ import annotations
+
+from typing import Any, Dict, List
+
+
+def _mk_citation_block(citations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    items: List[Dict[str, Any]] = []
+    for c in citations[:2]:
+        if c.get("type") == "table":
+            items.append({
+                "type": "TextBlock",
+                "text": f"SQL: {c.get('sql', '')}",
+                "wrap": True,
+                "spacing": "Small",
+                "isSubtle": True,
+            })
+        else:
+            excerpt = c.get("excerpt", "")
+            if excerpt:
+                items.append({
+                    "type": "TextBlock",
+                    "text": f"‚Ä¢ {excerpt}",
+                    "wrap": True,
+                    "spacing": "Small",
+                })
+    return items
+
+
+def build_answer_card(payload: Dict[str, Any]) -> Dict[str, Any]:
+    """Create a Teams-friendly Adaptive Card for an agent answer.
+
+    Expected payload keys: tool, result, citations, powerBiLink (optional)
+    """
+    tool = payload.get("tool", "")
+    title = {
+        "fabric_sql": "Structured Result",
+        "ai_search": "Contract Passage",
+        "graph": "Microsoft Graph Results",
+    }.get(tool, "Agent Result")
+
+    # Create a preview text if result is a list of rows or strings
+    preview: str = ""
+    result = payload.get("result")
+    if isinstance(result, list) and result:
+        first = result[0]
+        if isinstance(first, dict):
+            # join up to three k:v pairs
+            preview = ", ".join(
+                [f"{k}: {v}" for k, v in list(first.items())[:3]]
+            )
+        else:
+            preview = str(first)[:180]
+
+    body: List[Dict[str, Any]] = [
+        {"type": "TextBlock", "size": "Medium", "weight": "Bolder", "text": title},
+    ]
+    if preview:
+        body.append({"type": "TextBlock", "text": preview, "wrap": True})
+
+    citations = payload.get("citations") or []
+    if citations:
+        body.append({"type": "TextBlock", "text": "Citations:", "weight": "Bolder"})
+        body.extend(_mk_citation_block(citations))
+
+    actions: List[Dict[str, Any]] = []
+    if payload.get("powerBiLink"):
+        actions.append({
+            "type": "Action.OpenUrl",
+            "title": "Open in Power BI",
+            "url": payload["powerBiLink"],
+        })
+
+    return {
+        "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
+        "type": "AdaptiveCard",
+        "version": "1.5",
+        "body": body,
+        "actions": actions,
+    }
+
diff --git a/src/utils/helpers.py b/src/utils/helpers.py
new file mode 100644
index 0000000..58d27b7
--- /dev/null
+++ b/src/utils/helpers.py
@@ -0,0 +1,234 @@
+import hashlib
+import re
+import logging
+from typing import List, Dict, Any, Optional
+import uuid
+from datetime import datetime
+
+logger = logging.getLogger(__name__)
+
+def generate_file_hash(file_data: bytes) -> str:
+    """
+    Generate MD5 hash for file data
+    
+    Args:
+        file_data: Raw file bytes
+        
+    Returns:
+        MD5 hash string
+    """
+    return hashlib.md5(file_data).hexdigest()
+
+def validate_email_format(email: str) -> bool:
+    """
+    Validate email format using regex
+    
+    Args:
+        email: Email address to validate
+        
+    Returns:
+        True if valid email format
+    """
+    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
+    return bool(re.match(pattern, email.strip().lower()))
+
+def extract_emails_from_text(text: str) -> List[str]:
+    """
+    Extract email addresses from text using regex
+    
+    Args:
+        text: Text to search for emails
+        
+    Returns:
+        List of unique email addresses found
+    """
+    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
+    emails = re.findall(pattern, str(text))
+    return list(set(email.lower() for email in emails if validate_email_format(email)))
+
+def sanitize_filename(filename: str) -> str:
+    """
+    Sanitize filename by removing special characters
+    
+    Args:
+        filename: Original filename
+        
+    Returns:
+        Sanitized filename
+    """
+    # Remove special characters but keep dots and underscores
+    sanitized = re.sub(r'[^\w\-_\.]', '_', filename)
+    return sanitized.strip()
+
+def format_file_size(size_bytes: int) -> str:
+    """
+    Format file size in human readable format
+    
+    Args:
+        size_bytes: Size in bytes
+        
+    Returns:
+        Formatted size string
+    """
+    if size_bytes == 0:
+        return "0 B"
+    
+    size_names = ["B", "KB", "MB", "GB"]
+    size = size_bytes
+    i = 0
+    
+    while size >= 1024 and i < len(size_names) - 1:
+        size /= 1024.0
+        i += 1
+    
+    return f"{size:.1f} {size_names[i]}"
+
+def generate_unique_id(prefix: str = "") -> str:
+    """
+    Generate unique identifier with timestamp
+    
+    Args:
+        prefix: Optional prefix for the ID
+        
+    Returns:
+        Unique identifier string
+    """
+    timestamp = int(datetime.now().timestamp() * 1000)  # Include milliseconds
+    if prefix:
+        return f"{prefix}_{timestamp}"
+    return str(timestamp)
+
+def safe_get_dict_value(data: Dict[str, Any], key: str, default: Any = None) -> Any:
+    """
+    Safely get value from dictionary with nested key support
+    
+    Args:
+        data: Dictionary to search
+        key: Key to look for (supports dot notation for nested keys)
+        default: Default value if key not found
+        
+    Returns:
+        Value if found, default otherwise
+    """
+    try:
+        if '.' in key:
+            keys = key.split('.')
+            value = data
+            for k in keys:
+                value = value[k]
+            return value
+        else:
+            return data.get(key, default)
+    except (KeyError, TypeError):
+        return default
+
+def truncate_string(text: str, max_length: int = 100, suffix: str = "...") -> str:
+    """
+    Truncate string to maximum length
+    
+    Args:
+        text: Text to truncate
+        max_length: Maximum length allowed
+        suffix: Suffix to add when truncating
+        
+    Returns:
+        Truncated string
+    """
+    if len(text) <= max_length:
+        return text
+    
+    return text[:max_length - len(suffix)] + suffix
+
+def parse_azure_connection_string(connection_string: str) -> Dict[str, str]:
+    """
+    Parse Azure connection string into components
+    
+    Args:
+        connection_string: Azure connection string
+        
+    Returns:
+        Dictionary with connection components
+    """
+    components = {}
+    
+    try:
+        parts = connection_string.split(';')
+        for part in parts:
+            if '=' in part:
+                key, value = part.split('=', 1)
+                components[key] = value
+    except Exception as e:
+        logger.error(f"Error parsing connection string: {str(e)}")
+    
+    return components
+
+def validate_azure_config() -> Dict[str, bool]:
+    """
+    Validate that required Azure configuration is present
+    
+    Returns:
+        Dictionary with validation results for each service
+    """
+    import os
+    
+    config_status = {
+        'storage': bool(os.getenv('AZURE_STORAGE_CONNECTION_STRING')),
+        'cosmosdb': bool(os.getenv('AZURE_COSMOSDB_CONNECTION_STRING')),
+        'communication': bool(os.getenv('AZURE_COMMUNICATION_SERVICES_CONNECTION_STRING')),
+        'openai': bool(os.getenv('AZURE_OPENAI_ENDPOINT') and os.getenv('AZURE_OPENAI_API_KEY'))
+    }
+    
+    return config_status
+
+def log_function_execution(func_name: str, start_time: datetime, 
+                          end_time: datetime, success: bool, 
+                          additional_info: Dict[str, Any] = None):
+    """
+    Log function execution details
+    
+    Args:
+        func_name: Name of the function
+        start_time: Execution start time
+        end_time: Execution end time
+        success: Whether execution was successful
+        additional_info: Additional information to log
+    """
+    execution_time = (end_time - start_time).total_seconds()
+    status = "SUCCESS" if success else "FAILED"
+    
+    log_message = f"Function {func_name} {status} - Duration: {execution_time:.3f}s"
+    
+    if additional_info:
+        info_str = ", ".join([f"{k}: {v}" for k, v in additional_info.items()])
+        log_message += f" - {info_str}"
+    
+    if success:
+        logger.info(log_message)
+    else:
+        logger.error(log_message)
+
+def get_correlation_id(req) -> str:
+    """Retrieve or generate a correlation ID from an HTTP request."""
+    try:
+        cid = req.headers.get('x-correlation-id')  # type: ignore[attr-defined]
+        if cid and isinstance(cid, str) and len(cid) >= 8:
+            return cid
+    except Exception:
+        pass
+    return str(uuid.uuid4())
+
+class ConfigurationError(Exception):
+    """Custom exception for configuration errors"""
+    pass
+
+class ValidationError(Exception):
+    """Custom exception for validation errors"""
+    pass
+
+class StorageError(Exception):
+    """Custom exception for storage errors"""
+    pass
+
+class EmailError(Exception):
+    """Custom exception for email errors"""
+    pass
diff --git a/src/utils/pbi.py b/src/utils/pbi.py
new file mode 100644
index 0000000..47f4480
--- /dev/null
+++ b/src/utils/pbi.py
@@ -0,0 +1,27 @@
+import os
+import urllib.parse
+from typing import Dict, Any, Optional
+
+
+def build_pbi_deeplink(filters: Dict[str, Any]) -> Optional[str]:
+    """Build a Power BI report link with simple filters.
+
+    Requires env vars: `PBI_WORKSPACE_ID`, `PBI_REPORT_ID`.
+    Filters is a mapping like {"Table/Column": "Value", ...}.
+    """
+    ws = os.getenv("PBI_WORKSPACE_ID")
+    rep = os.getenv("PBI_REPORT_ID")
+    if not ws or not rep:
+        return None
+
+    base = f"https://app.powerbi.com/groups/{ws}/reports/{rep}/ReportSection"
+    # Build `filter` query with AND of each expression Table/Column eq 'value'
+    exprs = []
+    for col, val in filters.items():
+        if isinstance(val, str):
+            v = f"'{val}'"
+        else:
+            v = str(val)
+        exprs.append(f"{col} eq {v}")
+    q = urllib.parse.urlencode({"filter": " and ".join(exprs)}) if exprs else ""
+    return f"{base}?{q}" if q else base
diff --git a/teams/cards/answerCard.sample.json b/teams/cards/answerCard.sample.json
new file mode 100644
index 0000000..69c7391
--- /dev/null
+++ b/teams/cards/answerCard.sample.json
@@ -0,0 +1,14 @@
+{
+  "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
+  "type": "AdaptiveCard",
+  "version": "1.5",
+  "body": [
+    { "type": "TextBlock", "text": "Structured Result", "size": "Medium", "weight": "Bolder" },
+    { "type": "TextBlock", "text": "Carrier: X, Variance: 1243.55", "wrap": true },
+    { "type": "TextBlock", "text": "Citations:", "weight": "Bolder" },
+    { "type": "TextBlock", "text": "SQL: SELECT ...", "wrap": true, "isSubtle": true }
+  ],
+  "actions": [
+    { "type": "Action.OpenUrl", "title": "Open in Power BI", "url": "https://app.powerbi.com/..." }
+  ]
+}
diff --git a/tests/conftest.py b/tests/conftest.py
new file mode 100644
index 0000000..6ca9ea5
--- /dev/null
+++ b/tests/conftest.py
@@ -0,0 +1,8 @@
+import os
+import sys
+
+# Ensure project root is on sys.path so `src` package is importable during tests
+PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
+if PROJECT_ROOT not in sys.path:
+    sys.path.insert(0, PROJECT_ROOT)
+
diff --git a/tests/test_agents.py b/tests/test_agents.py
new file mode 100644
index 0000000..896d218
--- /dev/null
+++ b/tests/test_agents.py
@@ -0,0 +1,285 @@
+from src.agents import (
+    CarrierAgent,
+    DomainAgent,
+    CustomerOpsAgent,
+    ClaimsAgent,
+    OrchestratorAgent,
+    StructuredDataAgent,
+    UnstructuredDataAgent,
+)
+import responses
+import pytest
+
+from src.services.fabric_data_agent import FabricDataAgent
+from src.services.search_service import SearchService
+from src.services.graph_service import GraphService
+
+
+def test_orchestrator_routes_structured_queries() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "X", "overbilled": True}]},
+        )
+        fabric = FabricDataAgent("https://fabric.test", token="T")
+        structured = StructuredDataAgent(fabric)
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+
+        params = {"@from": "2024-01-01", "@to": "2024-01-31"}
+        result = orchestrator.handle(("variance_summary", params))
+
+        assert result == [{"carrier": "X", "overbilled": True}]
+
+
+def test_orchestrator_routes_unstructured_queries() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            (
+                "https://search.test/indexes/contracts/docs/search"
+                "?api-version=2021-04-30-Preview"
+            ),
+            json={
+                "value": [{"content": "Clause 7.4: minimum charge applies."}]
+            },
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+
+        result = orchestrator.handle("what does clause 7.4 say?")
+
+        assert result == ["Clause 7.4: minimum charge applies."]
+
+
+def test_orchestrator_routes_graph_queries() -> None:
+    with responses.RequestsMock() as rsps:
+        payload = {
+            "value": [
+                {
+                    "hitsContainers": [
+                        {
+                            "hits": [
+                                {"_source": {"subject": "mail about invoice"}}
+                            ]
+                        }
+                    ]
+                }
+            ]
+        }
+        rsps.add(
+            "POST",
+            "https://graph.test/search/query",
+            json=payload,
+        )
+        graph = GraphService(token="T", endpoint="https://graph.test")
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured, graph)
+
+        result = orchestrator.handle("show recent email")
+
+        assert result == ["mail about invoice"]
+
+
+def test_domain_agent_delegates_to_orchestrator() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Y"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        agent = DomainAgent(orchestrator)
+
+
+
+def test_carrier_and_customer_agents_delegate() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Z"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        carrier = CarrierAgent(orchestrator)
+        customer = CustomerOpsAgent(orchestrator)
+
+
+
+def test_claims_agent_delegates() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Z"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        claims = ClaimsAgent(orchestrator)
+
+        assert claims.handle("sql rate") == [{"carrier": "Z"}]
+
+
+def test_claims_agent_delegates() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Z"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        claims = ClaimsAgent(orchestrator)
+
+        assert claims.handle("sql rate") == [{"carrier": "Z"}]
+
+
+def test_claims_agent_delegates() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Z"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        claims = ClaimsAgent(orchestrator)
+
+        assert claims.handle("sql rate") == [{"carrier": "Z"}]
+
+
+def test_claims_agent_delegates() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "Z"}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orchestrator = OrchestratorAgent(structured, unstructured)
+        claims = ClaimsAgent(orchestrator)
+
+        assert claims.handle("sql rate") == [{"carrier": "Z"}]
+
+
+def test_orchestrator_citations_structured() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            "https://fabric.test/sql",
+            json={"rows": [{"carrier": "X", "overbilled": True}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orch = OrchestratorAgent(structured, unstructured)
+
+        params = {"@from": "2024-01-01", "@to": "2024-01-31"}
+        payload = orch.handle_with_citations(("variance_summary", params))
+
+        assert payload["tool"] == "fabric_sql"
+        assert isinstance(payload.get("citations"), list)
+        assert payload["citations"][0]["template"] == "variance_summary"
+
+
+def test_orchestrator_citations_unstructured() -> None:
+    with responses.RequestsMock() as rsps:
+        rsps.add(
+            "POST",
+            (
+                "https://search.test/indexes/contracts/docs/search"
+                "?api-version=2021-04-30-Preview"
+            ),
+            json={"value": [{"content": "C7.4 minimum charge applies."}]},
+        )
+        structured = StructuredDataAgent(
+            FabricDataAgent("https://fabric.test", token="T")
+        )
+        unstructured = UnstructuredDataAgent(
+            SearchService("https://search.test", "contracts", api_key="K")
+        )
+        orch = OrchestratorAgent(structured, unstructured)
+
+        payload = orch.handle_with_citations("what does clause 7.4 say?")
+
+        assert payload["tool"] == "ai_search"
+        assert len(payload["citations"]) >= 1
+        assert "excerpt" in payload["citations"][0]
+
+
+def test_structured_agent_rejects_unknown_template() -> None:
+    agent = StructuredDataAgent(
+        FabricDataAgent("https://fabric.test", token="T")
+    )
+    with pytest.raises(ValueError):
+        agent.query("missing", {})
+
+
+def test_fabric_run_sql_params_sends_parameters() -> None:
+    with responses.RequestsMock() as rsps:
+        def _cb(request):
+            body = request.body
+            import json as _json
+            data = _json.loads(body)
+            assert "parameters" in data
+            assert {"name": "@carrier", "value": "X"} in data["parameters"]
+            return (200, {}, _json.dumps({"rows": []}))
+
+        rsps.add_callback(
+            "POST",
+            "https://fabric.test/sql",
+            callback=_cb,
+            content_type="application/json",
+        )
+        agent = FabricDataAgent("https://fabric.test", token="T")
+        agent.run_sql_params(
+            "SELECT 1 WHERE carrier = @carrier", {"@carrier": "X"}
+        )
diff --git a/tests/test_router.py b/tests/test_router.py
new file mode 100644
index 0000000..1e82c28
--- /dev/null
+++ b/tests/test_router.py
@@ -0,0 +1,16 @@
+from src.agents.router import classify
+
+
+def test_numeric_routes_to_sql() -> None:
+    out = classify("How much were we overbilled last quarter?")
+    assert out["tool"] == "sql" and out["name"] == "variance_summary"
+
+
+def test_text_routes_to_rag() -> None:
+    out = classify("What does clause 7.4 say about minimums?")
+    assert out["tool"] == "rag"
+
+
+def test_graph_intent() -> None:
+    out = classify("Show me email from Acme last week")
+    assert out["tool"] == "graph"
diff --git a/tests/test_services.py b/tests/test_services.py
new file mode 100644
index 0000000..295fcd5
--- /dev/null
+++ b/tests/test_services.py
@@ -0,0 +1,76 @@
+import pytest
+import pandas as pd
+from src.services.excel_service import ExcelService
+from src.services.validation_service import ValidationService
+from src.models.validation_models import ValidationRule
+
+@pytest.fixture
+def excel_service():
+    return ExcelService()
+
+@pytest.fixture
+def validation_service():
+    return ValidationService()
+
+@pytest.fixture
+def sample_excel_data():
+    """Create sample Excel data for testing"""
+    data = {
+        'name': ['John Doe', 'Jane Smith', 'Bob Johnson'],
+        'email': ['john@example.com', 'invalid-email', 'bob@example.com'],
+        'age': [30, 25, 35],
+        'score': [85, 95, 75]
+    }
+    return pd.DataFrame(data)
+
+def test_excel_service_clean_dataframe(excel_service, sample_excel_data):
+    """Test DataFrame cleaning functionality"""
+    # Add some messy data
+    messy_data = sample_excel_data.copy()
+    messy_data.loc[3] = [None, None, None, None]  # Empty row
+    messy_data['name'] = messy_data['name'].astype(str) + '  '  # Add whitespace
+    
+    cleaned_df = excel_service._clean_dataframe(messy_data)
+    
+    # Check that empty row was removed
+    assert len(cleaned_df) == 3
+    
+    # Check that whitespace was stripped
+    assert not any(name.endswith('  ') for name in cleaned_df['name'])
+
+def test_validation_service_email_validation(validation_service, sample_excel_data):
+    """Test email validation"""
+    result = validation_service.validate_data(sample_excel_data, "test_file")
+    
+    # Should find one invalid email
+    email_errors = [error for error in result.errors if 'email' in error.column.lower()]
+    assert len(email_errors) > 0
+
+def test_validation_rule_creation():
+    """Test ValidationRule model"""
+    rule = ValidationRule(
+        rule_id="test_rule",
+        rule_name="Test Rule",
+        description="Test description",
+        rule_type="format",
+        parameters={"pattern": r"^test$"},
+        severity="error"
+    )
+    
+    assert rule.rule_id == "test_rule"
+    assert rule.severity == "error"
+
+def test_excel_service_email_extraction(excel_service, sample_excel_data):
+    """Test email extraction from DataFrame"""
+    emails = excel_service.extract_email_column(sample_excel_data, "email")
+    
+    # Should extract valid emails only
+    valid_emails = ['john@example.com', 'bob@example.com']
+    for email in valid_emails:
+        assert email in emails
+    
+    # Invalid email should not be included
+    assert 'invalid-email' not in emails
+
+if __name__ == "__main__":
+    pytest.main([__file__])
\ No newline at end of file
diff --git a/tests/test_storage.py b/tests/test_storage.py
new file mode 100644
index 0000000..edcb0da
--- /dev/null
+++ b/tests/test_storage.py
@@ -0,0 +1,127 @@
+from datetime import datetime, timezone, timedelta
+
+from src.services.storage_service import StorageService
+from src.models.validation_models import (
+    EmailNotification,
+    ValidationResult,
+    ValidationStatus,
+)
+
+
+class _FakeContainer:
+    def __init__(self):
+        self.items = {}
+
+    def create_item(self, item):
+        self.items[item["id"]] = item
+
+    def read_item(self, id, partition_key=None):
+        return self.items[id]
+
+    def replace_item(self, id, item):
+        self.items[id] = item
+
+    def query_items(self, query, parameters=None, enable_cross_partition_query=None):
+        # naive implementation only used by tests
+        if "FROM c WHERE c.id = @id" in query:
+            vid = next(p["value"] for p in parameters if p["name"] == "@id")
+            return [self.items.get(vid)] if vid in self.items else []
+        if "FROM c WHERE c.status = 'failed'" in query:
+            cutoff = next(p["value"] for p in parameters if p["name"] == "@cutoff")
+            # treat cutoff as iso string; return all failed older than cutoff
+            out = []
+            for item in self.items.values():
+                if item.get("status") == "failed" and item.get("timestamp", "") < cutoff:
+                    out.append(item)
+            return out
+        if "FROM c WHERE c.file_id = @file_id" in query:
+            fid = next(p["value"] for p in parameters if p["name"] == "@file_id")
+            return [it for it in self.items.values() if it.get("file_id") == fid]
+        return []
+
+
+class _FakeDatabase:
+    def __init__(self):
+        self.containers = {
+            "file-metadata": _FakeContainer(),
+            "validation-results": _FakeContainer(),
+            "email-notifications": _FakeContainer(),
+            "change-tracking": _FakeContainer(),
+        }
+
+    def get_container_client(self, name):
+        return self.containers[name]
+
+    # Cosmos SDK create_if_not_exists compat not used in tests
+
+
+class _FakeCosmos:
+    def __init__(self):
+        self.db = _FakeDatabase()
+
+    def get_database_client(self, name):
+        return self.db
+
+    # used during init (ignored in tests)
+    def create_database_if_not_exists(self, name):
+        return self.db
+
+
+def test_storage_get_email_notification(monkeypatch):
+    svc = StorageService()
+    fake = _FakeCosmos()
+    svc.cosmos_client = fake
+    db = fake.get_database_client(svc.database_name)
+    emails = db.get_container_client(svc.containers["emails"])
+
+    rec = EmailNotification(
+        notification_id="n1",
+        file_id="f1",
+        validation_id="v1",
+        recipient_email="a@b.com",
+        subject="s",
+        sent_timestamp=datetime.now(timezone.utc),
+        delivery_status="sent",
+    )
+    item = rec.model_dump()
+    item["id"] = rec.notification_id
+    item["sent_timestamp"] = rec.sent_timestamp.isoformat()
+    emails.create_item(item)
+
+    got = svc.get_email_notification("n1")
+    assert got and got.notification_id == "n1"
+
+
+def test_storage_list_failed_validations(monkeypatch):
+    svc = StorageService()
+    fake = _FakeCosmos()
+    svc.cosmos_client = fake
+    db = fake.get_database_client(svc.database_name)
+    vals = db.get_container_client(svc.containers["validations"])
+
+    ts_old = (datetime.now(timezone.utc) - timedelta(days=10)).isoformat()
+    vals.create_item({
+        "id": "v1",
+        "file_id": "f1",
+        "status": "failed",
+        "timestamp": ts_old,
+        "errors": [],
+        "warnings": []
+    })
+
+    results = svc.list_failed_validations(days_older_than=3, limit=100)
+    assert len(results) >= 1
+
+
+def test_update_validation_status(monkeypatch):
+    svc = StorageService()
+    fake = _FakeCosmos()
+    svc.cosmos_client = fake
+    db = fake.get_database_client(svc.database_name)
+    vals = db.get_container_client(svc.containers["validations"])
+    vals.create_item({"id": "v1", "file_id": "f1", "status": "failed", "timestamp": datetime.now(timezone.utc).isoformat(), "errors": [], "warnings": []})
+
+    ok = svc.update_validation_status("v1", ValidationStatus.CORRECTED)
+    assert ok is True
+    assert vals.items["v1"]["status"] == "corrected"
+
diff --git a/tests/test_utils.py b/tests/test_utils.py
new file mode 100644
index 0000000..f1dd238
--- /dev/null
+++ b/tests/test_utils.py
@@ -0,0 +1,101 @@
+import pytest
+from src.utils.helpers import (
+    validate_email_format,
+    generate_file_hash,
+    sanitize_filename,
+    format_file_size,
+    truncate_string,
+)
+from src.utils.pbi import build_pbi_deeplink
+
+def test_validate_email_format():
+    """Test email format validation"""
+    valid_emails = [
+        'test@example.com',
+        'user.name@domain.co.uk',
+        'test123@subdomain.example.org'
+    ]
+    
+    invalid_emails = [
+        'invalid-email',
+        '@example.com',
+        'test@',
+        'test.example.com',
+        ''
+    ]
+    
+    for email in valid_emails:
+        assert validate_email_format(email), f"Valid email failed: {email}"
+    
+    for email in invalid_emails:
+        assert not validate_email_format(email), f"Invalid email passed: {email}"
+
+def test_generate_file_hash():
+    """Test file hash generation"""
+    data1 = b"test data"
+    data2 = b"test data"
+    data3 = b"different data"
+    
+    hash1 = generate_file_hash(data1)
+    hash2 = generate_file_hash(data2)
+    hash3 = generate_file_hash(data3)
+    
+    # Same data should produce same hash
+    assert hash1 == hash2
+    
+    # Different data should produce different hash
+    assert hash1 != hash3
+
+def test_sanitize_filename():
+    """Test filename sanitization"""
+    test_cases = [
+        ('normal_file.xlsx', 'normal_file.xlsx'),
+        ('file with spaces.xlsx', 'file_with_spaces.xlsx'),
+        ('file@#$%^&*().xlsx', 'file_________.xlsx'),
+        ('file-name_123.xlsx', 'file-name_123.xlsx')
+    ]
+    
+    for original, expected in test_cases:
+        result = sanitize_filename(original)
+        assert result == expected, f"Failed for {original}: got {result}, expected {expected}"
+
+def test_format_file_size():
+    """Test file size formatting"""
+    test_cases = [
+        (0, "0 B"),
+        (1024, "1.0 KB"),
+        (1024 * 1024, "1.0 MB"),
+        (1536, "1.5 KB"),
+        (2048 * 1024 * 1024, "2.0 GB")
+    ]
+    
+    for size_bytes, expected in test_cases:
+        result = format_file_size(size_bytes)
+        assert result == expected, f"Failed for {size_bytes}: got {result}, expected {expected}"
+
+def test_truncate_string():
+    """Test string truncation"""
+    long_text = "This is a very long string that needs to be truncated"
+    
+    # Test truncation
+    result = truncate_string(long_text, 20)
+    assert len(result) <= 20
+    assert result.endswith("...")
+    
+    # Test no truncation needed
+    short_text = "Short"
+    result = truncate_string(short_text, 20)
+    assert result == short_text
+
+
+def test_build_pbi_deeplink(monkeypatch):
+    monkeypatch.setenv("PBI_WORKSPACE_ID", "ws")
+    monkeypatch.setenv("PBI_REPORT_ID", "rep")
+    url = build_pbi_deeplink(
+        {"vw_Variance/Carrier": "X", "vw_Variance/SKU": "812"}
+    )
+    assert "groups/ws/reports/rep" in url
+    assert "filter=" in url
+
+if __name__ == "__main__":
+    pytest.main([__file__])
diff --git a/tools/repo_sanity.py b/tools/repo_sanity.py
new file mode 100644
index 0000000..246599a
--- /dev/null
+++ b/tools/repo_sanity.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python3
+"""
+Fail CI if we detect:
+- stubs ('pass', '...') in function bodies
+- NotImplementedError / TODO / FIXME
+- wildcard imports, bare except
+- sync 'requests.*' calls (use httpx)
+"""
+
+from __future__ import annotations
+
+import pathlib
+import re
+import sys
+
+ROOT = pathlib.Path(__file__).resolve().parents[1]
+ALLOWLIST_DIRS = {
+    "venv",
+    ".venv",
+    "site-packages",
+    "__pycache__",
+    "build",
+    "dist",
+    ".git",
+}
+
+PATTERNS = {
+    r"^\s*pass\s*$": "Stub 'pass' found",
+    r"^\s*\.\.\.\s*$": "Stub '...' found",
+    r"NotImplementedError": "NotImplementedError left in code",
+    r"#\s*(TODO|FIXME)\b": "TODO/FIXME comment left in code",
+    r"from\s+[\w\.]+\s+import\s+\*": "Wildcard import",
+    r"\bexcept\s*:\s*": "Bare except",
+    (
+        r"\brequests\.(get|post|put|delete|patch|head)\b"
+    ): "Use httpx instead of requests",
+}
+
+
+def scan(path: pathlib.Path) -> list[str]:
+    if any(part in ALLOWLIST_DIRS for part in path.parts):
+        return []
+    try:
+        lines = path.read_text(encoding="utf-8", errors="ignore").splitlines()
+    except (OSError, UnicodeError):  # pragma: no cover - file access issues
+        return []
+    findings: list[str] = []
+    for i, line in enumerate(lines, 1):
+        for pattern, message in PATTERNS.items():
+            if re.search(pattern, line):
+                findings.append(f"{path}:{i}: {message}: {line.strip()}")
+    return findings
+
+
+def main() -> int:
+    findings: list[str] = []
+    for file in ROOT.rglob("*.py"):
+        findings.extend(scan(file))
+    if findings:
+        print("\n‚úñ repo_sanity found issues:\n" + "\n".join(findings))
+        return 1
+    print("‚úì repo_sanity: clean")
+    return 0
+
+
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
diff --git a/tools/run_tests.sh b/tools/run_tests.sh
new file mode 100755
index 0000000..b3a0fe0
--- /dev/null
+++ b/tools/run_tests.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+echo "Linting with flake8..."
+flake8
+
+echo "Running unit tests..."
+pytest -q
+
+echo "All good."
diff --git a/tools/setup_cloud.sh b/tools/setup_cloud.sh
new file mode 100755
index 0000000..60a5fbd
--- /dev/null
+++ b/tools/setup_cloud.sh
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+python --version
+pip --version
+
+echo "Installing Python dependencies..."
+pip install --upgrade pip
+pip install -r requirements.txt
+
+echo "Environment ready. To run tests: tools/run_tests.sh"
